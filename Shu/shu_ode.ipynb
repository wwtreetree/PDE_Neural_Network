{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import dill\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "plt.rcParams['font.family'] = 'Times New Roman'\n",
    "    \n",
    "from src.ode_PINN_hardBC import ODE_PINN_HARDBC\n",
    "from src.first_order_odesys_PINN_hardBC import ORDER1_ODESYS_PINN_HARDBC\n",
    "from src.second_order_odesys_PINN_hardBC import ORDER2_ODESYS_PINN_HARDBC\n",
    "from src.ode_PINN_softBC import ODE_PINN_SOFTBC\n",
    "from src.ode_PINN_adaptCollectionPoint import  ODE_PINN_AdaptiveCollectionPoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Savepickle(obj, doc_path):\n",
    "    with open(doc_path, 'wb') as file:\n",
    "        dill.dump(obj, file)     \n",
    "\n",
    "def Readpickle(doc_path):\n",
    "    with open(doc_path, 'rb') as file:\n",
    "        return dill.load(file)\n",
    "    \n",
    "def SaveModel(model, path): \n",
    "    if model.model_type == 'ODE_PINN_SOFTBC':\n",
    "        data = {'model_type': model.model_type,\n",
    "                'f': model.f, \n",
    "                'bound' : (model.lb,model.ub),\n",
    "                'BC' : model.BC , \n",
    "                'lambdas' : model.lambdas,\n",
    "                'n_hidden': model.n_hidden,\n",
    "                'n_layers': model.n_layers,\n",
    "                'rff_para': model.rff_para,\n",
    "                'rff_B'   : model.rff_B,\n",
    "                'para_dict' : model.state_dict() , \n",
    "                'validate_loss' : model.validate_loss,\n",
    "                'train_loss': model.train_loss,\n",
    "                'L2_loss': model.L2_loss}\n",
    "    else:\n",
    "        data = {'model_type': model.model_type,\n",
    "                'f': model.f, \n",
    "                'bound' : (model.lb,model.ub),\n",
    "                'BC' : model.BC , \n",
    "                'n_hidden': model.n_hidden,\n",
    "                'n_layers': model.n_layers,\n",
    "                'rff_para': model.rff_para,\n",
    "                'rff_B'   : model.rff_B,\n",
    "                'para_dict' : model.state_dict() , \n",
    "                'validate_loss' : model.validate_loss,\n",
    "                'train_loss': model.train_loss,\n",
    "                'L2_loss': model.L2_loss}\n",
    "    Savepickle(data, path)\n",
    "    \n",
    "def LoadModel(path):\n",
    "    data = Readpickle(path)\n",
    "    \n",
    "    if data['model_type'] == 'ODE_PINN_HARDBC':\n",
    "        model = ODE_PINN_HARDBC(f=data['f'], lb=data['bound'][0], ub=data['bound'][1], BC=data['BC'], \n",
    "                                n_hidden=data['n_hidden'], n_layers=data['n_layers'])\n",
    "        \n",
    "    elif data['model_type'] == 'ODE_PINN_SOFTBC':\n",
    "        model = ODE_PINN_SOFTBC(f=data['f'], lb=data['bound'][0], ub=data['bound'][1], BC=data['BC'], lambdas=data['lambdas'],\n",
    "                                n_hidden=data['n_hidden'], n_layers=data['n_layers'])\n",
    "        \n",
    "    elif data['model_type'] == 'ODE_PINN_AdaptiveCollectionPoint':\n",
    "        model = ODE_PINN_AdaptiveCollectionPoint(f=data['f'], lb=data['bound'][0], ub=data['bound'][1], BC=data['BC'], \n",
    "                                n_hidden=data['n_hidden'], n_layers=data['n_layers'])\n",
    "        \n",
    "    elif data['model_type'] == 'ORDER1_ODESYS_PINN_HARDBC':\n",
    "        model = ORDER1_ODESYS_PINN_HARDBC(f=data['f'], lb=data['bound'][0], ub=data['bound'][1], BC=data['BC'], \n",
    "                                n_hidden=data['n_hidden'], n_layers=data['n_layers'])\n",
    "        \n",
    "    elif data['model_type'] == 'ORDER2_ODESYS_PINN_HARDBC':\n",
    "        model = ORDER2_ODESYS_PINN_HARDBC(f=data['f'], lb=data['bound'][0], ub=data['bound'][1], BC=data['BC'], \n",
    "                                n_hidden=data['n_hidden'], n_layers=data['n_layers'])\n",
    "     \n",
    "    model.load_state_dict(data['para_dict'])\n",
    "    model.validate_loss = data['validate_loss']\n",
    "    model.train_loss = data['train_loss']\n",
    "    model.L2_loss = data['L2_loss']\n",
    "    return model\n",
    "\n",
    "def moving_average(x, half_window_size):\n",
    "    res = []\n",
    "    x = np.array(x)\n",
    "    for i in range(len(x)):\n",
    "        if i == 0:\n",
    "            res.append(x[i]) \n",
    "        elif i < half_window_size:\n",
    "            res.append(x[0:2*i+1].mean())\n",
    "        elif i <= len(x)-1-half_window_size:\n",
    "            res.append(x[i-half_window_size:i+half_window_size+1].mean())\n",
    "        else:\n",
    "            res.append(x[2*i+1-len(x):].mean())\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High Frequency Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1/5000] -------------------- \n",
      "Epoch [1/5000], Step [1/47], Loss: 1303.6852\n",
      "Epoch [1/5000], Step [9/47], Loss: 921.6166\n",
      "Epoch [1/5000], Step [18/47], Loss: 861.6320\n",
      "Epoch [1/5000], Step [27/47], Loss: 272.4565\n",
      "Epoch [1/5000], Step [36/47], Loss: 60.0038\n",
      "Epoch [1/5000], Step [45/47], Loss: 30.2157\n",
      "Epoch [1/5000], Avg. Train Sample Loss: 461.2231, Avg. Validate Sample Loss: 34.1983,                             L2 Loss: 1.5213\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [2/5000] -------------------- \n",
      "Epoch [2/5000], Step [1/47], Loss: 37.2113\n",
      "Epoch [2/5000], Step [9/47], Loss: 32.5623\n",
      "Epoch [2/5000], Step [18/47], Loss: 18.5912\n",
      "Epoch [2/5000], Step [27/47], Loss: 15.8298\n",
      "Epoch [2/5000], Step [36/47], Loss: 15.6463\n",
      "Epoch [2/5000], Step [45/47], Loss: 22.2101\n",
      "Epoch [2/5000], Avg. Train Sample Loss: 23.3300, Avg. Validate Sample Loss: 17.5776,                             L2 Loss: 1.3476\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [3/5000] -------------------- \n",
      "Epoch [3/5000], Step [1/47], Loss: 19.5536\n",
      "Epoch [3/5000], Step [9/47], Loss: 20.4923\n",
      "Epoch [3/5000], Step [18/47], Loss: 15.5981\n",
      "Epoch [3/5000], Step [27/47], Loss: 15.8687\n",
      "Epoch [3/5000], Step [36/47], Loss: 17.5847\n",
      "Epoch [3/5000], Step [45/47], Loss: 17.1379\n",
      "Epoch [3/5000], Avg. Train Sample Loss: 17.3822, Avg. Validate Sample Loss: 15.6894,                             L2 Loss: 1.2619\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [4/5000] -------------------- \n",
      "Epoch [4/5000], Step [1/47], Loss: 17.4644\n",
      "Epoch [4/5000], Step [9/47], Loss: 13.2238\n",
      "Epoch [4/5000], Step [18/47], Loss: 13.0411\n",
      "Epoch [4/5000], Step [27/47], Loss: 16.9313\n",
      "Epoch [4/5000], Step [36/47], Loss: 13.6262\n",
      "Epoch [4/5000], Step [45/47], Loss: 12.6859\n",
      "Epoch [4/5000], Avg. Train Sample Loss: 15.0916, Avg. Validate Sample Loss: 14.0154,                             L2 Loss: 1.1559\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [5/5000] -------------------- \n",
      "Epoch [5/5000], Step [1/47], Loss: 14.3658\n",
      "Epoch [5/5000], Step [9/47], Loss: 15.8901\n",
      "Epoch [5/5000], Step [18/47], Loss: 15.9844\n",
      "Epoch [5/5000], Step [27/47], Loss: 12.4424\n",
      "Epoch [5/5000], Step [36/47], Loss: 14.6263\n",
      "Epoch [5/5000], Step [45/47], Loss: 14.3981\n",
      "Epoch [5/5000], Avg. Train Sample Loss: 12.6990, Avg. Validate Sample Loss: 11.2100,                             L2 Loss: 1.0433\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [6/5000] -------------------- \n",
      "Epoch [6/5000], Step [1/47], Loss: 10.9210\n",
      "Epoch [6/5000], Step [9/47], Loss: 12.4727\n",
      "Epoch [6/5000], Step [18/47], Loss: 9.1729\n",
      "Epoch [6/5000], Step [27/47], Loss: 9.0660\n",
      "Epoch [6/5000], Step [36/47], Loss: 10.4490\n",
      "Epoch [6/5000], Step [45/47], Loss: 9.8462\n",
      "Epoch [6/5000], Avg. Train Sample Loss: 10.3179, Avg. Validate Sample Loss: 9.5696,                             L2 Loss: 0.9292\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [7/5000] -------------------- \n",
      "Epoch [7/5000], Step [1/47], Loss: 7.3906\n",
      "Epoch [7/5000], Step [9/47], Loss: 6.0798\n",
      "Epoch [7/5000], Step [18/47], Loss: 7.8267\n",
      "Epoch [7/5000], Step [27/47], Loss: 9.0160\n",
      "Epoch [7/5000], Step [36/47], Loss: 8.1799\n",
      "Epoch [7/5000], Step [45/47], Loss: 6.3395\n",
      "Epoch [7/5000], Avg. Train Sample Loss: 8.2046, Avg. Validate Sample Loss: 7.4650,                             L2 Loss: 0.8147\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [8/5000] -------------------- \n",
      "Epoch [8/5000], Step [1/47], Loss: 6.9741\n",
      "Epoch [8/5000], Step [9/47], Loss: 6.7932\n",
      "Epoch [8/5000], Step [18/47], Loss: 5.6774\n",
      "Epoch [8/5000], Step [27/47], Loss: 5.8319\n",
      "Epoch [8/5000], Step [36/47], Loss: 6.0258\n",
      "Epoch [8/5000], Step [45/47], Loss: 5.1924\n",
      "Epoch [8/5000], Avg. Train Sample Loss: 6.3970, Avg. Validate Sample Loss: 4.9294,                             L2 Loss: 0.7066\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [9/5000] -------------------- \n",
      "Epoch [9/5000], Step [1/47], Loss: 6.1531\n",
      "Epoch [9/5000], Step [9/47], Loss: 4.4154\n",
      "Epoch [9/5000], Step [18/47], Loss: 4.3908\n",
      "Epoch [9/5000], Step [27/47], Loss: 5.2859\n",
      "Epoch [9/5000], Step [36/47], Loss: 4.8534\n",
      "Epoch [9/5000], Step [45/47], Loss: 4.0458\n",
      "Epoch [9/5000], Avg. Train Sample Loss: 4.9031, Avg. Validate Sample Loss: 4.1556,                             L2 Loss: 0.6057\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [10/5000] -------------------- \n",
      "Epoch [10/5000], Step [1/47], Loss: 3.7820\n",
      "Epoch [10/5000], Step [9/47], Loss: 4.4262\n",
      "Epoch [10/5000], Step [18/47], Loss: 3.7782\n",
      "Epoch [10/5000], Step [27/47], Loss: 3.1740\n",
      "Epoch [10/5000], Step [36/47], Loss: 3.5300\n",
      "Epoch [10/5000], Step [45/47], Loss: 4.2072\n",
      "Epoch [10/5000], Avg. Train Sample Loss: 3.7601, Avg. Validate Sample Loss: 3.0582,                             L2 Loss: 0.5173\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [11/5000] -------------------- \n",
      "Epoch [11/5000], Step [1/47], Loss: 3.9070\n",
      "Epoch [11/5000], Step [9/47], Loss: 2.4110\n",
      "Epoch [11/5000], Step [18/47], Loss: 4.5347\n",
      "Epoch [11/5000], Step [27/47], Loss: 3.2214\n",
      "Epoch [11/5000], Step [36/47], Loss: 2.2374\n",
      "Epoch [11/5000], Step [45/47], Loss: 1.7767\n",
      "Epoch [11/5000], Avg. Train Sample Loss: 2.9373, Avg. Validate Sample Loss: 2.4182,                             L2 Loss: 0.4463\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [12/5000] -------------------- \n",
      "Epoch [12/5000], Step [1/47], Loss: 4.9639\n",
      "Epoch [12/5000], Step [9/47], Loss: 2.4859\n",
      "Epoch [12/5000], Step [18/47], Loss: 3.1447\n",
      "Epoch [12/5000], Step [27/47], Loss: 2.5194\n",
      "Epoch [12/5000], Step [36/47], Loss: 2.3364\n",
      "Epoch [12/5000], Step [45/47], Loss: 2.6195\n",
      "Epoch [12/5000], Avg. Train Sample Loss: 2.4265, Avg. Validate Sample Loss: 2.5528,                             L2 Loss: 0.4012\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [13/5000] -------------------- \n",
      "Epoch [13/5000], Step [1/47], Loss: 2.6868\n",
      "Epoch [13/5000], Step [9/47], Loss: 1.8835\n",
      "Epoch [13/5000], Step [18/47], Loss: 2.3163\n",
      "Epoch [13/5000], Step [27/47], Loss: 1.5543\n",
      "Epoch [13/5000], Step [36/47], Loss: 2.2505\n",
      "Epoch [13/5000], Step [45/47], Loss: 1.9389\n",
      "Epoch [13/5000], Avg. Train Sample Loss: 2.1357, Avg. Validate Sample Loss: 1.8173,                             L2 Loss: 0.3530\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [14/5000] -------------------- \n",
      "Epoch [14/5000], Step [1/47], Loss: 1.6361\n",
      "Epoch [14/5000], Step [9/47], Loss: 2.1616\n",
      "Epoch [14/5000], Step [18/47], Loss: 1.5132\n",
      "Epoch [14/5000], Step [27/47], Loss: 1.7171\n",
      "Epoch [14/5000], Step [36/47], Loss: 1.7987\n",
      "Epoch [14/5000], Step [45/47], Loss: 2.1170\n",
      "Epoch [14/5000], Avg. Train Sample Loss: 1.8839, Avg. Validate Sample Loss: 1.7738,                             L2 Loss: 0.3288\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [15/5000] -------------------- \n",
      "Epoch [15/5000], Step [1/47], Loss: 1.5776\n",
      "Epoch [15/5000], Step [9/47], Loss: 2.0539\n",
      "Epoch [15/5000], Step [18/47], Loss: 1.6981\n",
      "Epoch [15/5000], Step [27/47], Loss: 1.4913\n",
      "Epoch [15/5000], Step [36/47], Loss: 1.9455\n",
      "Epoch [15/5000], Step [45/47], Loss: 1.3683\n",
      "Epoch [15/5000], Avg. Train Sample Loss: 1.7907, Avg. Validate Sample Loss: 1.7195,                             L2 Loss: 0.3049\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [16/5000] -------------------- \n",
      "Epoch [16/5000], Step [1/47], Loss: 1.5967\n",
      "Epoch [16/5000], Step [9/47], Loss: 1.6981\n",
      "Epoch [16/5000], Step [18/47], Loss: 1.8564\n",
      "Epoch [16/5000], Step [27/47], Loss: 1.4281\n",
      "Epoch [16/5000], Step [36/47], Loss: 1.3606\n",
      "Epoch [16/5000], Step [45/47], Loss: 1.8766\n",
      "Epoch [16/5000], Avg. Train Sample Loss: 1.6324, Avg. Validate Sample Loss: 1.6439,                             L2 Loss: 0.2888\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [17/5000] -------------------- \n",
      "Epoch [17/5000], Step [1/47], Loss: 1.4484\n",
      "Epoch [17/5000], Step [9/47], Loss: 1.1967\n",
      "Epoch [17/5000], Step [18/47], Loss: 1.6063\n",
      "Epoch [17/5000], Step [27/47], Loss: 1.6467\n",
      "Epoch [17/5000], Step [36/47], Loss: 1.8844\n",
      "Epoch [17/5000], Step [45/47], Loss: 1.4469\n",
      "Epoch [17/5000], Avg. Train Sample Loss: 1.5549, Avg. Validate Sample Loss: 1.4309,                             L2 Loss: 0.2755\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [18/5000] -------------------- \n",
      "Epoch [18/5000], Step [1/47], Loss: 1.2116\n",
      "Epoch [18/5000], Step [9/47], Loss: 1.2121\n",
      "Epoch [18/5000], Step [18/47], Loss: 1.5005\n",
      "Epoch [18/5000], Step [27/47], Loss: 1.4788\n",
      "Epoch [18/5000], Step [36/47], Loss: 1.1825\n",
      "Epoch [18/5000], Step [45/47], Loss: 1.4595\n",
      "Epoch [18/5000], Avg. Train Sample Loss: 1.5213, Avg. Validate Sample Loss: 1.3049,                             L2 Loss: 0.2647\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [19/5000] -------------------- \n",
      "Epoch [19/5000], Step [1/47], Loss: 1.3052\n",
      "Epoch [19/5000], Step [9/47], Loss: 1.5057\n",
      "Epoch [19/5000], Step [18/47], Loss: 1.4083\n",
      "Epoch [19/5000], Step [27/47], Loss: 1.2349\n",
      "Epoch [19/5000], Step [36/47], Loss: 1.3508\n",
      "Epoch [19/5000], Step [45/47], Loss: 1.2859\n",
      "Epoch [19/5000], Avg. Train Sample Loss: 1.4293, Avg. Validate Sample Loss: 1.3443,                             L2 Loss: 0.2555\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [20/5000] -------------------- \n",
      "Epoch [20/5000], Step [1/47], Loss: 1.2522\n",
      "Epoch [20/5000], Step [9/47], Loss: 1.5781\n",
      "Epoch [20/5000], Step [18/47], Loss: 0.8999\n",
      "Epoch [20/5000], Step [27/47], Loss: 1.2419\n",
      "Epoch [20/5000], Step [36/47], Loss: 1.5347\n",
      "Epoch [20/5000], Step [45/47], Loss: 1.1349\n",
      "Epoch [20/5000], Avg. Train Sample Loss: 1.3687, Avg. Validate Sample Loss: 1.3994,                             L2 Loss: 0.2459\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [21/5000] -------------------- \n",
      "Epoch [21/5000], Step [1/47], Loss: 1.3804\n",
      "Epoch [21/5000], Step [9/47], Loss: 1.4009\n",
      "Epoch [21/5000], Step [18/47], Loss: 1.4915\n",
      "Epoch [21/5000], Step [27/47], Loss: 1.3501\n",
      "Epoch [21/5000], Step [36/47], Loss: 1.7101\n",
      "Epoch [21/5000], Step [45/47], Loss: 1.1739\n",
      "Epoch [21/5000], Avg. Train Sample Loss: 1.3617, Avg. Validate Sample Loss: 1.3131,                             L2 Loss: 0.2382\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [22/5000] -------------------- \n",
      "Epoch [22/5000], Step [1/47], Loss: 1.0978\n",
      "Epoch [22/5000], Step [9/47], Loss: 1.1867\n",
      "Epoch [22/5000], Step [18/47], Loss: 1.4908\n",
      "Epoch [22/5000], Step [27/47], Loss: 1.1087\n",
      "Epoch [22/5000], Step [36/47], Loss: 1.1081\n",
      "Epoch [22/5000], Step [45/47], Loss: 1.3363\n",
      "Epoch [22/5000], Avg. Train Sample Loss: 1.2989, Avg. Validate Sample Loss: 1.2274,                             L2 Loss: 0.2307\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [23/5000] -------------------- \n",
      "Epoch [23/5000], Step [1/47], Loss: 1.2890\n",
      "Epoch [23/5000], Step [9/47], Loss: 1.0694\n",
      "Epoch [23/5000], Step [18/47], Loss: 1.1972\n",
      "Epoch [23/5000], Step [27/47], Loss: 1.0314\n",
      "Epoch [23/5000], Step [36/47], Loss: 1.4350\n",
      "Epoch [23/5000], Step [45/47], Loss: 1.0864\n",
      "Epoch [23/5000], Avg. Train Sample Loss: 1.2540, Avg. Validate Sample Loss: 1.1924,                             L2 Loss: 0.2237\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [24/5000] -------------------- \n",
      "Epoch [24/5000], Step [1/47], Loss: 1.2289\n",
      "Epoch [24/5000], Step [9/47], Loss: 1.5680\n",
      "Epoch [24/5000], Step [18/47], Loss: 1.3003\n",
      "Epoch [24/5000], Step [27/47], Loss: 0.8888\n",
      "Epoch [24/5000], Step [36/47], Loss: 1.1910\n",
      "Epoch [24/5000], Step [45/47], Loss: 1.5557\n",
      "Epoch [24/5000], Avg. Train Sample Loss: 1.2079, Avg. Validate Sample Loss: 1.2920,                             L2 Loss: 0.2208\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [25/5000] -------------------- \n",
      "Epoch [25/5000], Step [1/47], Loss: 1.1364\n",
      "Epoch [25/5000], Step [9/47], Loss: 1.1158\n",
      "Epoch [25/5000], Step [18/47], Loss: 1.3744\n",
      "Epoch [25/5000], Step [27/47], Loss: 1.1009\n",
      "Epoch [25/5000], Step [36/47], Loss: 1.3730\n",
      "Epoch [25/5000], Step [45/47], Loss: 1.1612\n",
      "Epoch [25/5000], Avg. Train Sample Loss: 1.1765, Avg. Validate Sample Loss: 1.2041,                             L2 Loss: 0.2130\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [26/5000] -------------------- \n",
      "Epoch [26/5000], Step [1/47], Loss: 1.1007\n",
      "Epoch [26/5000], Step [9/47], Loss: 0.8051\n",
      "Epoch [26/5000], Step [18/47], Loss: 1.0462\n",
      "Epoch [26/5000], Step [27/47], Loss: 1.0217\n",
      "Epoch [26/5000], Step [36/47], Loss: 1.2338\n",
      "Epoch [26/5000], Step [45/47], Loss: 1.1883\n",
      "Epoch [26/5000], Avg. Train Sample Loss: 1.1101, Avg. Validate Sample Loss: 1.1090,                             L2 Loss: 0.2038\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [27/5000] -------------------- \n",
      "Epoch [27/5000], Step [1/47], Loss: 0.9480\n",
      "Epoch [27/5000], Step [9/47], Loss: 1.2303\n",
      "Epoch [27/5000], Step [18/47], Loss: 0.9955\n",
      "Epoch [27/5000], Step [27/47], Loss: 1.0190\n",
      "Epoch [27/5000], Step [36/47], Loss: 1.0965\n",
      "Epoch [27/5000], Step [45/47], Loss: 1.2819\n",
      "Epoch [27/5000], Avg. Train Sample Loss: 1.0841, Avg. Validate Sample Loss: 1.0021,                             L2 Loss: 0.2000\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [28/5000] -------------------- \n",
      "Epoch [28/5000], Step [1/47], Loss: 1.0192\n",
      "Epoch [28/5000], Step [9/47], Loss: 1.0859\n",
      "Epoch [28/5000], Step [18/47], Loss: 0.7650\n",
      "Epoch [28/5000], Step [27/47], Loss: 1.3224\n",
      "Epoch [28/5000], Step [36/47], Loss: 0.8938\n",
      "Epoch [28/5000], Step [45/47], Loss: 1.2758\n",
      "Epoch [28/5000], Avg. Train Sample Loss: 1.0436, Avg. Validate Sample Loss: 1.0604,                             L2 Loss: 0.1922\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [29/5000] -------------------- \n",
      "Epoch [29/5000], Step [1/47], Loss: 0.8933\n",
      "Epoch [29/5000], Step [9/47], Loss: 0.8345\n",
      "Epoch [29/5000], Step [18/47], Loss: 1.0754\n",
      "Epoch [29/5000], Step [27/47], Loss: 1.0312\n",
      "Epoch [29/5000], Step [36/47], Loss: 1.1790\n",
      "Epoch [29/5000], Step [45/47], Loss: 0.8348\n",
      "Epoch [29/5000], Avg. Train Sample Loss: 1.0151, Avg. Validate Sample Loss: 1.0121,                             L2 Loss: 0.1875\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [30/5000] -------------------- \n",
      "Epoch [30/5000], Step [1/47], Loss: 1.2363\n",
      "Epoch [30/5000], Step [9/47], Loss: 0.9137\n",
      "Epoch [30/5000], Step [18/47], Loss: 0.7712\n",
      "Epoch [30/5000], Step [27/47], Loss: 0.8073\n",
      "Epoch [30/5000], Step [36/47], Loss: 1.0445\n",
      "Epoch [30/5000], Step [45/47], Loss: 0.7449\n",
      "Epoch [30/5000], Avg. Train Sample Loss: 0.9839, Avg. Validate Sample Loss: 0.9843,                             L2 Loss: 0.1830\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [31/5000] -------------------- \n",
      "Epoch [31/5000], Step [1/47], Loss: 1.2503\n",
      "Epoch [31/5000], Step [9/47], Loss: 0.8873\n",
      "Epoch [31/5000], Step [18/47], Loss: 1.0158\n",
      "Epoch [31/5000], Step [27/47], Loss: 0.8824\n",
      "Epoch [31/5000], Step [36/47], Loss: 0.8716\n",
      "Epoch [31/5000], Step [45/47], Loss: 0.8383\n",
      "Epoch [31/5000], Avg. Train Sample Loss: 0.9714, Avg. Validate Sample Loss: 0.9079,                             L2 Loss: 0.1797\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [32/5000] -------------------- \n",
      "Epoch [32/5000], Step [1/47], Loss: 0.9532\n",
      "Epoch [32/5000], Step [9/47], Loss: 0.7798\n",
      "Epoch [32/5000], Step [18/47], Loss: 1.0196\n",
      "Epoch [32/5000], Step [27/47], Loss: 1.0597\n",
      "Epoch [32/5000], Step [36/47], Loss: 1.2082\n",
      "Epoch [32/5000], Step [45/47], Loss: 0.8736\n",
      "Epoch [32/5000], Avg. Train Sample Loss: 0.9253, Avg. Validate Sample Loss: 0.9680,                             L2 Loss: 0.1737\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [33/5000] -------------------- \n",
      "Epoch [33/5000], Step [1/47], Loss: 0.9336\n",
      "Epoch [33/5000], Step [9/47], Loss: 0.8493\n",
      "Epoch [33/5000], Step [18/47], Loss: 0.6912\n",
      "Epoch [33/5000], Step [27/47], Loss: 1.3284\n",
      "Epoch [33/5000], Step [36/47], Loss: 0.8402\n",
      "Epoch [33/5000], Step [45/47], Loss: 0.8076\n",
      "Epoch [33/5000], Avg. Train Sample Loss: 0.9079, Avg. Validate Sample Loss: 0.9743,                             L2 Loss: 0.1737\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [34/5000] -------------------- \n",
      "Epoch [34/5000], Step [1/47], Loss: 0.8395\n",
      "Epoch [34/5000], Step [9/47], Loss: 0.5788\n",
      "Epoch [34/5000], Step [18/47], Loss: 0.7804\n",
      "Epoch [34/5000], Step [27/47], Loss: 1.0472\n",
      "Epoch [34/5000], Step [36/47], Loss: 0.6711\n",
      "Epoch [34/5000], Step [45/47], Loss: 0.8690\n",
      "Epoch [34/5000], Avg. Train Sample Loss: 0.9035, Avg. Validate Sample Loss: 0.9972,                             L2 Loss: 0.1748\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [35/5000] -------------------- \n",
      "Epoch [35/5000], Step [1/47], Loss: 1.1197\n",
      "Epoch [35/5000], Step [9/47], Loss: 0.9409\n",
      "Epoch [35/5000], Step [18/47], Loss: 0.9617\n",
      "Epoch [35/5000], Step [27/47], Loss: 0.9902\n",
      "Epoch [35/5000], Step [36/47], Loss: 1.1182\n",
      "Epoch [35/5000], Step [45/47], Loss: 0.9139\n",
      "Epoch [35/5000], Avg. Train Sample Loss: 0.8495, Avg. Validate Sample Loss: 0.8715,                             L2 Loss: 0.1630\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [36/5000] -------------------- \n",
      "Epoch [36/5000], Step [1/47], Loss: 0.9317\n",
      "Epoch [36/5000], Step [9/47], Loss: 0.6743\n",
      "Epoch [36/5000], Step [18/47], Loss: 1.0380\n",
      "Epoch [36/5000], Step [27/47], Loss: 0.5427\n",
      "Epoch [36/5000], Step [36/47], Loss: 0.6570\n",
      "Epoch [36/5000], Step [45/47], Loss: 0.7307\n",
      "Epoch [36/5000], Avg. Train Sample Loss: 0.8133, Avg. Validate Sample Loss: 0.7513,                             L2 Loss: 0.1561\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [37/5000] -------------------- \n",
      "Epoch [37/5000], Step [1/47], Loss: 0.8460\n",
      "Epoch [37/5000], Step [9/47], Loss: 0.8598\n",
      "Epoch [37/5000], Step [18/47], Loss: 0.8881\n",
      "Epoch [37/5000], Step [27/47], Loss: 0.7999\n",
      "Epoch [37/5000], Step [36/47], Loss: 0.7480\n",
      "Epoch [37/5000], Step [45/47], Loss: 0.6720\n",
      "Epoch [37/5000], Avg. Train Sample Loss: 0.7954, Avg. Validate Sample Loss: 0.7386,                             L2 Loss: 0.1519\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [38/5000] -------------------- \n",
      "Epoch [38/5000], Step [1/47], Loss: 0.7443\n",
      "Epoch [38/5000], Step [9/47], Loss: 0.6045\n",
      "Epoch [38/5000], Step [18/47], Loss: 0.7217\n",
      "Epoch [38/5000], Step [27/47], Loss: 0.9752\n",
      "Epoch [38/5000], Step [36/47], Loss: 0.9357\n",
      "Epoch [38/5000], Step [45/47], Loss: 0.9250\n",
      "Epoch [38/5000], Avg. Train Sample Loss: 0.7638, Avg. Validate Sample Loss: 0.7117,                             L2 Loss: 0.1462\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [39/5000] -------------------- \n",
      "Epoch [39/5000], Step [1/47], Loss: 0.8565\n",
      "Epoch [39/5000], Step [9/47], Loss: 0.6880\n",
      "Epoch [39/5000], Step [18/47], Loss: 0.6736\n",
      "Epoch [39/5000], Step [27/47], Loss: 0.8958\n",
      "Epoch [39/5000], Step [36/47], Loss: 0.6503\n",
      "Epoch [39/5000], Step [45/47], Loss: 0.5874\n",
      "Epoch [39/5000], Avg. Train Sample Loss: 0.7307, Avg. Validate Sample Loss: 0.6485,                             L2 Loss: 0.1422\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [40/5000] -------------------- \n",
      "Epoch [40/5000], Step [1/47], Loss: 0.6714\n",
      "Epoch [40/5000], Step [9/47], Loss: 0.6508\n",
      "Epoch [40/5000], Step [18/47], Loss: 0.6086\n",
      "Epoch [40/5000], Step [27/47], Loss: 0.7026\n",
      "Epoch [40/5000], Step [36/47], Loss: 0.8149\n",
      "Epoch [40/5000], Step [45/47], Loss: 0.4298\n",
      "Epoch [40/5000], Avg. Train Sample Loss: 0.7136, Avg. Validate Sample Loss: 0.6320,                             L2 Loss: 0.1383\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [41/5000] -------------------- \n",
      "Epoch [41/5000], Step [1/47], Loss: 0.8562\n",
      "Epoch [41/5000], Step [9/47], Loss: 0.8848\n",
      "Epoch [41/5000], Step [18/47], Loss: 0.5120\n",
      "Epoch [41/5000], Step [27/47], Loss: 0.9799\n",
      "Epoch [41/5000], Step [36/47], Loss: 0.7303\n",
      "Epoch [41/5000], Step [45/47], Loss: 0.5663\n",
      "Epoch [41/5000], Avg. Train Sample Loss: 0.6818, Avg. Validate Sample Loss: 0.7113,                             L2 Loss: 0.1335\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [42/5000] -------------------- \n",
      "Epoch [42/5000], Step [1/47], Loss: 0.5599\n",
      "Epoch [42/5000], Step [9/47], Loss: 0.5711\n",
      "Epoch [42/5000], Step [18/47], Loss: 0.3994\n",
      "Epoch [42/5000], Step [27/47], Loss: 0.9875\n",
      "Epoch [42/5000], Step [36/47], Loss: 0.5274\n",
      "Epoch [42/5000], Step [45/47], Loss: 0.6652\n",
      "Epoch [42/5000], Avg. Train Sample Loss: 0.6618, Avg. Validate Sample Loss: 0.6314,                             L2 Loss: 0.1292\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [43/5000] -------------------- \n",
      "Epoch [43/5000], Step [1/47], Loss: 0.6756\n",
      "Epoch [43/5000], Step [9/47], Loss: 0.6246\n",
      "Epoch [43/5000], Step [18/47], Loss: 0.4533\n",
      "Epoch [43/5000], Step [27/47], Loss: 0.7707\n",
      "Epoch [43/5000], Step [36/47], Loss: 0.5341\n",
      "Epoch [43/5000], Step [45/47], Loss: 0.3876\n",
      "Epoch [43/5000], Avg. Train Sample Loss: 0.6377, Avg. Validate Sample Loss: 0.6771,                             L2 Loss: 0.1311\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [44/5000] -------------------- \n",
      "Epoch [44/5000], Step [1/47], Loss: 0.7467\n",
      "Epoch [44/5000], Step [9/47], Loss: 0.4169\n",
      "Epoch [44/5000], Step [18/47], Loss: 0.7120\n",
      "Epoch [44/5000], Step [27/47], Loss: 0.6910\n",
      "Epoch [44/5000], Step [36/47], Loss: 0.5083\n",
      "Epoch [44/5000], Step [45/47], Loss: 0.5924\n",
      "Epoch [44/5000], Avg. Train Sample Loss: 0.6219, Avg. Validate Sample Loss: 0.6154,                             L2 Loss: 0.1229\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [45/5000] -------------------- \n",
      "Epoch [45/5000], Step [1/47], Loss: 0.4523\n",
      "Epoch [45/5000], Step [9/47], Loss: 0.6033\n",
      "Epoch [45/5000], Step [18/47], Loss: 0.4184\n",
      "Epoch [45/5000], Step [27/47], Loss: 0.7400\n",
      "Epoch [45/5000], Step [36/47], Loss: 0.7716\n",
      "Epoch [45/5000], Step [45/47], Loss: 0.7566\n",
      "Epoch [45/5000], Avg. Train Sample Loss: 0.6103, Avg. Validate Sample Loss: 0.6948,                             L2 Loss: 0.1269\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [46/5000] -------------------- \n",
      "Epoch [46/5000], Step [1/47], Loss: 0.5993\n",
      "Epoch [46/5000], Step [9/47], Loss: 0.4997\n",
      "Epoch [46/5000], Step [18/47], Loss: 0.6249\n",
      "Epoch [46/5000], Step [27/47], Loss: 0.5747\n",
      "Epoch [46/5000], Step [36/47], Loss: 0.6146\n",
      "Epoch [46/5000], Step [45/47], Loss: 0.4234\n",
      "Epoch [46/5000], Avg. Train Sample Loss: 0.5857, Avg. Validate Sample Loss: 0.5641,                             L2 Loss: 0.1136\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [47/5000] -------------------- \n",
      "Epoch [47/5000], Step [1/47], Loss: 0.8246\n",
      "Epoch [47/5000], Step [9/47], Loss: 0.6785\n",
      "Epoch [47/5000], Step [18/47], Loss: 0.3995\n",
      "Epoch [47/5000], Step [27/47], Loss: 0.4607\n",
      "Epoch [47/5000], Step [36/47], Loss: 0.6128\n",
      "Epoch [47/5000], Step [45/47], Loss: 0.6737\n",
      "Epoch [47/5000], Avg. Train Sample Loss: 0.5572, Avg. Validate Sample Loss: 0.5498,                             L2 Loss: 0.1109\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [48/5000] -------------------- \n",
      "Epoch [48/5000], Step [1/47], Loss: 0.7374\n",
      "Epoch [48/5000], Step [9/47], Loss: 0.4282\n",
      "Epoch [48/5000], Step [18/47], Loss: 0.4561\n",
      "Epoch [48/5000], Step [27/47], Loss: 0.8280\n",
      "Epoch [48/5000], Step [36/47], Loss: 0.3400\n",
      "Epoch [48/5000], Step [45/47], Loss: 0.5072\n",
      "Epoch [48/5000], Avg. Train Sample Loss: 0.5543, Avg. Validate Sample Loss: 0.5100,                             L2 Loss: 0.1081\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [49/5000] -------------------- \n",
      "Epoch [49/5000], Step [1/47], Loss: 0.5419\n",
      "Epoch [49/5000], Step [9/47], Loss: 0.5855\n",
      "Epoch [49/5000], Step [18/47], Loss: 0.4764\n",
      "Epoch [49/5000], Step [27/47], Loss: 0.6462\n",
      "Epoch [49/5000], Step [36/47], Loss: 0.4172\n",
      "Epoch [49/5000], Step [45/47], Loss: 0.4396\n",
      "Epoch [49/5000], Avg. Train Sample Loss: 0.5203, Avg. Validate Sample Loss: 0.5069,                             L2 Loss: 0.1036\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [50/5000] -------------------- \n",
      "Epoch [50/5000], Step [1/47], Loss: 0.6468\n",
      "Epoch [50/5000], Step [9/47], Loss: 0.6013\n",
      "Epoch [50/5000], Step [18/47], Loss: 0.5870\n",
      "Epoch [50/5000], Step [27/47], Loss: 0.5606\n",
      "Epoch [50/5000], Step [36/47], Loss: 0.4538\n",
      "Epoch [50/5000], Step [45/47], Loss: 0.5489\n",
      "Epoch [50/5000], Avg. Train Sample Loss: 0.5294, Avg. Validate Sample Loss: 0.4786,                             L2 Loss: 0.1005\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [51/5000] -------------------- \n",
      "Epoch [51/5000], Step [1/47], Loss: 0.6558\n",
      "Epoch [51/5000], Step [9/47], Loss: 0.4797\n",
      "Epoch [51/5000], Step [18/47], Loss: 0.5333\n",
      "Epoch [51/5000], Step [27/47], Loss: 0.6284\n",
      "Epoch [51/5000], Step [36/47], Loss: 0.3813\n",
      "Epoch [51/5000], Step [45/47], Loss: 0.5027\n",
      "Epoch [51/5000], Avg. Train Sample Loss: 0.4928, Avg. Validate Sample Loss: 0.4551,                             L2 Loss: 0.0976\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [52/5000] -------------------- \n",
      "Epoch [52/5000], Step [1/47], Loss: 0.6405\n",
      "Epoch [52/5000], Step [9/47], Loss: 0.3445\n",
      "Epoch [52/5000], Step [18/47], Loss: 0.3689\n",
      "Epoch [52/5000], Step [27/47], Loss: 0.6861\n",
      "Epoch [52/5000], Step [36/47], Loss: 0.4414\n",
      "Epoch [52/5000], Step [45/47], Loss: 0.7979\n",
      "Epoch [52/5000], Avg. Train Sample Loss: 0.4735, Avg. Validate Sample Loss: 0.4709,                             L2 Loss: 0.0966\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [53/5000] -------------------- \n",
      "Epoch [53/5000], Step [1/47], Loss: 0.4869\n",
      "Epoch [53/5000], Step [9/47], Loss: 0.5262\n",
      "Epoch [53/5000], Step [18/47], Loss: 0.4731\n",
      "Epoch [53/5000], Step [27/47], Loss: 0.3801\n",
      "Epoch [53/5000], Step [36/47], Loss: 0.4216\n",
      "Epoch [53/5000], Step [45/47], Loss: 0.4470\n",
      "Epoch [53/5000], Avg. Train Sample Loss: 0.4645, Avg. Validate Sample Loss: 0.4469,                             L2 Loss: 0.0929\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [54/5000] -------------------- \n",
      "Epoch [54/5000], Step [1/47], Loss: 0.5999\n",
      "Epoch [54/5000], Step [9/47], Loss: 0.6665\n",
      "Epoch [54/5000], Step [18/47], Loss: 0.6689\n",
      "Epoch [54/5000], Step [27/47], Loss: 0.5251\n",
      "Epoch [54/5000], Step [36/47], Loss: 0.5397\n",
      "Epoch [54/5000], Step [45/47], Loss: 0.6019\n",
      "Epoch [54/5000], Avg. Train Sample Loss: 0.4620, Avg. Validate Sample Loss: 0.4833,                             L2 Loss: 0.0911\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [55/5000] -------------------- \n",
      "Epoch [55/5000], Step [1/47], Loss: 0.6625\n",
      "Epoch [55/5000], Step [9/47], Loss: 0.4780\n",
      "Epoch [55/5000], Step [18/47], Loss: 0.4143\n",
      "Epoch [55/5000], Step [27/47], Loss: 0.4048\n",
      "Epoch [55/5000], Step [36/47], Loss: 0.4204\n",
      "Epoch [55/5000], Step [45/47], Loss: 0.3191\n",
      "Epoch [55/5000], Avg. Train Sample Loss: 0.4431, Avg. Validate Sample Loss: 0.4518,                             L2 Loss: 0.0896\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [56/5000] -------------------- \n",
      "Epoch [56/5000], Step [1/47], Loss: 0.4942\n",
      "Epoch [56/5000], Step [9/47], Loss: 0.2708\n",
      "Epoch [56/5000], Step [18/47], Loss: 0.4772\n",
      "Epoch [56/5000], Step [27/47], Loss: 0.5028\n",
      "Epoch [56/5000], Step [36/47], Loss: 0.5756\n",
      "Epoch [56/5000], Step [45/47], Loss: 0.4818\n",
      "Epoch [56/5000], Avg. Train Sample Loss: 0.4388, Avg. Validate Sample Loss: 0.5207,                             L2 Loss: 0.0947\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [57/5000] -------------------- \n",
      "Epoch [57/5000], Step [1/47], Loss: 0.4631\n",
      "Epoch [57/5000], Step [9/47], Loss: 0.5446\n",
      "Epoch [57/5000], Step [18/47], Loss: 0.4766\n",
      "Epoch [57/5000], Step [27/47], Loss: 0.4279\n",
      "Epoch [57/5000], Step [36/47], Loss: 0.4454\n",
      "Epoch [57/5000], Step [45/47], Loss: 0.3971\n",
      "Epoch [57/5000], Avg. Train Sample Loss: 0.4370, Avg. Validate Sample Loss: 0.4249,                             L2 Loss: 0.0843\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [58/5000] -------------------- \n",
      "Epoch [58/5000], Step [1/47], Loss: 0.4037\n",
      "Epoch [58/5000], Step [9/47], Loss: 0.4951\n",
      "Epoch [58/5000], Step [18/47], Loss: 0.4519\n",
      "Epoch [58/5000], Step [27/47], Loss: 0.2650\n",
      "Epoch [58/5000], Step [36/47], Loss: 0.4064\n",
      "Epoch [58/5000], Step [45/47], Loss: 0.3094\n",
      "Epoch [58/5000], Avg. Train Sample Loss: 0.4271, Avg. Validate Sample Loss: 0.4288,                             L2 Loss: 0.0831\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [59/5000] -------------------- \n",
      "Epoch [59/5000], Step [1/47], Loss: 0.4275\n",
      "Epoch [59/5000], Step [9/47], Loss: 0.2494\n",
      "Epoch [59/5000], Step [18/47], Loss: 0.3722\n",
      "Epoch [59/5000], Step [27/47], Loss: 0.4575\n",
      "Epoch [59/5000], Step [36/47], Loss: 0.2912\n",
      "Epoch [59/5000], Step [45/47], Loss: 0.2869\n",
      "Epoch [59/5000], Avg. Train Sample Loss: 0.4057, Avg. Validate Sample Loss: 0.4389,                             L2 Loss: 0.0804\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [60/5000] -------------------- \n",
      "Epoch [60/5000], Step [1/47], Loss: 0.2805\n",
      "Epoch [60/5000], Step [9/47], Loss: 0.5188\n",
      "Epoch [60/5000], Step [18/47], Loss: 0.4116\n",
      "Epoch [60/5000], Step [27/47], Loss: 0.5248\n",
      "Epoch [60/5000], Step [36/47], Loss: 0.6744\n",
      "Epoch [60/5000], Step [45/47], Loss: 0.4101\n",
      "Epoch [60/5000], Avg. Train Sample Loss: 0.4117, Avg. Validate Sample Loss: 0.3825,                             L2 Loss: 0.0794\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [61/5000] -------------------- \n",
      "Epoch [61/5000], Step [1/47], Loss: 0.3129\n",
      "Epoch [61/5000], Step [9/47], Loss: 0.3411\n",
      "Epoch [61/5000], Step [18/47], Loss: 0.3744\n",
      "Epoch [61/5000], Step [27/47], Loss: 0.4461\n",
      "Epoch [61/5000], Step [36/47], Loss: 0.2877\n",
      "Epoch [61/5000], Step [45/47], Loss: 0.4439\n",
      "Epoch [61/5000], Avg. Train Sample Loss: 0.4116, Avg. Validate Sample Loss: 0.3784,                             L2 Loss: 0.0763\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [62/5000] -------------------- \n",
      "Epoch [62/5000], Step [1/47], Loss: 0.4719\n",
      "Epoch [62/5000], Step [9/47], Loss: 0.3157\n",
      "Epoch [62/5000], Step [18/47], Loss: 0.2741\n",
      "Epoch [62/5000], Step [27/47], Loss: 0.3713\n",
      "Epoch [62/5000], Step [36/47], Loss: 0.3328\n",
      "Epoch [62/5000], Step [45/47], Loss: 0.4408\n",
      "Epoch [62/5000], Avg. Train Sample Loss: 0.3889, Avg. Validate Sample Loss: 0.4213,                             L2 Loss: 0.0751\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [63/5000] -------------------- \n",
      "Epoch [63/5000], Step [1/47], Loss: 0.2916\n",
      "Epoch [63/5000], Step [9/47], Loss: 0.4143\n",
      "Epoch [63/5000], Step [18/47], Loss: 0.4633\n",
      "Epoch [63/5000], Step [27/47], Loss: 0.2592\n",
      "Epoch [63/5000], Step [36/47], Loss: 0.5819\n",
      "Epoch [63/5000], Step [45/47], Loss: 0.3211\n",
      "Epoch [63/5000], Avg. Train Sample Loss: 0.3911, Avg. Validate Sample Loss: 0.4531,                             L2 Loss: 0.0823\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [64/5000] -------------------- \n",
      "Epoch [64/5000], Step [1/47], Loss: 0.3563\n",
      "Epoch [64/5000], Step [9/47], Loss: 0.3962\n",
      "Epoch [64/5000], Step [18/47], Loss: 0.3034\n",
      "Epoch [64/5000], Step [27/47], Loss: 0.5781\n",
      "Epoch [64/5000], Step [36/47], Loss: 0.5285\n",
      "Epoch [64/5000], Step [45/47], Loss: 0.6345\n",
      "Epoch [64/5000], Avg. Train Sample Loss: 0.3861, Avg. Validate Sample Loss: 0.4026,                             L2 Loss: 0.0737\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [65/5000] -------------------- \n",
      "Epoch [65/5000], Step [1/47], Loss: 0.3511\n",
      "Epoch [65/5000], Step [9/47], Loss: 0.3425\n",
      "Epoch [65/5000], Step [18/47], Loss: 0.4496\n",
      "Epoch [65/5000], Step [27/47], Loss: 0.3039\n",
      "Epoch [65/5000], Step [36/47], Loss: 0.3312\n",
      "Epoch [65/5000], Step [45/47], Loss: 0.4233\n",
      "Epoch [65/5000], Avg. Train Sample Loss: 0.3772, Avg. Validate Sample Loss: 0.4247,                             L2 Loss: 0.0743\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [66/5000] -------------------- \n",
      "Epoch [66/5000], Step [1/47], Loss: 0.3843\n",
      "Epoch [66/5000], Step [9/47], Loss: 0.2877\n",
      "Epoch [66/5000], Step [18/47], Loss: 0.5084\n",
      "Epoch [66/5000], Step [27/47], Loss: 0.6214\n",
      "Epoch [66/5000], Step [36/47], Loss: 0.3220\n",
      "Epoch [66/5000], Step [45/47], Loss: 0.3978\n",
      "Epoch [66/5000], Avg. Train Sample Loss: 0.3734, Avg. Validate Sample Loss: 0.3651,                             L2 Loss: 0.0812\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [67/5000] -------------------- \n",
      "Epoch [67/5000], Step [1/47], Loss: 0.5934\n",
      "Epoch [67/5000], Step [9/47], Loss: 0.2135\n",
      "Epoch [67/5000], Step [18/47], Loss: 0.2998\n",
      "Epoch [67/5000], Step [27/47], Loss: 0.4453\n",
      "Epoch [67/5000], Step [36/47], Loss: 0.4539\n",
      "Epoch [67/5000], Step [45/47], Loss: 0.2860\n",
      "Epoch [67/5000], Avg. Train Sample Loss: 0.3772, Avg. Validate Sample Loss: 0.3716,                             L2 Loss: 0.0690\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [68/5000] -------------------- \n",
      "Epoch [68/5000], Step [1/47], Loss: 0.4116\n",
      "Epoch [68/5000], Step [9/47], Loss: 0.3721\n",
      "Epoch [68/5000], Step [18/47], Loss: 0.4116\n",
      "Epoch [68/5000], Step [27/47], Loss: 0.3621\n",
      "Epoch [68/5000], Step [36/47], Loss: 0.4562\n",
      "Epoch [68/5000], Step [45/47], Loss: 0.3346\n",
      "Epoch [68/5000], Avg. Train Sample Loss: 0.3738, Avg. Validate Sample Loss: 0.3009,                             L2 Loss: 0.0678\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [69/5000] -------------------- \n",
      "Epoch [69/5000], Step [1/47], Loss: 0.2840\n",
      "Epoch [69/5000], Step [9/47], Loss: 0.2668\n",
      "Epoch [69/5000], Step [18/47], Loss: 0.4622\n",
      "Epoch [69/5000], Step [27/47], Loss: 0.3844\n",
      "Epoch [69/5000], Step [36/47], Loss: 0.3851\n",
      "Epoch [69/5000], Step [45/47], Loss: 0.2874\n",
      "Epoch [69/5000], Avg. Train Sample Loss: 0.3653, Avg. Validate Sample Loss: 0.3092,                             L2 Loss: 0.0687\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [70/5000] -------------------- \n",
      "Epoch [70/5000], Step [1/47], Loss: 0.2298\n",
      "Epoch [70/5000], Step [9/47], Loss: 0.4508\n",
      "Epoch [70/5000], Step [18/47], Loss: 0.3751\n",
      "Epoch [70/5000], Step [27/47], Loss: 0.3488\n",
      "Epoch [70/5000], Step [36/47], Loss: 0.2746\n",
      "Epoch [70/5000], Step [45/47], Loss: 0.2932\n",
      "Epoch [70/5000], Avg. Train Sample Loss: 0.3691, Avg. Validate Sample Loss: 0.3651,                             L2 Loss: 0.0673\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [71/5000] -------------------- \n",
      "Epoch [71/5000], Step [1/47], Loss: 0.3478\n",
      "Epoch [71/5000], Step [9/47], Loss: 0.2881\n",
      "Epoch [71/5000], Step [18/47], Loss: 0.3770\n",
      "Epoch [71/5000], Step [27/47], Loss: 0.3915\n",
      "Epoch [71/5000], Step [36/47], Loss: 0.3707\n",
      "Epoch [71/5000], Step [45/47], Loss: 0.5326\n",
      "Epoch [71/5000], Avg. Train Sample Loss: 0.3608, Avg. Validate Sample Loss: 0.3599,                             L2 Loss: 0.0675\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [72/5000] -------------------- \n",
      "Epoch [72/5000], Step [1/47], Loss: 0.4911\n",
      "Epoch [72/5000], Step [9/47], Loss: 0.3505\n",
      "Epoch [72/5000], Step [18/47], Loss: 0.4816\n",
      "Epoch [72/5000], Step [27/47], Loss: 0.5135\n",
      "Epoch [72/5000], Step [36/47], Loss: 0.3770\n",
      "Epoch [72/5000], Step [45/47], Loss: 0.3100\n",
      "Epoch [72/5000], Avg. Train Sample Loss: 0.3655, Avg. Validate Sample Loss: 0.3656,                             L2 Loss: 0.0652\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [73/5000] -------------------- \n",
      "Epoch [73/5000], Step [1/47], Loss: 0.2881\n",
      "Epoch [73/5000], Step [9/47], Loss: 0.4390\n",
      "Epoch [73/5000], Step [18/47], Loss: 0.4412\n",
      "Epoch [73/5000], Step [27/47], Loss: 0.4005\n",
      "Epoch [73/5000], Step [36/47], Loss: 0.3582\n",
      "Epoch [73/5000], Step [45/47], Loss: 0.4692\n",
      "Epoch [73/5000], Avg. Train Sample Loss: 0.3499, Avg. Validate Sample Loss: 0.3539,                             L2 Loss: 0.0647\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [74/5000] -------------------- \n",
      "Epoch [74/5000], Step [1/47], Loss: 0.1870\n",
      "Epoch [74/5000], Step [9/47], Loss: 0.3094\n",
      "Epoch [74/5000], Step [18/47], Loss: 0.3372\n",
      "Epoch [74/5000], Step [27/47], Loss: 0.4710\n",
      "Epoch [74/5000], Step [36/47], Loss: 0.2977\n",
      "Epoch [74/5000], Step [45/47], Loss: 0.3450\n",
      "Epoch [74/5000], Avg. Train Sample Loss: 0.3581, Avg. Validate Sample Loss: 0.3328,                             L2 Loss: 0.0666\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [75/5000] -------------------- \n",
      "Epoch [75/5000], Step [1/47], Loss: 0.3366\n",
      "Epoch [75/5000], Step [9/47], Loss: 0.3548\n",
      "Epoch [75/5000], Step [18/47], Loss: 0.2949\n",
      "Epoch [75/5000], Step [27/47], Loss: 0.2847\n",
      "Epoch [75/5000], Step [36/47], Loss: 0.5572\n",
      "Epoch [75/5000], Step [45/47], Loss: 0.4704\n",
      "Epoch [75/5000], Avg. Train Sample Loss: 0.3544, Avg. Validate Sample Loss: 0.3513,                             L2 Loss: 0.0672\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [76/5000] -------------------- \n",
      "Epoch [76/5000], Step [1/47], Loss: 0.2526\n",
      "Epoch [76/5000], Step [9/47], Loss: 0.5561\n",
      "Epoch [76/5000], Step [18/47], Loss: 0.2095\n",
      "Epoch [76/5000], Step [27/47], Loss: 0.2773\n",
      "Epoch [76/5000], Step [36/47], Loss: 0.3276\n",
      "Epoch [76/5000], Step [45/47], Loss: 0.3776\n",
      "Epoch [76/5000], Avg. Train Sample Loss: 0.3555, Avg. Validate Sample Loss: 0.3047,                             L2 Loss: 0.0652\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [77/5000] -------------------- \n",
      "Epoch [77/5000], Step [1/47], Loss: 0.2848\n",
      "Epoch [77/5000], Step [9/47], Loss: 0.2937\n",
      "Epoch [77/5000], Step [18/47], Loss: 0.2234\n",
      "Epoch [77/5000], Step [27/47], Loss: 0.2786\n",
      "Epoch [77/5000], Step [36/47], Loss: 0.5085\n",
      "Epoch [77/5000], Step [45/47], Loss: 0.3592\n",
      "Epoch [77/5000], Avg. Train Sample Loss: 0.3502, Avg. Validate Sample Loss: 0.3496,                             L2 Loss: 0.0630\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [78/5000] -------------------- \n",
      "Epoch [78/5000], Step [1/47], Loss: 0.4442\n",
      "Epoch [78/5000], Step [9/47], Loss: 0.2609\n",
      "Epoch [78/5000], Step [18/47], Loss: 0.3486\n",
      "Epoch [78/5000], Step [27/47], Loss: 0.5303\n",
      "Epoch [78/5000], Step [36/47], Loss: 0.4342\n",
      "Epoch [78/5000], Step [45/47], Loss: 0.2334\n",
      "Epoch [78/5000], Avg. Train Sample Loss: 0.3622, Avg. Validate Sample Loss: 0.3199,                             L2 Loss: 0.0672\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [79/5000] -------------------- \n",
      "Epoch [79/5000], Step [1/47], Loss: 0.3737\n",
      "Epoch [79/5000], Step [9/47], Loss: 0.4547\n",
      "Epoch [79/5000], Step [18/47], Loss: 0.4137\n",
      "Epoch [79/5000], Step [27/47], Loss: 0.4593\n",
      "Epoch [79/5000], Step [36/47], Loss: 0.4664\n",
      "Epoch [79/5000], Step [45/47], Loss: 0.3165\n",
      "Epoch [79/5000], Avg. Train Sample Loss: 0.3576, Avg. Validate Sample Loss: 0.3399,                             L2 Loss: 0.0672\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [80/5000] -------------------- \n",
      "Epoch [80/5000], Step [1/47], Loss: 0.3481\n",
      "Epoch [80/5000], Step [9/47], Loss: 0.2813\n",
      "Epoch [80/5000], Step [18/47], Loss: 0.2917\n",
      "Epoch [80/5000], Step [27/47], Loss: 0.3013\n",
      "Epoch [80/5000], Step [36/47], Loss: 0.2034\n",
      "Epoch [80/5000], Step [45/47], Loss: 0.4525\n",
      "Epoch [80/5000], Avg. Train Sample Loss: 0.3400, Avg. Validate Sample Loss: 0.3145,                             L2 Loss: 0.0612\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [81/5000] -------------------- \n",
      "Epoch [81/5000], Step [1/47], Loss: 0.2044\n",
      "Epoch [81/5000], Step [9/47], Loss: 0.4164\n",
      "Epoch [81/5000], Step [18/47], Loss: 0.3241\n",
      "Epoch [81/5000], Step [27/47], Loss: 0.3332\n",
      "Epoch [81/5000], Step [36/47], Loss: 0.3328\n",
      "Epoch [81/5000], Step [45/47], Loss: 0.3300\n",
      "Epoch [81/5000], Avg. Train Sample Loss: 0.3380, Avg. Validate Sample Loss: 0.3873,                             L2 Loss: 0.0712\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [82/5000] -------------------- \n",
      "Epoch [82/5000], Step [1/47], Loss: 0.4260\n",
      "Epoch [82/5000], Step [9/47], Loss: 0.2855\n",
      "Epoch [82/5000], Step [18/47], Loss: 0.3904\n",
      "Epoch [82/5000], Step [27/47], Loss: 0.1979\n",
      "Epoch [82/5000], Step [36/47], Loss: 0.2958\n",
      "Epoch [82/5000], Step [45/47], Loss: 0.2956\n",
      "Epoch [82/5000], Avg. Train Sample Loss: 0.3396, Avg. Validate Sample Loss: 0.3280,                             L2 Loss: 0.0616\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [83/5000] -------------------- \n",
      "Epoch [83/5000], Step [1/47], Loss: 0.3166\n",
      "Epoch [83/5000], Step [9/47], Loss: 0.3657\n",
      "Epoch [83/5000], Step [18/47], Loss: 0.4479\n",
      "Epoch [83/5000], Step [27/47], Loss: 0.3390\n",
      "Epoch [83/5000], Step [36/47], Loss: 0.3183\n",
      "Epoch [83/5000], Step [45/47], Loss: 0.2646\n",
      "Epoch [83/5000], Avg. Train Sample Loss: 0.3388, Avg. Validate Sample Loss: 0.3330,                             L2 Loss: 0.0616\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [84/5000] -------------------- \n",
      "Epoch [84/5000], Step [1/47], Loss: 0.4394\n",
      "Epoch [84/5000], Step [9/47], Loss: 0.3330\n",
      "Epoch [84/5000], Step [18/47], Loss: 0.2664\n",
      "Epoch [84/5000], Step [27/47], Loss: 0.3435\n",
      "Epoch [84/5000], Step [36/47], Loss: 0.2913\n",
      "Epoch [84/5000], Step [45/47], Loss: 0.3045\n",
      "Epoch [84/5000], Avg. Train Sample Loss: 0.3345, Avg. Validate Sample Loss: 0.3525,                             L2 Loss: 0.0645\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [85/5000] -------------------- \n",
      "Epoch [85/5000], Step [1/47], Loss: 0.5699\n",
      "Epoch [85/5000], Step [9/47], Loss: 0.1584\n",
      "Epoch [85/5000], Step [18/47], Loss: 0.3648\n",
      "Epoch [85/5000], Step [27/47], Loss: 0.3018\n",
      "Epoch [85/5000], Step [36/47], Loss: 0.3873\n",
      "Epoch [85/5000], Step [45/47], Loss: 0.2677\n",
      "Epoch [85/5000], Avg. Train Sample Loss: 0.3382, Avg. Validate Sample Loss: 0.3835,                             L2 Loss: 0.0702\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [86/5000] -------------------- \n",
      "Epoch [86/5000], Step [1/47], Loss: 0.5015\n",
      "Epoch [86/5000], Step [9/47], Loss: 0.4448\n",
      "Epoch [86/5000], Step [18/47], Loss: 0.2816\n",
      "Epoch [86/5000], Step [27/47], Loss: 0.4829\n",
      "Epoch [86/5000], Step [36/47], Loss: 0.3331\n",
      "Epoch [86/5000], Step [45/47], Loss: 0.2641\n",
      "Epoch [86/5000], Avg. Train Sample Loss: 0.3489, Avg. Validate Sample Loss: 0.3204,                             L2 Loss: 0.0606\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [87/5000] -------------------- \n",
      "Epoch [87/5000], Step [1/47], Loss: 0.4077\n",
      "Epoch [87/5000], Step [9/47], Loss: 0.3214\n",
      "Epoch [87/5000], Step [18/47], Loss: 0.4207\n",
      "Epoch [87/5000], Step [27/47], Loss: 0.1866\n",
      "Epoch [87/5000], Step [36/47], Loss: 0.1496\n",
      "Epoch [87/5000], Step [45/47], Loss: 0.3044\n",
      "Epoch [87/5000], Avg. Train Sample Loss: 0.3288, Avg. Validate Sample Loss: 0.3303,                             L2 Loss: 0.0607\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [88/5000] -------------------- \n",
      "Epoch [88/5000], Step [1/47], Loss: 0.4005\n",
      "Epoch [88/5000], Step [9/47], Loss: 0.2430\n",
      "Epoch [88/5000], Step [18/47], Loss: 0.2322\n",
      "Epoch [88/5000], Step [27/47], Loss: 0.3806\n",
      "Epoch [88/5000], Step [36/47], Loss: 0.3369\n",
      "Epoch [88/5000], Step [45/47], Loss: 0.3050\n",
      "Epoch [88/5000], Avg. Train Sample Loss: 0.3241, Avg. Validate Sample Loss: 0.3701,                             L2 Loss: 0.0678\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [89/5000] -------------------- \n",
      "Epoch [89/5000], Step [1/47], Loss: 0.2710\n",
      "Epoch [89/5000], Step [9/47], Loss: 0.2485\n",
      "Epoch [89/5000], Step [18/47], Loss: 0.4766\n",
      "Epoch [89/5000], Step [27/47], Loss: 0.2708\n",
      "Epoch [89/5000], Step [36/47], Loss: 0.2015\n",
      "Epoch [89/5000], Step [45/47], Loss: 0.3621\n",
      "Epoch [89/5000], Avg. Train Sample Loss: 0.3366, Avg. Validate Sample Loss: 0.3396,                             L2 Loss: 0.0606\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [90/5000] -------------------- \n",
      "Epoch [90/5000], Step [1/47], Loss: 0.2794\n",
      "Epoch [90/5000], Step [9/47], Loss: 0.3276\n",
      "Epoch [90/5000], Step [18/47], Loss: 0.2793\n",
      "Epoch [90/5000], Step [27/47], Loss: 0.3284\n",
      "Epoch [90/5000], Step [36/47], Loss: 0.3406\n",
      "Epoch [90/5000], Step [45/47], Loss: 0.3297\n",
      "Epoch [90/5000], Avg. Train Sample Loss: 0.3272, Avg. Validate Sample Loss: 0.3480,                             L2 Loss: 0.0636\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [91/5000] -------------------- \n",
      "Epoch [91/5000], Step [1/47], Loss: 0.4284\n",
      "Epoch [91/5000], Step [9/47], Loss: 0.5360\n",
      "Epoch [91/5000], Step [18/47], Loss: 0.4325\n",
      "Epoch [91/5000], Step [27/47], Loss: 0.3163\n",
      "Epoch [91/5000], Step [36/47], Loss: 0.2690\n",
      "Epoch [91/5000], Step [45/47], Loss: 0.3282\n",
      "Epoch [91/5000], Avg. Train Sample Loss: 0.3365, Avg. Validate Sample Loss: 0.3214,                             L2 Loss: 0.0647\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [92/5000] -------------------- \n",
      "Epoch [92/5000], Step [1/47], Loss: 0.3555\n",
      "Epoch [92/5000], Step [9/47], Loss: 0.3432\n",
      "Epoch [92/5000], Step [18/47], Loss: 0.3572\n",
      "Epoch [92/5000], Step [27/47], Loss: 0.2688\n",
      "Epoch [92/5000], Step [36/47], Loss: 0.2986\n",
      "Epoch [92/5000], Step [45/47], Loss: 0.3564\n",
      "Epoch [92/5000], Avg. Train Sample Loss: 0.3212, Avg. Validate Sample Loss: 0.3110,                             L2 Loss: 0.0631\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [93/5000] -------------------- \n",
      "Epoch [93/5000], Step [1/47], Loss: 0.4680\n",
      "Epoch [93/5000], Step [9/47], Loss: 0.3498\n",
      "Epoch [93/5000], Step [18/47], Loss: 0.4428\n",
      "Epoch [93/5000], Step [27/47], Loss: 0.2597\n",
      "Epoch [93/5000], Step [36/47], Loss: 0.4817\n",
      "Epoch [93/5000], Step [45/47], Loss: 0.3406\n",
      "Epoch [93/5000], Avg. Train Sample Loss: 0.3186, Avg. Validate Sample Loss: 0.3076,                             L2 Loss: 0.0582\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [94/5000] -------------------- \n",
      "Epoch [94/5000], Step [1/47], Loss: 0.3226\n",
      "Epoch [94/5000], Step [9/47], Loss: 0.1973\n",
      "Epoch [94/5000], Step [18/47], Loss: 0.3201\n",
      "Epoch [94/5000], Step [27/47], Loss: 0.3423\n",
      "Epoch [94/5000], Step [36/47], Loss: 0.3111\n",
      "Epoch [94/5000], Step [45/47], Loss: 0.2155\n",
      "Epoch [94/5000], Avg. Train Sample Loss: 0.3182, Avg. Validate Sample Loss: 0.3188,                             L2 Loss: 0.0654\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [95/5000] -------------------- \n",
      "Epoch [95/5000], Step [1/47], Loss: 0.3209\n",
      "Epoch [95/5000], Step [9/47], Loss: 0.3820\n",
      "Epoch [95/5000], Step [18/47], Loss: 0.3606\n",
      "Epoch [95/5000], Step [27/47], Loss: 0.2870\n",
      "Epoch [95/5000], Step [36/47], Loss: 0.2087\n",
      "Epoch [95/5000], Step [45/47], Loss: 0.4838\n",
      "Epoch [95/5000], Avg. Train Sample Loss: 0.3150, Avg. Validate Sample Loss: 0.4202,                             L2 Loss: 0.0754\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [96/5000] -------------------- \n",
      "Epoch [96/5000], Step [1/47], Loss: 0.4538\n",
      "Epoch [96/5000], Step [9/47], Loss: 0.4388\n",
      "Epoch [96/5000], Step [18/47], Loss: 0.3588\n",
      "Epoch [96/5000], Step [27/47], Loss: 0.3423\n",
      "Epoch [96/5000], Step [36/47], Loss: 0.2860\n",
      "Epoch [96/5000], Step [45/47], Loss: 0.2954\n",
      "Epoch [96/5000], Avg. Train Sample Loss: 0.3193, Avg. Validate Sample Loss: 0.3151,                             L2 Loss: 0.0599\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [97/5000] -------------------- \n",
      "Epoch [97/5000], Step [1/47], Loss: 0.3178\n",
      "Epoch [97/5000], Step [9/47], Loss: 0.2286\n",
      "Epoch [97/5000], Step [18/47], Loss: 0.3367\n",
      "Epoch [97/5000], Step [27/47], Loss: 0.3303\n",
      "Epoch [97/5000], Step [36/47], Loss: 0.3610\n",
      "Epoch [97/5000], Step [45/47], Loss: 0.2652\n",
      "Epoch [97/5000], Avg. Train Sample Loss: 0.3130, Avg. Validate Sample Loss: 0.2840,                             L2 Loss: 0.0564\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [98/5000] -------------------- \n",
      "Epoch [98/5000], Step [1/47], Loss: 0.2954\n",
      "Epoch [98/5000], Step [9/47], Loss: 0.3008\n",
      "Epoch [98/5000], Step [18/47], Loss: 0.2767\n",
      "Epoch [98/5000], Step [27/47], Loss: 0.2522\n",
      "Epoch [98/5000], Step [36/47], Loss: 0.3139\n",
      "Epoch [98/5000], Step [45/47], Loss: 0.2627\n",
      "Epoch [98/5000], Avg. Train Sample Loss: 0.3050, Avg. Validate Sample Loss: 0.2912,                             L2 Loss: 0.0572\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [99/5000] -------------------- \n",
      "Epoch [99/5000], Step [1/47], Loss: 0.1049\n",
      "Epoch [99/5000], Step [9/47], Loss: 0.2178\n",
      "Epoch [99/5000], Step [18/47], Loss: 0.5531\n",
      "Epoch [99/5000], Step [27/47], Loss: 0.2931\n",
      "Epoch [99/5000], Step [36/47], Loss: 0.2760\n",
      "Epoch [99/5000], Step [45/47], Loss: 0.4327\n",
      "Epoch [99/5000], Avg. Train Sample Loss: 0.3115, Avg. Validate Sample Loss: 0.2743,                             L2 Loss: 0.0586\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [100/5000] -------------------- \n",
      "Epoch [100/5000], Step [1/47], Loss: 0.3167\n",
      "Epoch [100/5000], Step [9/47], Loss: 0.3323\n",
      "Epoch [100/5000], Step [18/47], Loss: 0.2939\n",
      "Epoch [100/5000], Step [27/47], Loss: 0.2873\n",
      "Epoch [100/5000], Step [36/47], Loss: 0.4561\n",
      "Epoch [100/5000], Step [45/47], Loss: 0.3750\n",
      "Epoch [100/5000], Avg. Train Sample Loss: 0.3060, Avg. Validate Sample Loss: 0.2882,                             L2 Loss: 0.0570\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [101/5000] -------------------- \n",
      "Epoch [101/5000], Step [1/47], Loss: 0.3832\n",
      "Epoch [101/5000], Step [9/47], Loss: 0.5415\n",
      "Epoch [101/5000], Step [18/47], Loss: 0.3409\n",
      "Epoch [101/5000], Step [27/47], Loss: 0.3056\n",
      "Epoch [101/5000], Step [36/47], Loss: 0.4431\n",
      "Epoch [101/5000], Step [45/47], Loss: 0.1115\n",
      "Epoch [101/5000], Avg. Train Sample Loss: 0.3027, Avg. Validate Sample Loss: 0.2615,                             L2 Loss: 0.0558\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [102/5000] -------------------- \n",
      "Epoch [102/5000], Step [1/47], Loss: 0.2427\n",
      "Epoch [102/5000], Step [9/47], Loss: 0.2805\n",
      "Epoch [102/5000], Step [18/47], Loss: 0.3225\n",
      "Epoch [102/5000], Step [27/47], Loss: 0.4284\n",
      "Epoch [102/5000], Step [36/47], Loss: 0.2721\n",
      "Epoch [102/5000], Step [45/47], Loss: 0.3015\n",
      "Epoch [102/5000], Avg. Train Sample Loss: 0.2917, Avg. Validate Sample Loss: 0.2970,                             L2 Loss: 0.0520\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [103/5000] -------------------- \n",
      "Epoch [103/5000], Step [1/47], Loss: 0.3967\n",
      "Epoch [103/5000], Step [9/47], Loss: 0.3684\n",
      "Epoch [103/5000], Step [18/47], Loss: 0.2360\n",
      "Epoch [103/5000], Step [27/47], Loss: 0.2984\n",
      "Epoch [103/5000], Step [36/47], Loss: 0.2459\n",
      "Epoch [103/5000], Step [45/47], Loss: 0.1342\n",
      "Epoch [103/5000], Avg. Train Sample Loss: 0.2841, Avg. Validate Sample Loss: 0.2690,                             L2 Loss: 0.0528\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [104/5000] -------------------- \n",
      "Epoch [104/5000], Step [1/47], Loss: 0.2400\n",
      "Epoch [104/5000], Step [9/47], Loss: 0.2830\n",
      "Epoch [104/5000], Step [18/47], Loss: 0.2794\n",
      "Epoch [104/5000], Step [27/47], Loss: 0.2203\n",
      "Epoch [104/5000], Step [36/47], Loss: 0.1343\n",
      "Epoch [104/5000], Step [45/47], Loss: 0.1756\n",
      "Epoch [104/5000], Avg. Train Sample Loss: 0.2821, Avg. Validate Sample Loss: 0.3052,                             L2 Loss: 0.0558\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [105/5000] -------------------- \n",
      "Epoch [105/5000], Step [1/47], Loss: 0.2487\n",
      "Epoch [105/5000], Step [9/47], Loss: 0.2110\n",
      "Epoch [105/5000], Step [18/47], Loss: 0.4514\n",
      "Epoch [105/5000], Step [27/47], Loss: 0.3073\n",
      "Epoch [105/5000], Step [36/47], Loss: 0.1580\n",
      "Epoch [105/5000], Step [45/47], Loss: 0.2611\n",
      "Epoch [105/5000], Avg. Train Sample Loss: 0.2846, Avg. Validate Sample Loss: 0.2661,                             L2 Loss: 0.0540\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [106/5000] -------------------- \n",
      "Epoch [106/5000], Step [1/47], Loss: 0.1885\n",
      "Epoch [106/5000], Step [9/47], Loss: 0.2298\n",
      "Epoch [106/5000], Step [18/47], Loss: 0.3338\n",
      "Epoch [106/5000], Step [27/47], Loss: 0.2541\n",
      "Epoch [106/5000], Step [36/47], Loss: 0.3189\n",
      "Epoch [106/5000], Step [45/47], Loss: 0.2075\n",
      "Epoch [106/5000], Avg. Train Sample Loss: 0.2906, Avg. Validate Sample Loss: 0.2647,                             L2 Loss: 0.0626\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [107/5000] -------------------- \n",
      "Epoch [107/5000], Step [1/47], Loss: 0.3354\n",
      "Epoch [107/5000], Step [9/47], Loss: 0.2177\n",
      "Epoch [107/5000], Step [18/47], Loss: 0.2995\n",
      "Epoch [107/5000], Step [27/47], Loss: 0.2468\n",
      "Epoch [107/5000], Step [36/47], Loss: 0.2718\n",
      "Epoch [107/5000], Step [45/47], Loss: 0.2994\n",
      "Epoch [107/5000], Avg. Train Sample Loss: 0.2802, Avg. Validate Sample Loss: 0.2628,                             L2 Loss: 0.0525\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [108/5000] -------------------- \n",
      "Epoch [108/5000], Step [1/47], Loss: 0.3146\n",
      "Epoch [108/5000], Step [9/47], Loss: 0.3145\n",
      "Epoch [108/5000], Step [18/47], Loss: 0.2323\n",
      "Epoch [108/5000], Step [27/47], Loss: 0.2938\n",
      "Epoch [108/5000], Step [36/47], Loss: 0.1248\n",
      "Epoch [108/5000], Step [45/47], Loss: 0.3102\n",
      "Epoch [108/5000], Avg. Train Sample Loss: 0.2774, Avg. Validate Sample Loss: 0.2845,                             L2 Loss: 0.0557\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [109/5000] -------------------- \n",
      "Epoch [109/5000], Step [1/47], Loss: 0.3447\n",
      "Epoch [109/5000], Step [9/47], Loss: 0.2841\n",
      "Epoch [109/5000], Step [18/47], Loss: 0.1799\n",
      "Epoch [109/5000], Step [27/47], Loss: 0.2181\n",
      "Epoch [109/5000], Step [36/47], Loss: 0.2497\n",
      "Epoch [109/5000], Step [45/47], Loss: 0.3147\n",
      "Epoch [109/5000], Avg. Train Sample Loss: 0.2666, Avg. Validate Sample Loss: 0.2758,                             L2 Loss: 0.0489\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [110/5000] -------------------- \n",
      "Epoch [110/5000], Step [1/47], Loss: 0.1715\n",
      "Epoch [110/5000], Step [9/47], Loss: 0.3186\n",
      "Epoch [110/5000], Step [18/47], Loss: 0.2460\n",
      "Epoch [110/5000], Step [27/47], Loss: 0.2597\n",
      "Epoch [110/5000], Step [36/47], Loss: 0.2002\n",
      "Epoch [110/5000], Step [45/47], Loss: 0.2106\n",
      "Epoch [110/5000], Avg. Train Sample Loss: 0.2516, Avg. Validate Sample Loss: 0.2400,                             L2 Loss: 0.0507\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [111/5000] -------------------- \n",
      "Epoch [111/5000], Step [1/47], Loss: 0.2340\n",
      "Epoch [111/5000], Step [9/47], Loss: 0.2399\n",
      "Epoch [111/5000], Step [18/47], Loss: 0.3274\n",
      "Epoch [111/5000], Step [27/47], Loss: 0.4162\n",
      "Epoch [111/5000], Step [36/47], Loss: 0.1980\n",
      "Epoch [111/5000], Step [45/47], Loss: 0.3482\n",
      "Epoch [111/5000], Avg. Train Sample Loss: 0.2526, Avg. Validate Sample Loss: 0.3076,                             L2 Loss: 0.0632\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [112/5000] -------------------- \n",
      "Epoch [112/5000], Step [1/47], Loss: 0.3437\n",
      "Epoch [112/5000], Step [9/47], Loss: 0.3119\n",
      "Epoch [112/5000], Step [18/47], Loss: 0.2334\n",
      "Epoch [112/5000], Step [27/47], Loss: 0.1569\n",
      "Epoch [112/5000], Step [36/47], Loss: 0.1765\n",
      "Epoch [112/5000], Step [45/47], Loss: 0.4203\n",
      "Epoch [112/5000], Avg. Train Sample Loss: 0.2577, Avg. Validate Sample Loss: 0.2505,                             L2 Loss: 0.0490\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [113/5000] -------------------- \n",
      "Epoch [113/5000], Step [1/47], Loss: 0.3339\n",
      "Epoch [113/5000], Step [9/47], Loss: 0.2004\n",
      "Epoch [113/5000], Step [18/47], Loss: 0.2175\n",
      "Epoch [113/5000], Step [27/47], Loss: 0.1727\n",
      "Epoch [113/5000], Step [36/47], Loss: 0.2235\n",
      "Epoch [113/5000], Step [45/47], Loss: 0.2374\n",
      "Epoch [113/5000], Avg. Train Sample Loss: 0.2364, Avg. Validate Sample Loss: 0.2292,                             L2 Loss: 0.0500\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [114/5000] -------------------- \n",
      "Epoch [114/5000], Step [1/47], Loss: 0.1753\n",
      "Epoch [114/5000], Step [9/47], Loss: 0.2077\n",
      "Epoch [114/5000], Step [18/47], Loss: 0.3516\n",
      "Epoch [114/5000], Step [27/47], Loss: 0.2442\n",
      "Epoch [114/5000], Step [36/47], Loss: 0.1159\n",
      "Epoch [114/5000], Step [45/47], Loss: 0.2718\n",
      "Epoch [114/5000], Avg. Train Sample Loss: 0.2406, Avg. Validate Sample Loss: 0.2131,                             L2 Loss: 0.0431\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [115/5000] -------------------- \n",
      "Epoch [115/5000], Step [1/47], Loss: 0.2433\n",
      "Epoch [115/5000], Step [9/47], Loss: 0.3421\n",
      "Epoch [115/5000], Step [18/47], Loss: 0.2872\n",
      "Epoch [115/5000], Step [27/47], Loss: 0.2377\n",
      "Epoch [115/5000], Step [36/47], Loss: 0.1301\n",
      "Epoch [115/5000], Step [45/47], Loss: 0.1192\n",
      "Epoch [115/5000], Avg. Train Sample Loss: 0.2303, Avg. Validate Sample Loss: 0.2316,                             L2 Loss: 0.0457\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [116/5000] -------------------- \n",
      "Epoch [116/5000], Step [1/47], Loss: 0.1042\n",
      "Epoch [116/5000], Step [9/47], Loss: 0.2133\n",
      "Epoch [116/5000], Step [18/47], Loss: 0.2775\n",
      "Epoch [116/5000], Step [27/47], Loss: 0.0988\n",
      "Epoch [116/5000], Step [36/47], Loss: 0.2506\n",
      "Epoch [116/5000], Step [45/47], Loss: 0.1773\n",
      "Epoch [116/5000], Avg. Train Sample Loss: 0.2290, Avg. Validate Sample Loss: 0.2169,                             L2 Loss: 0.0414\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [117/5000] -------------------- \n",
      "Epoch [117/5000], Step [1/47], Loss: 0.3545\n",
      "Epoch [117/5000], Step [9/47], Loss: 0.2290\n",
      "Epoch [117/5000], Step [18/47], Loss: 0.1152\n",
      "Epoch [117/5000], Step [27/47], Loss: 0.2077\n",
      "Epoch [117/5000], Step [36/47], Loss: 0.1180\n",
      "Epoch [117/5000], Step [45/47], Loss: 0.3531\n",
      "Epoch [117/5000], Avg. Train Sample Loss: 0.2173, Avg. Validate Sample Loss: 0.2148,                             L2 Loss: 0.0424\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [118/5000] -------------------- \n",
      "Epoch [118/5000], Step [1/47], Loss: 0.1583\n",
      "Epoch [118/5000], Step [9/47], Loss: 0.1993\n",
      "Epoch [118/5000], Step [18/47], Loss: 0.2377\n",
      "Epoch [118/5000], Step [27/47], Loss: 0.3204\n",
      "Epoch [118/5000], Step [36/47], Loss: 0.1497\n",
      "Epoch [118/5000], Step [45/47], Loss: 0.3667\n",
      "Epoch [118/5000], Avg. Train Sample Loss: 0.2206, Avg. Validate Sample Loss: 0.2639,                             L2 Loss: 0.0445\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [119/5000] -------------------- \n",
      "Epoch [119/5000], Step [1/47], Loss: 0.3098\n",
      "Epoch [119/5000], Step [9/47], Loss: 0.2125\n",
      "Epoch [119/5000], Step [18/47], Loss: 0.2631\n",
      "Epoch [119/5000], Step [27/47], Loss: 0.2134\n",
      "Epoch [119/5000], Step [36/47], Loss: 0.3103\n",
      "Epoch [119/5000], Step [45/47], Loss: 0.0747\n",
      "Epoch [119/5000], Avg. Train Sample Loss: 0.2175, Avg. Validate Sample Loss: 0.2608,                             L2 Loss: 0.0511\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [120/5000] -------------------- \n",
      "Epoch [120/5000], Step [1/47], Loss: 0.2415\n",
      "Epoch [120/5000], Step [9/47], Loss: 0.0979\n",
      "Epoch [120/5000], Step [18/47], Loss: 0.1340\n",
      "Epoch [120/5000], Step [27/47], Loss: 0.3181\n",
      "Epoch [120/5000], Step [36/47], Loss: 0.1799\n",
      "Epoch [120/5000], Step [45/47], Loss: 0.1993\n",
      "Epoch [120/5000], Avg. Train Sample Loss: 0.2117, Avg. Validate Sample Loss: 0.1840,                             L2 Loss: 0.0421\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [121/5000] -------------------- \n",
      "Epoch [121/5000], Step [1/47], Loss: 0.1427\n",
      "Epoch [121/5000], Step [9/47], Loss: 0.2169\n",
      "Epoch [121/5000], Step [18/47], Loss: 0.1574\n",
      "Epoch [121/5000], Step [27/47], Loss: 0.1672\n",
      "Epoch [121/5000], Step [36/47], Loss: 0.2139\n",
      "Epoch [121/5000], Step [45/47], Loss: 0.1123\n",
      "Epoch [121/5000], Avg. Train Sample Loss: 0.2052, Avg. Validate Sample Loss: 0.1746,                             L2 Loss: 0.0409\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [122/5000] -------------------- \n",
      "Epoch [122/5000], Step [1/47], Loss: 0.1445\n",
      "Epoch [122/5000], Step [9/47], Loss: 0.2210\n",
      "Epoch [122/5000], Step [18/47], Loss: 0.2185\n",
      "Epoch [122/5000], Step [27/47], Loss: 0.2536\n",
      "Epoch [122/5000], Step [36/47], Loss: 0.2473\n",
      "Epoch [122/5000], Step [45/47], Loss: 0.1428\n",
      "Epoch [122/5000], Avg. Train Sample Loss: 0.2075, Avg. Validate Sample Loss: 0.1967,                             L2 Loss: 0.0455\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [123/5000] -------------------- \n",
      "Epoch [123/5000], Step [1/47], Loss: 0.1841\n",
      "Epoch [123/5000], Step [9/47], Loss: 0.1801\n",
      "Epoch [123/5000], Step [18/47], Loss: 0.2482\n",
      "Epoch [123/5000], Step [27/47], Loss: 0.1997\n",
      "Epoch [123/5000], Step [36/47], Loss: 0.2702\n",
      "Epoch [123/5000], Step [45/47], Loss: 0.1645\n",
      "Epoch [123/5000], Avg. Train Sample Loss: 0.1978, Avg. Validate Sample Loss: 0.1865,                             L2 Loss: 0.0379\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [124/5000] -------------------- \n",
      "Epoch [124/5000], Step [1/47], Loss: 0.0861\n",
      "Epoch [124/5000], Step [9/47], Loss: 0.1733\n",
      "Epoch [124/5000], Step [18/47], Loss: 0.3272\n",
      "Epoch [124/5000], Step [27/47], Loss: 0.2580\n",
      "Epoch [124/5000], Step [36/47], Loss: 0.1340\n",
      "Epoch [124/5000], Step [45/47], Loss: 0.1206\n",
      "Epoch [124/5000], Avg. Train Sample Loss: 0.1967, Avg. Validate Sample Loss: 0.2162,                             L2 Loss: 0.0372\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [125/5000] -------------------- \n",
      "Epoch [125/5000], Step [1/47], Loss: 0.1834\n",
      "Epoch [125/5000], Step [9/47], Loss: 0.2488\n",
      "Epoch [125/5000], Step [18/47], Loss: 0.2444\n",
      "Epoch [125/5000], Step [27/47], Loss: 0.1018\n",
      "Epoch [125/5000], Step [36/47], Loss: 0.3509\n",
      "Epoch [125/5000], Step [45/47], Loss: 0.1351\n",
      "Epoch [125/5000], Avg. Train Sample Loss: 0.1905, Avg. Validate Sample Loss: 0.1605,                             L2 Loss: 0.0342\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [126/5000] -------------------- \n",
      "Epoch [126/5000], Step [1/47], Loss: 0.1942\n",
      "Epoch [126/5000], Step [9/47], Loss: 0.2365\n",
      "Epoch [126/5000], Step [18/47], Loss: 0.2194\n",
      "Epoch [126/5000], Step [27/47], Loss: 0.1935\n",
      "Epoch [126/5000], Step [36/47], Loss: 0.1317\n",
      "Epoch [126/5000], Step [45/47], Loss: 0.2117\n",
      "Epoch [126/5000], Avg. Train Sample Loss: 0.2014, Avg. Validate Sample Loss: 0.1990,                             L2 Loss: 0.0362\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [127/5000] -------------------- \n",
      "Epoch [127/5000], Step [1/47], Loss: 0.1753\n",
      "Epoch [127/5000], Step [9/47], Loss: 0.2693\n",
      "Epoch [127/5000], Step [18/47], Loss: 0.1852\n",
      "Epoch [127/5000], Step [27/47], Loss: 0.3026\n",
      "Epoch [127/5000], Step [36/47], Loss: 0.1424\n",
      "Epoch [127/5000], Step [45/47], Loss: 0.1303\n",
      "Epoch [127/5000], Avg. Train Sample Loss: 0.1902, Avg. Validate Sample Loss: 0.2124,                             L2 Loss: 0.0420\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [128/5000] -------------------- \n",
      "Epoch [128/5000], Step [1/47], Loss: 0.2116\n",
      "Epoch [128/5000], Step [9/47], Loss: 0.1413\n",
      "Epoch [128/5000], Step [18/47], Loss: 0.3346\n",
      "Epoch [128/5000], Step [27/47], Loss: 0.0926\n",
      "Epoch [128/5000], Step [36/47], Loss: 0.1546\n",
      "Epoch [128/5000], Step [45/47], Loss: 0.0715\n",
      "Epoch [128/5000], Avg. Train Sample Loss: 0.1895, Avg. Validate Sample Loss: 0.1866,                             L2 Loss: 0.0345\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [129/5000] -------------------- \n",
      "Epoch [129/5000], Step [1/47], Loss: 0.2732\n",
      "Epoch [129/5000], Step [9/47], Loss: 0.1797\n",
      "Epoch [129/5000], Step [18/47], Loss: 0.2954\n",
      "Epoch [129/5000], Step [27/47], Loss: 0.1987\n",
      "Epoch [129/5000], Step [36/47], Loss: 0.1310\n",
      "Epoch [129/5000], Step [45/47], Loss: 0.1430\n",
      "Epoch [129/5000], Avg. Train Sample Loss: 0.1850, Avg. Validate Sample Loss: 0.1676,                             L2 Loss: 0.0304\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [130/5000] -------------------- \n",
      "Epoch [130/5000], Step [1/47], Loss: 0.3065\n",
      "Epoch [130/5000], Step [9/47], Loss: 0.1393\n",
      "Epoch [130/5000], Step [18/47], Loss: 0.2272\n",
      "Epoch [130/5000], Step [27/47], Loss: 0.1174\n",
      "Epoch [130/5000], Step [36/47], Loss: 0.2273\n",
      "Epoch [130/5000], Step [45/47], Loss: 0.2275\n",
      "Epoch [130/5000], Avg. Train Sample Loss: 0.1857, Avg. Validate Sample Loss: 0.1710,                             L2 Loss: 0.0313\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [131/5000] -------------------- \n",
      "Epoch [131/5000], Step [1/47], Loss: 0.1667\n",
      "Epoch [131/5000], Step [9/47], Loss: 0.1371\n",
      "Epoch [131/5000], Step [18/47], Loss: 0.1344\n",
      "Epoch [131/5000], Step [27/47], Loss: 0.1685\n",
      "Epoch [131/5000], Step [36/47], Loss: 0.2876\n",
      "Epoch [131/5000], Step [45/47], Loss: 0.2205\n",
      "Epoch [131/5000], Avg. Train Sample Loss: 0.1875, Avg. Validate Sample Loss: 0.1707,                             L2 Loss: 0.0385\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [132/5000] -------------------- \n",
      "Epoch [132/5000], Step [1/47], Loss: 0.2529\n",
      "Epoch [132/5000], Step [9/47], Loss: 0.1919\n",
      "Epoch [132/5000], Step [18/47], Loss: 0.2249\n",
      "Epoch [132/5000], Step [27/47], Loss: 0.1217\n",
      "Epoch [132/5000], Step [36/47], Loss: 0.2530\n",
      "Epoch [132/5000], Step [45/47], Loss: 0.1802\n",
      "Epoch [132/5000], Avg. Train Sample Loss: 0.1781, Avg. Validate Sample Loss: 0.1938,                             L2 Loss: 0.0285\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [133/5000] -------------------- \n",
      "Epoch [133/5000], Step [1/47], Loss: 0.3254\n",
      "Epoch [133/5000], Step [9/47], Loss: 0.1522\n",
      "Epoch [133/5000], Step [18/47], Loss: 0.1753\n",
      "Epoch [133/5000], Step [27/47], Loss: 0.0941\n",
      "Epoch [133/5000], Step [36/47], Loss: 0.2001\n",
      "Epoch [133/5000], Step [45/47], Loss: 0.3208\n",
      "Epoch [133/5000], Avg. Train Sample Loss: 0.1791, Avg. Validate Sample Loss: 0.1914,                             L2 Loss: 0.0312\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [134/5000] -------------------- \n",
      "Epoch [134/5000], Step [1/47], Loss: 0.2140\n",
      "Epoch [134/5000], Step [9/47], Loss: 0.1781\n",
      "Epoch [134/5000], Step [18/47], Loss: 0.2257\n",
      "Epoch [134/5000], Step [27/47], Loss: 0.1224\n",
      "Epoch [134/5000], Step [36/47], Loss: 0.3019\n",
      "Epoch [134/5000], Step [45/47], Loss: 0.1469\n",
      "Epoch [134/5000], Avg. Train Sample Loss: 0.1824, Avg. Validate Sample Loss: 0.1887,                             L2 Loss: 0.0335\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [135/5000] -------------------- \n",
      "Epoch [135/5000], Step [1/47], Loss: 0.1859\n",
      "Epoch [135/5000], Step [9/47], Loss: 0.1193\n",
      "Epoch [135/5000], Step [18/47], Loss: 0.1965\n",
      "Epoch [135/5000], Step [27/47], Loss: 0.1860\n",
      "Epoch [135/5000], Step [36/47], Loss: 0.0980\n",
      "Epoch [135/5000], Step [45/47], Loss: 0.1891\n",
      "Epoch [135/5000], Avg. Train Sample Loss: 0.1787, Avg. Validate Sample Loss: 0.1624,                             L2 Loss: 0.0330\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [136/5000] -------------------- \n",
      "Epoch [136/5000], Step [1/47], Loss: 0.1079\n",
      "Epoch [136/5000], Step [9/47], Loss: 0.1348\n",
      "Epoch [136/5000], Step [18/47], Loss: 0.1137\n",
      "Epoch [136/5000], Step [27/47], Loss: 0.1733\n",
      "Epoch [136/5000], Step [36/47], Loss: 0.1155\n",
      "Epoch [136/5000], Step [45/47], Loss: 0.1830\n",
      "Epoch [136/5000], Avg. Train Sample Loss: 0.1752, Avg. Validate Sample Loss: 0.1688,                             L2 Loss: 0.0317\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [137/5000] -------------------- \n",
      "Epoch [137/5000], Step [1/47], Loss: 0.1786\n",
      "Epoch [137/5000], Step [9/47], Loss: 0.1805\n",
      "Epoch [137/5000], Step [18/47], Loss: 0.1928\n",
      "Epoch [137/5000], Step [27/47], Loss: 0.1789\n",
      "Epoch [137/5000], Step [36/47], Loss: 0.1325\n",
      "Epoch [137/5000], Step [45/47], Loss: 0.1138\n",
      "Epoch [137/5000], Avg. Train Sample Loss: 0.1811, Avg. Validate Sample Loss: 0.1784,                             L2 Loss: 0.0331\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [138/5000] -------------------- \n",
      "Epoch [138/5000], Step [1/47], Loss: 0.1349\n",
      "Epoch [138/5000], Step [9/47], Loss: 0.1649\n",
      "Epoch [138/5000], Step [18/47], Loss: 0.1418\n",
      "Epoch [138/5000], Step [27/47], Loss: 0.1495\n",
      "Epoch [138/5000], Step [36/47], Loss: 0.1231\n",
      "Epoch [138/5000], Step [45/47], Loss: 0.1437\n",
      "Epoch [138/5000], Avg. Train Sample Loss: 0.1764, Avg. Validate Sample Loss: 0.1502,                             L2 Loss: 0.0276\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [139/5000] -------------------- \n",
      "Epoch [139/5000], Step [1/47], Loss: 0.1493\n",
      "Epoch [139/5000], Step [9/47], Loss: 0.1606\n",
      "Epoch [139/5000], Step [18/47], Loss: 0.1688\n",
      "Epoch [139/5000], Step [27/47], Loss: 0.1132\n",
      "Epoch [139/5000], Step [36/47], Loss: 0.1794\n",
      "Epoch [139/5000], Step [45/47], Loss: 0.1420\n",
      "Epoch [139/5000], Avg. Train Sample Loss: 0.1738, Avg. Validate Sample Loss: 0.1643,                             L2 Loss: 0.0287\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [140/5000] -------------------- \n",
      "Epoch [140/5000], Step [1/47], Loss: 0.1331\n",
      "Epoch [140/5000], Step [9/47], Loss: 0.1701\n",
      "Epoch [140/5000], Step [18/47], Loss: 0.1246\n",
      "Epoch [140/5000], Step [27/47], Loss: 0.1951\n",
      "Epoch [140/5000], Step [36/47], Loss: 0.2000\n",
      "Epoch [140/5000], Step [45/47], Loss: 0.1778\n",
      "Epoch [140/5000], Avg. Train Sample Loss: 0.1862, Avg. Validate Sample Loss: 0.1669,                             L2 Loss: 0.0333\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [141/5000] -------------------- \n",
      "Epoch [141/5000], Step [1/47], Loss: 0.1829\n",
      "Epoch [141/5000], Step [9/47], Loss: 0.1475\n",
      "Epoch [141/5000], Step [18/47], Loss: 0.1109\n",
      "Epoch [141/5000], Step [27/47], Loss: 0.2053\n",
      "Epoch [141/5000], Step [36/47], Loss: 0.2339\n",
      "Epoch [141/5000], Step [45/47], Loss: 0.1968\n",
      "Epoch [141/5000], Avg. Train Sample Loss: 0.1764, Avg. Validate Sample Loss: 0.1970,                             L2 Loss: 0.0331\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [142/5000] -------------------- \n",
      "Epoch [142/5000], Step [1/47], Loss: 0.1756\n",
      "Epoch [142/5000], Step [9/47], Loss: 0.2166\n",
      "Epoch [142/5000], Step [18/47], Loss: 0.2517\n",
      "Epoch [142/5000], Step [27/47], Loss: 0.1129\n",
      "Epoch [142/5000], Step [36/47], Loss: 0.0921\n",
      "Epoch [142/5000], Step [45/47], Loss: 0.1415\n",
      "Epoch [142/5000], Avg. Train Sample Loss: 0.1730, Avg. Validate Sample Loss: 0.1963,                             L2 Loss: 0.0453\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [143/5000] -------------------- \n",
      "Epoch [143/5000], Step [1/47], Loss: 0.3217\n",
      "Epoch [143/5000], Step [9/47], Loss: 0.1320\n",
      "Epoch [143/5000], Step [18/47], Loss: 0.3663\n",
      "Epoch [143/5000], Step [27/47], Loss: 0.1933\n",
      "Epoch [143/5000], Step [36/47], Loss: 0.2723\n",
      "Epoch [143/5000], Step [45/47], Loss: 0.1084\n",
      "Epoch [143/5000], Avg. Train Sample Loss: 0.1698, Avg. Validate Sample Loss: 0.1689,                             L2 Loss: 0.0313\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [144/5000] -------------------- \n",
      "Epoch [144/5000], Step [1/47], Loss: 0.0925\n",
      "Epoch [144/5000], Step [9/47], Loss: 0.0989\n",
      "Epoch [144/5000], Step [18/47], Loss: 0.2359\n",
      "Epoch [144/5000], Step [27/47], Loss: 0.2580\n",
      "Epoch [144/5000], Step [36/47], Loss: 0.1391\n",
      "Epoch [144/5000], Step [45/47], Loss: 0.0867\n",
      "Epoch [144/5000], Avg. Train Sample Loss: 0.1649, Avg. Validate Sample Loss: 0.1562,                             L2 Loss: 0.0237\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [145/5000] -------------------- \n",
      "Epoch [145/5000], Step [1/47], Loss: 0.2131\n",
      "Epoch [145/5000], Step [9/47], Loss: 0.1305\n",
      "Epoch [145/5000], Step [18/47], Loss: 0.1189\n",
      "Epoch [145/5000], Step [27/47], Loss: 0.1402\n",
      "Epoch [145/5000], Step [36/47], Loss: 0.1668\n",
      "Epoch [145/5000], Step [45/47], Loss: 0.1650\n",
      "Epoch [145/5000], Avg. Train Sample Loss: 0.1758, Avg. Validate Sample Loss: 0.1576,                             L2 Loss: 0.0345\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [146/5000] -------------------- \n",
      "Epoch [146/5000], Step [1/47], Loss: 0.1464\n",
      "Epoch [146/5000], Step [9/47], Loss: 0.1328\n",
      "Epoch [146/5000], Step [18/47], Loss: 0.1682\n",
      "Epoch [146/5000], Step [27/47], Loss: 0.1492\n",
      "Epoch [146/5000], Step [36/47], Loss: 0.1451\n",
      "Epoch [146/5000], Step [45/47], Loss: 0.1639\n",
      "Epoch [146/5000], Avg. Train Sample Loss: 0.1650, Avg. Validate Sample Loss: 0.1653,                             L2 Loss: 0.0280\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [147/5000] -------------------- \n",
      "Epoch [147/5000], Step [1/47], Loss: 0.1976\n",
      "Epoch [147/5000], Step [9/47], Loss: 0.1511\n",
      "Epoch [147/5000], Step [18/47], Loss: 0.2756\n",
      "Epoch [147/5000], Step [27/47], Loss: 0.1222\n",
      "Epoch [147/5000], Step [36/47], Loss: 0.1222\n",
      "Epoch [147/5000], Step [45/47], Loss: 0.1652\n",
      "Epoch [147/5000], Avg. Train Sample Loss: 0.1664, Avg. Validate Sample Loss: 0.1546,                             L2 Loss: 0.0323\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [148/5000] -------------------- \n",
      "Epoch [148/5000], Step [1/47], Loss: 0.2628\n",
      "Epoch [148/5000], Step [9/47], Loss: 0.3236\n",
      "Epoch [148/5000], Step [18/47], Loss: 0.1537\n",
      "Epoch [148/5000], Step [27/47], Loss: 0.2319\n",
      "Epoch [148/5000], Step [36/47], Loss: 0.2740\n",
      "Epoch [148/5000], Step [45/47], Loss: 0.1198\n",
      "Epoch [148/5000], Avg. Train Sample Loss: 0.1736, Avg. Validate Sample Loss: 0.1421,                             L2 Loss: 0.0308\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [149/5000] -------------------- \n",
      "Epoch [149/5000], Step [1/47], Loss: 0.1884\n",
      "Epoch [149/5000], Step [9/47], Loss: 0.1394\n",
      "Epoch [149/5000], Step [18/47], Loss: 0.2237\n",
      "Epoch [149/5000], Step [27/47], Loss: 0.2085\n",
      "Epoch [149/5000], Step [36/47], Loss: 0.2852\n",
      "Epoch [149/5000], Step [45/47], Loss: 0.1133\n",
      "Epoch [149/5000], Avg. Train Sample Loss: 0.1661, Avg. Validate Sample Loss: 0.1432,                             L2 Loss: 0.0228\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [150/5000] -------------------- \n",
      "Epoch [150/5000], Step [1/47], Loss: 0.1786\n",
      "Epoch [150/5000], Step [9/47], Loss: 0.1802\n",
      "Epoch [150/5000], Step [18/47], Loss: 0.2779\n",
      "Epoch [150/5000], Step [27/47], Loss: 0.1307\n",
      "Epoch [150/5000], Step [36/47], Loss: 0.0863\n",
      "Epoch [150/5000], Step [45/47], Loss: 0.3011\n",
      "Epoch [150/5000], Avg. Train Sample Loss: 0.1739, Avg. Validate Sample Loss: 0.1947,                             L2 Loss: 0.0434\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [151/5000] -------------------- \n",
      "Epoch [151/5000], Step [1/47], Loss: 0.1644\n",
      "Epoch [151/5000], Step [9/47], Loss: 0.1850\n",
      "Epoch [151/5000], Step [18/47], Loss: 0.1333\n",
      "Epoch [151/5000], Step [27/47], Loss: 0.1888\n",
      "Epoch [151/5000], Step [36/47], Loss: 0.1436\n",
      "Epoch [151/5000], Step [45/47], Loss: 0.1163\n",
      "Epoch [151/5000], Avg. Train Sample Loss: 0.1737, Avg. Validate Sample Loss: 0.1939,                             L2 Loss: 0.0363\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [152/5000] -------------------- \n",
      "Epoch [152/5000], Step [1/47], Loss: 0.1910\n",
      "Epoch [152/5000], Step [9/47], Loss: 0.1525\n",
      "Epoch [152/5000], Step [18/47], Loss: 0.1248\n",
      "Epoch [152/5000], Step [27/47], Loss: 0.1488\n",
      "Epoch [152/5000], Step [36/47], Loss: 0.1538\n",
      "Epoch [152/5000], Step [45/47], Loss: 0.1086\n",
      "Epoch [152/5000], Avg. Train Sample Loss: 0.1659, Avg. Validate Sample Loss: 0.1442,                             L2 Loss: 0.0297\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [153/5000] -------------------- \n",
      "Epoch [153/5000], Step [1/47], Loss: 0.1650\n",
      "Epoch [153/5000], Step [9/47], Loss: 0.1031\n",
      "Epoch [153/5000], Step [18/47], Loss: 0.0794\n",
      "Epoch [153/5000], Step [27/47], Loss: 0.2650\n",
      "Epoch [153/5000], Step [36/47], Loss: 0.1182\n",
      "Epoch [153/5000], Step [45/47], Loss: 0.2754\n",
      "Epoch [153/5000], Avg. Train Sample Loss: 0.1577, Avg. Validate Sample Loss: 0.1645,                             L2 Loss: 0.0284\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [154/5000] -------------------- \n",
      "Epoch [154/5000], Step [1/47], Loss: 0.2224\n",
      "Epoch [154/5000], Step [9/47], Loss: 0.2378\n",
      "Epoch [154/5000], Step [18/47], Loss: 0.1130\n",
      "Epoch [154/5000], Step [27/47], Loss: 0.1704\n",
      "Epoch [154/5000], Step [36/47], Loss: 0.1857\n",
      "Epoch [154/5000], Step [45/47], Loss: 0.1253\n",
      "Epoch [154/5000], Avg. Train Sample Loss: 0.1636, Avg. Validate Sample Loss: 0.1386,                             L2 Loss: 0.0296\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [155/5000] -------------------- \n",
      "Epoch [155/5000], Step [1/47], Loss: 0.2123\n",
      "Epoch [155/5000], Step [9/47], Loss: 0.0920\n",
      "Epoch [155/5000], Step [18/47], Loss: 0.2239\n",
      "Epoch [155/5000], Step [27/47], Loss: 0.1426\n",
      "Epoch [155/5000], Step [36/47], Loss: 0.1050\n",
      "Epoch [155/5000], Step [45/47], Loss: 0.2133\n",
      "Epoch [155/5000], Avg. Train Sample Loss: 0.1581, Avg. Validate Sample Loss: 0.1383,                             L2 Loss: 0.0264\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [156/5000] -------------------- \n",
      "Epoch [156/5000], Step [1/47], Loss: 0.1164\n",
      "Epoch [156/5000], Step [9/47], Loss: 0.0925\n",
      "Epoch [156/5000], Step [18/47], Loss: 0.1696\n",
      "Epoch [156/5000], Step [27/47], Loss: 0.1065\n",
      "Epoch [156/5000], Step [36/47], Loss: 0.1415\n",
      "Epoch [156/5000], Step [45/47], Loss: 0.1296\n",
      "Epoch [156/5000], Avg. Train Sample Loss: 0.1659, Avg. Validate Sample Loss: 0.1230,                             L2 Loss: 0.0271\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [157/5000] -------------------- \n",
      "Epoch [157/5000], Step [1/47], Loss: 0.1235\n",
      "Epoch [157/5000], Step [9/47], Loss: 0.1055\n",
      "Epoch [157/5000], Step [18/47], Loss: 0.1402\n",
      "Epoch [157/5000], Step [27/47], Loss: 0.1921\n",
      "Epoch [157/5000], Step [36/47], Loss: 0.1728\n",
      "Epoch [157/5000], Step [45/47], Loss: 0.1538\n",
      "Epoch [157/5000], Avg. Train Sample Loss: 0.1555, Avg. Validate Sample Loss: 0.1465,                             L2 Loss: 0.0312\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [158/5000] -------------------- \n",
      "Epoch [158/5000], Step [1/47], Loss: 0.0870\n",
      "Epoch [158/5000], Step [9/47], Loss: 0.1475\n",
      "Epoch [158/5000], Step [18/47], Loss: 0.3015\n",
      "Epoch [158/5000], Step [27/47], Loss: 0.0829\n",
      "Epoch [158/5000], Step [36/47], Loss: 0.1939\n",
      "Epoch [158/5000], Step [45/47], Loss: 0.1533\n",
      "Epoch [158/5000], Avg. Train Sample Loss: 0.1619, Avg. Validate Sample Loss: 0.1621,                             L2 Loss: 0.0303\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [159/5000] -------------------- \n",
      "Epoch [159/5000], Step [1/47], Loss: 0.1418\n",
      "Epoch [159/5000], Step [9/47], Loss: 0.2261\n",
      "Epoch [159/5000], Step [18/47], Loss: 0.2204\n",
      "Epoch [159/5000], Step [27/47], Loss: 0.1545\n",
      "Epoch [159/5000], Step [36/47], Loss: 0.0976\n",
      "Epoch [159/5000], Step [45/47], Loss: 0.1566\n",
      "Epoch [159/5000], Avg. Train Sample Loss: 0.1610, Avg. Validate Sample Loss: 0.1210,                             L2 Loss: 0.0228\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [160/5000] -------------------- \n",
      "Epoch [160/5000], Step [1/47], Loss: 0.1675\n",
      "Epoch [160/5000], Step [9/47], Loss: 0.1764\n",
      "Epoch [160/5000], Step [18/47], Loss: 0.1235\n",
      "Epoch [160/5000], Step [27/47], Loss: 0.2237\n",
      "Epoch [160/5000], Step [36/47], Loss: 0.0774\n",
      "Epoch [160/5000], Step [45/47], Loss: 0.1593\n",
      "Epoch [160/5000], Avg. Train Sample Loss: 0.1537, Avg. Validate Sample Loss: 0.1749,                             L2 Loss: 0.0250\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [161/5000] -------------------- \n",
      "Epoch [161/5000], Step [1/47], Loss: 0.0847\n",
      "Epoch [161/5000], Step [9/47], Loss: 0.1306\n",
      "Epoch [161/5000], Step [18/47], Loss: 0.1225\n",
      "Epoch [161/5000], Step [27/47], Loss: 0.2127\n",
      "Epoch [161/5000], Step [36/47], Loss: 0.1328\n",
      "Epoch [161/5000], Step [45/47], Loss: 0.0944\n",
      "Epoch [161/5000], Avg. Train Sample Loss: 0.1538, Avg. Validate Sample Loss: 0.1588,                             L2 Loss: 0.0308\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [162/5000] -------------------- \n",
      "Epoch [162/5000], Step [1/47], Loss: 0.0846\n",
      "Epoch [162/5000], Step [9/47], Loss: 0.1020\n",
      "Epoch [162/5000], Step [18/47], Loss: 0.2030\n",
      "Epoch [162/5000], Step [27/47], Loss: 0.0847\n",
      "Epoch [162/5000], Step [36/47], Loss: 0.2348\n",
      "Epoch [162/5000], Step [45/47], Loss: 0.1216\n",
      "Epoch [162/5000], Avg. Train Sample Loss: 0.1522, Avg. Validate Sample Loss: 0.1481,                             L2 Loss: 0.0288\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [163/5000] -------------------- \n",
      "Epoch [163/5000], Step [1/47], Loss: 0.2175\n",
      "Epoch [163/5000], Step [9/47], Loss: 0.0913\n",
      "Epoch [163/5000], Step [18/47], Loss: 0.1515\n",
      "Epoch [163/5000], Step [27/47], Loss: 0.1573\n",
      "Epoch [163/5000], Step [36/47], Loss: 0.2207\n",
      "Epoch [163/5000], Step [45/47], Loss: 0.1313\n",
      "Epoch [163/5000], Avg. Train Sample Loss: 0.1500, Avg. Validate Sample Loss: 0.1215,                             L2 Loss: 0.0266\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [164/5000] -------------------- \n",
      "Epoch [164/5000], Step [1/47], Loss: 0.0827\n",
      "Epoch [164/5000], Step [9/47], Loss: 0.1002\n",
      "Epoch [164/5000], Step [18/47], Loss: 0.0702\n",
      "Epoch [164/5000], Step [27/47], Loss: 0.0740\n",
      "Epoch [164/5000], Step [36/47], Loss: 0.1340\n",
      "Epoch [164/5000], Step [45/47], Loss: 0.1216\n",
      "Epoch [164/5000], Avg. Train Sample Loss: 0.1476, Avg. Validate Sample Loss: 0.1758,                             L2 Loss: 0.0247\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [165/5000] -------------------- \n",
      "Epoch [165/5000], Step [1/47], Loss: 0.2231\n",
      "Epoch [165/5000], Step [9/47], Loss: 0.1823\n",
      "Epoch [165/5000], Step [18/47], Loss: 0.1911\n",
      "Epoch [165/5000], Step [27/47], Loss: 0.0864\n",
      "Epoch [165/5000], Step [36/47], Loss: 0.1080\n",
      "Epoch [165/5000], Step [45/47], Loss: 0.1944\n",
      "Epoch [165/5000], Avg. Train Sample Loss: 0.1525, Avg. Validate Sample Loss: 0.1587,                             L2 Loss: 0.0299\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [166/5000] -------------------- \n",
      "Epoch [166/5000], Step [1/47], Loss: 0.1904\n",
      "Epoch [166/5000], Step [9/47], Loss: 0.1549\n",
      "Epoch [166/5000], Step [18/47], Loss: 0.2619\n",
      "Epoch [166/5000], Step [27/47], Loss: 0.1911\n",
      "Epoch [166/5000], Step [36/47], Loss: 0.0914\n",
      "Epoch [166/5000], Step [45/47], Loss: 0.1127\n",
      "Epoch [166/5000], Avg. Train Sample Loss: 0.1473, Avg. Validate Sample Loss: 0.1382,                             L2 Loss: 0.0248\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [167/5000] -------------------- \n",
      "Epoch [167/5000], Step [1/47], Loss: 0.1381\n",
      "Epoch [167/5000], Step [9/47], Loss: 0.2332\n",
      "Epoch [167/5000], Step [18/47], Loss: 0.0705\n",
      "Epoch [167/5000], Step [27/47], Loss: 0.1579\n",
      "Epoch [167/5000], Step [36/47], Loss: 0.0789\n",
      "Epoch [167/5000], Step [45/47], Loss: 0.2384\n",
      "Epoch [167/5000], Avg. Train Sample Loss: 0.1473, Avg. Validate Sample Loss: 0.1517,                             L2 Loss: 0.0308\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [168/5000] -------------------- \n",
      "Epoch [168/5000], Step [1/47], Loss: 0.1649\n",
      "Epoch [168/5000], Step [9/47], Loss: 0.1223\n",
      "Epoch [168/5000], Step [18/47], Loss: 0.1157\n",
      "Epoch [168/5000], Step [27/47], Loss: 0.0782\n",
      "Epoch [168/5000], Step [36/47], Loss: 0.1235\n",
      "Epoch [168/5000], Step [45/47], Loss: 0.1551\n",
      "Epoch [168/5000], Avg. Train Sample Loss: 0.1507, Avg. Validate Sample Loss: 0.1449,                             L2 Loss: 0.0221\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [169/5000] -------------------- \n",
      "Epoch [169/5000], Step [1/47], Loss: 0.1929\n",
      "Epoch [169/5000], Step [9/47], Loss: 0.1309\n",
      "Epoch [169/5000], Step [18/47], Loss: 0.1474\n",
      "Epoch [169/5000], Step [27/47], Loss: 0.2295\n",
      "Epoch [169/5000], Step [36/47], Loss: 0.1080\n",
      "Epoch [169/5000], Step [45/47], Loss: 0.1572\n",
      "Epoch [169/5000], Avg. Train Sample Loss: 0.1466, Avg. Validate Sample Loss: 0.1189,                             L2 Loss: 0.0265\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [170/5000] -------------------- \n",
      "Epoch [170/5000], Step [1/47], Loss: 0.2378\n",
      "Epoch [170/5000], Step [9/47], Loss: 0.1243\n",
      "Epoch [170/5000], Step [18/47], Loss: 0.0763\n",
      "Epoch [170/5000], Step [27/47], Loss: 0.0984\n",
      "Epoch [170/5000], Step [36/47], Loss: 0.1256\n",
      "Epoch [170/5000], Step [45/47], Loss: 0.1658\n",
      "Epoch [170/5000], Avg. Train Sample Loss: 0.1442, Avg. Validate Sample Loss: 0.1127,                             L2 Loss: 0.0206\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [171/5000] -------------------- \n",
      "Epoch [171/5000], Step [1/47], Loss: 0.0880\n",
      "Epoch [171/5000], Step [9/47], Loss: 0.1153\n",
      "Epoch [171/5000], Step [18/47], Loss: 0.1941\n",
      "Epoch [171/5000], Step [27/47], Loss: 0.0467\n",
      "Epoch [171/5000], Step [36/47], Loss: 0.1437\n",
      "Epoch [171/5000], Step [45/47], Loss: 0.1195\n",
      "Epoch [171/5000], Avg. Train Sample Loss: 0.1422, Avg. Validate Sample Loss: 0.1381,                             L2 Loss: 0.0251\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [172/5000] -------------------- \n",
      "Epoch [172/5000], Step [1/47], Loss: 0.0708\n",
      "Epoch [172/5000], Step [9/47], Loss: 0.0940\n",
      "Epoch [172/5000], Step [18/47], Loss: 0.0874\n",
      "Epoch [172/5000], Step [27/47], Loss: 0.2615\n",
      "Epoch [172/5000], Step [36/47], Loss: 0.2202\n",
      "Epoch [172/5000], Step [45/47], Loss: 0.0814\n",
      "Epoch [172/5000], Avg. Train Sample Loss: 0.1403, Avg. Validate Sample Loss: 0.1232,                             L2 Loss: 0.0208\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [173/5000] -------------------- \n",
      "Epoch [173/5000], Step [1/47], Loss: 0.1172\n",
      "Epoch [173/5000], Step [9/47], Loss: 0.1289\n",
      "Epoch [173/5000], Step [18/47], Loss: 0.0934\n",
      "Epoch [173/5000], Step [27/47], Loss: 0.1113\n",
      "Epoch [173/5000], Step [36/47], Loss: 0.1595\n",
      "Epoch [173/5000], Step [45/47], Loss: 0.0751\n",
      "Epoch [173/5000], Avg. Train Sample Loss: 0.1435, Avg. Validate Sample Loss: 0.1474,                             L2 Loss: 0.0282\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [174/5000] -------------------- \n",
      "Epoch [174/5000], Step [1/47], Loss: 0.1184\n",
      "Epoch [174/5000], Step [9/47], Loss: 0.0752\n",
      "Epoch [174/5000], Step [18/47], Loss: 0.0665\n",
      "Epoch [174/5000], Step [27/47], Loss: 0.1750\n",
      "Epoch [174/5000], Step [36/47], Loss: 0.1023\n",
      "Epoch [174/5000], Step [45/47], Loss: 0.1811\n",
      "Epoch [174/5000], Avg. Train Sample Loss: 0.1408, Avg. Validate Sample Loss: 0.1372,                             L2 Loss: 0.0202\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [175/5000] -------------------- \n",
      "Epoch [175/5000], Step [1/47], Loss: 0.1226\n",
      "Epoch [175/5000], Step [9/47], Loss: 0.1190\n",
      "Epoch [175/5000], Step [18/47], Loss: 0.1048\n",
      "Epoch [175/5000], Step [27/47], Loss: 0.0716\n",
      "Epoch [175/5000], Step [36/47], Loss: 0.2716\n",
      "Epoch [175/5000], Step [45/47], Loss: 0.1746\n",
      "Epoch [175/5000], Avg. Train Sample Loss: 0.1416, Avg. Validate Sample Loss: 0.1128,                             L2 Loss: 0.0228\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [176/5000] -------------------- \n",
      "Epoch [176/5000], Step [1/47], Loss: 0.0874\n",
      "Epoch [176/5000], Step [9/47], Loss: 0.0684\n",
      "Epoch [176/5000], Step [18/47], Loss: 0.2133\n",
      "Epoch [176/5000], Step [27/47], Loss: 0.1073\n",
      "Epoch [176/5000], Step [36/47], Loss: 0.1397\n",
      "Epoch [176/5000], Step [45/47], Loss: 0.1508\n",
      "Epoch [176/5000], Avg. Train Sample Loss: 0.1370, Avg. Validate Sample Loss: 0.1747,                             L2 Loss: 0.0338\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [177/5000] -------------------- \n",
      "Epoch [177/5000], Step [1/47], Loss: 0.1384\n",
      "Epoch [177/5000], Step [9/47], Loss: 0.1336\n",
      "Epoch [177/5000], Step [18/47], Loss: 0.1674\n",
      "Epoch [177/5000], Step [27/47], Loss: 0.1582\n",
      "Epoch [177/5000], Step [36/47], Loss: 0.1163\n",
      "Epoch [177/5000], Step [45/47], Loss: 0.0787\n",
      "Epoch [177/5000], Avg. Train Sample Loss: 0.1411, Avg. Validate Sample Loss: 0.1485,                             L2 Loss: 0.0286\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [178/5000] -------------------- \n",
      "Epoch [178/5000], Step [1/47], Loss: 0.0795\n",
      "Epoch [178/5000], Step [9/47], Loss: 0.2496\n",
      "Epoch [178/5000], Step [18/47], Loss: 0.0665\n",
      "Epoch [178/5000], Step [27/47], Loss: 0.0648\n",
      "Epoch [178/5000], Step [36/47], Loss: 0.1482\n",
      "Epoch [178/5000], Step [45/47], Loss: 0.1423\n",
      "Epoch [178/5000], Avg. Train Sample Loss: 0.1377, Avg. Validate Sample Loss: 0.1564,                             L2 Loss: 0.0309\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [179/5000] -------------------- \n",
      "Epoch [179/5000], Step [1/47], Loss: 0.1513\n",
      "Epoch [179/5000], Step [9/47], Loss: 0.0733\n",
      "Epoch [179/5000], Step [18/47], Loss: 0.0904\n",
      "Epoch [179/5000], Step [27/47], Loss: 0.1280\n",
      "Epoch [179/5000], Step [36/47], Loss: 0.2465\n",
      "Epoch [179/5000], Step [45/47], Loss: 0.0667\n",
      "Epoch [179/5000], Avg. Train Sample Loss: 0.1336, Avg. Validate Sample Loss: 0.1108,                             L2 Loss: 0.0253\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [180/5000] -------------------- \n",
      "Epoch [180/5000], Step [1/47], Loss: 0.0799\n",
      "Epoch [180/5000], Step [9/47], Loss: 0.0679\n",
      "Epoch [180/5000], Step [18/47], Loss: 0.1370\n",
      "Epoch [180/5000], Step [27/47], Loss: 0.1678\n",
      "Epoch [180/5000], Step [36/47], Loss: 0.1462\n",
      "Epoch [180/5000], Step [45/47], Loss: 0.0709\n",
      "Epoch [180/5000], Avg. Train Sample Loss: 0.1347, Avg. Validate Sample Loss: 0.1729,                             L2 Loss: 0.0280\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [181/5000] -------------------- \n",
      "Epoch [181/5000], Step [1/47], Loss: 0.2342\n",
      "Epoch [181/5000], Step [9/47], Loss: 0.0936\n",
      "Epoch [181/5000], Step [18/47], Loss: 0.1216\n",
      "Epoch [181/5000], Step [27/47], Loss: 0.0920\n",
      "Epoch [181/5000], Step [36/47], Loss: 0.1048\n",
      "Epoch [181/5000], Step [45/47], Loss: 0.0837\n",
      "Epoch [181/5000], Avg. Train Sample Loss: 0.1406, Avg. Validate Sample Loss: 0.1172,                             L2 Loss: 0.0236\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [182/5000] -------------------- \n",
      "Epoch [182/5000], Step [1/47], Loss: 0.1811\n",
      "Epoch [182/5000], Step [9/47], Loss: 0.1096\n",
      "Epoch [182/5000], Step [18/47], Loss: 0.0612\n",
      "Epoch [182/5000], Step [27/47], Loss: 0.1113\n",
      "Epoch [182/5000], Step [36/47], Loss: 0.1443\n",
      "Epoch [182/5000], Step [45/47], Loss: 0.1929\n",
      "Epoch [182/5000], Avg. Train Sample Loss: 0.1324, Avg. Validate Sample Loss: 0.1282,                             L2 Loss: 0.0258\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [183/5000] -------------------- \n",
      "Epoch [183/5000], Step [1/47], Loss: 0.0783\n",
      "Epoch [183/5000], Step [9/47], Loss: 0.1812\n",
      "Epoch [183/5000], Step [18/47], Loss: 0.0639\n",
      "Epoch [183/5000], Step [27/47], Loss: 0.0721\n",
      "Epoch [183/5000], Step [36/47], Loss: 0.1466\n",
      "Epoch [183/5000], Step [45/47], Loss: 0.1054\n",
      "Epoch [183/5000], Avg. Train Sample Loss: 0.1324, Avg. Validate Sample Loss: 0.1368,                             L2 Loss: 0.0231\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [184/5000] -------------------- \n",
      "Epoch [184/5000], Step [1/47], Loss: 0.1505\n",
      "Epoch [184/5000], Step [9/47], Loss: 0.0802\n",
      "Epoch [184/5000], Step [18/47], Loss: 0.1141\n",
      "Epoch [184/5000], Step [27/47], Loss: 0.1370\n",
      "Epoch [184/5000], Step [36/47], Loss: 0.0879\n",
      "Epoch [184/5000], Step [45/47], Loss: 0.0881\n",
      "Epoch [184/5000], Avg. Train Sample Loss: 0.1316, Avg. Validate Sample Loss: 0.1073,                             L2 Loss: 0.0181\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [185/5000] -------------------- \n",
      "Epoch [185/5000], Step [1/47], Loss: 0.1258\n",
      "Epoch [185/5000], Step [9/47], Loss: 0.1246\n",
      "Epoch [185/5000], Step [18/47], Loss: 0.2487\n",
      "Epoch [185/5000], Step [27/47], Loss: 0.0866\n",
      "Epoch [185/5000], Step [36/47], Loss: 0.1373\n",
      "Epoch [185/5000], Step [45/47], Loss: 0.0759\n",
      "Epoch [185/5000], Avg. Train Sample Loss: 0.1313, Avg. Validate Sample Loss: 0.1446,                             L2 Loss: 0.0188\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [186/5000] -------------------- \n",
      "Epoch [186/5000], Step [1/47], Loss: 0.1094\n",
      "Epoch [186/5000], Step [9/47], Loss: 0.0829\n",
      "Epoch [186/5000], Step [18/47], Loss: 0.2266\n",
      "Epoch [186/5000], Step [27/47], Loss: 0.0729\n",
      "Epoch [186/5000], Step [36/47], Loss: 0.2204\n",
      "Epoch [186/5000], Step [45/47], Loss: 0.1628\n",
      "Epoch [186/5000], Avg. Train Sample Loss: 0.1420, Avg. Validate Sample Loss: 0.1121,                             L2 Loss: 0.0175\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [187/5000] -------------------- \n",
      "Epoch [187/5000], Step [1/47], Loss: 0.0713\n",
      "Epoch [187/5000], Step [9/47], Loss: 0.1568\n",
      "Epoch [187/5000], Step [18/47], Loss: 0.1110\n",
      "Epoch [187/5000], Step [27/47], Loss: 0.2129\n",
      "Epoch [187/5000], Step [36/47], Loss: 0.0874\n",
      "Epoch [187/5000], Step [45/47], Loss: 0.1213\n",
      "Epoch [187/5000], Avg. Train Sample Loss: 0.1272, Avg. Validate Sample Loss: 0.1028,                             L2 Loss: 0.0204\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [188/5000] -------------------- \n",
      "Epoch [188/5000], Step [1/47], Loss: 0.0689\n",
      "Epoch [188/5000], Step [9/47], Loss: 0.1148\n",
      "Epoch [188/5000], Step [18/47], Loss: 0.1805\n",
      "Epoch [188/5000], Step [27/47], Loss: 0.1745\n",
      "Epoch [188/5000], Step [36/47], Loss: 0.1661\n",
      "Epoch [188/5000], Step [45/47], Loss: 0.1544\n",
      "Epoch [188/5000], Avg. Train Sample Loss: 0.1289, Avg. Validate Sample Loss: 0.1412,                             L2 Loss: 0.0188\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [189/5000] -------------------- \n",
      "Epoch [189/5000], Step [1/47], Loss: 0.0943\n",
      "Epoch [189/5000], Step [9/47], Loss: 0.1257\n",
      "Epoch [189/5000], Step [18/47], Loss: 0.0800\n",
      "Epoch [189/5000], Step [27/47], Loss: 0.0955\n",
      "Epoch [189/5000], Step [36/47], Loss: 0.0903\n",
      "Epoch [189/5000], Step [45/47], Loss: 0.1608\n",
      "Epoch [189/5000], Avg. Train Sample Loss: 0.1253, Avg. Validate Sample Loss: 0.1288,                             L2 Loss: 0.0247\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [190/5000] -------------------- \n",
      "Epoch [190/5000], Step [1/47], Loss: 0.1916\n",
      "Epoch [190/5000], Step [9/47], Loss: 0.1211\n",
      "Epoch [190/5000], Step [18/47], Loss: 0.0813\n",
      "Epoch [190/5000], Step [27/47], Loss: 0.0834\n",
      "Epoch [190/5000], Step [36/47], Loss: 0.1417\n",
      "Epoch [190/5000], Step [45/47], Loss: 0.1795\n",
      "Epoch [190/5000], Avg. Train Sample Loss: 0.1257, Avg. Validate Sample Loss: 0.1288,                             L2 Loss: 0.0243\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [191/5000] -------------------- \n",
      "Epoch [191/5000], Step [1/47], Loss: 0.1082\n",
      "Epoch [191/5000], Step [9/47], Loss: 0.1086\n",
      "Epoch [191/5000], Step [18/47], Loss: 0.1546\n",
      "Epoch [191/5000], Step [27/47], Loss: 0.1220\n",
      "Epoch [191/5000], Step [36/47], Loss: 0.1341\n",
      "Epoch [191/5000], Step [45/47], Loss: 0.1601\n",
      "Epoch [191/5000], Avg. Train Sample Loss: 0.1220, Avg. Validate Sample Loss: 0.1222,                             L2 Loss: 0.0201\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [192/5000] -------------------- \n",
      "Epoch [192/5000], Step [1/47], Loss: 0.0823\n",
      "Epoch [192/5000], Step [9/47], Loss: 0.2216\n",
      "Epoch [192/5000], Step [18/47], Loss: 0.0820\n",
      "Epoch [192/5000], Step [27/47], Loss: 0.1783\n",
      "Epoch [192/5000], Step [36/47], Loss: 0.1933\n",
      "Epoch [192/5000], Step [45/47], Loss: 0.1460\n",
      "Epoch [192/5000], Avg. Train Sample Loss: 0.1268, Avg. Validate Sample Loss: 0.1049,                             L2 Loss: 0.0228\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [193/5000] -------------------- \n",
      "Epoch [193/5000], Step [1/47], Loss: 0.2570\n",
      "Epoch [193/5000], Step [9/47], Loss: 0.1026\n",
      "Epoch [193/5000], Step [18/47], Loss: 0.1185\n",
      "Epoch [193/5000], Step [27/47], Loss: 0.1725\n",
      "Epoch [193/5000], Step [36/47], Loss: 0.1332\n",
      "Epoch [193/5000], Step [45/47], Loss: 0.1176\n",
      "Epoch [193/5000], Avg. Train Sample Loss: 0.1226, Avg. Validate Sample Loss: 0.1292,                             L2 Loss: 0.0182\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [194/5000] -------------------- \n",
      "Epoch [194/5000], Step [1/47], Loss: 0.1088\n",
      "Epoch [194/5000], Step [9/47], Loss: 0.0784\n",
      "Epoch [194/5000], Step [18/47], Loss: 0.0563\n",
      "Epoch [194/5000], Step [27/47], Loss: 0.1977\n",
      "Epoch [194/5000], Step [36/47], Loss: 0.1568\n",
      "Epoch [194/5000], Step [45/47], Loss: 0.1270\n",
      "Epoch [194/5000], Avg. Train Sample Loss: 0.1377, Avg. Validate Sample Loss: 0.1468,                             L2 Loss: 0.0377\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [195/5000] -------------------- \n",
      "Epoch [195/5000], Step [1/47], Loss: 0.1531\n",
      "Epoch [195/5000], Step [9/47], Loss: 0.0902\n",
      "Epoch [195/5000], Step [18/47], Loss: 0.1331\n",
      "Epoch [195/5000], Step [27/47], Loss: 0.1952\n",
      "Epoch [195/5000], Step [36/47], Loss: 0.0918\n",
      "Epoch [195/5000], Step [45/47], Loss: 0.3000\n",
      "Epoch [195/5000], Avg. Train Sample Loss: 0.1250, Avg. Validate Sample Loss: 0.1381,                             L2 Loss: 0.0197\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [196/5000] -------------------- \n",
      "Epoch [196/5000], Step [1/47], Loss: 0.1274\n",
      "Epoch [196/5000], Step [9/47], Loss: 0.1522\n",
      "Epoch [196/5000], Step [18/47], Loss: 0.1040\n",
      "Epoch [196/5000], Step [27/47], Loss: 0.1251\n",
      "Epoch [196/5000], Step [36/47], Loss: 0.0796\n",
      "Epoch [196/5000], Step [45/47], Loss: 0.0927\n",
      "Epoch [196/5000], Avg. Train Sample Loss: 0.1179, Avg. Validate Sample Loss: 0.1412,                             L2 Loss: 0.0265\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [197/5000] -------------------- \n",
      "Epoch [197/5000], Step [1/47], Loss: 0.1087\n",
      "Epoch [197/5000], Step [9/47], Loss: 0.1252\n",
      "Epoch [197/5000], Step [18/47], Loss: 0.1233\n",
      "Epoch [197/5000], Step [27/47], Loss: 0.0946\n",
      "Epoch [197/5000], Step [36/47], Loss: 0.0869\n",
      "Epoch [197/5000], Step [45/47], Loss: 0.1156\n",
      "Epoch [197/5000], Avg. Train Sample Loss: 0.1210, Avg. Validate Sample Loss: 0.1423,                             L2 Loss: 0.0267\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [198/5000] -------------------- \n",
      "Epoch [198/5000], Step [1/47], Loss: 0.1221\n",
      "Epoch [198/5000], Step [9/47], Loss: 0.2095\n",
      "Epoch [198/5000], Step [18/47], Loss: 0.2238\n",
      "Epoch [198/5000], Step [27/47], Loss: 0.1030\n",
      "Epoch [198/5000], Step [36/47], Loss: 0.1919\n",
      "Epoch [198/5000], Step [45/47], Loss: 0.0395\n",
      "Epoch [198/5000], Avg. Train Sample Loss: 0.1230, Avg. Validate Sample Loss: 0.1339,                             L2 Loss: 0.0219\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [199/5000] -------------------- \n",
      "Epoch [199/5000], Step [1/47], Loss: 0.0969\n",
      "Epoch [199/5000], Step [9/47], Loss: 0.1101\n",
      "Epoch [199/5000], Step [18/47], Loss: 0.1875\n",
      "Epoch [199/5000], Step [27/47], Loss: 0.1850\n",
      "Epoch [199/5000], Step [36/47], Loss: 0.1112\n",
      "Epoch [199/5000], Step [45/47], Loss: 0.0957\n",
      "Epoch [199/5000], Avg. Train Sample Loss: 0.1257, Avg. Validate Sample Loss: 0.1291,                             L2 Loss: 0.0275\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [200/5000] -------------------- \n",
      "Epoch [200/5000], Step [1/47], Loss: 0.0846\n",
      "Epoch [200/5000], Step [9/47], Loss: 0.2121\n",
      "Epoch [200/5000], Step [18/47], Loss: 0.0888\n",
      "Epoch [200/5000], Step [27/47], Loss: 0.1524\n",
      "Epoch [200/5000], Step [36/47], Loss: 0.0962\n",
      "Epoch [200/5000], Step [45/47], Loss: 0.1098\n",
      "Epoch [200/5000], Avg. Train Sample Loss: 0.1175, Avg. Validate Sample Loss: 0.1196,                             L2 Loss: 0.0224\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [201/5000] -------------------- \n",
      "Epoch [201/5000], Step [1/47], Loss: 0.1623\n",
      "Epoch [201/5000], Step [9/47], Loss: 0.1057\n",
      "Epoch [201/5000], Step [18/47], Loss: 0.0990\n",
      "Epoch [201/5000], Step [27/47], Loss: 0.1006\n",
      "Epoch [201/5000], Step [36/47], Loss: 0.1193\n",
      "Epoch [201/5000], Step [45/47], Loss: 0.0605\n",
      "Epoch [201/5000], Avg. Train Sample Loss: 0.1140, Avg. Validate Sample Loss: 0.0985,                             L2 Loss: 0.0161\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [202/5000] -------------------- \n",
      "Epoch [202/5000], Step [1/47], Loss: 0.0743\n",
      "Epoch [202/5000], Step [9/47], Loss: 0.1804\n",
      "Epoch [202/5000], Step [18/47], Loss: 0.1641\n",
      "Epoch [202/5000], Step [27/47], Loss: 0.0957\n",
      "Epoch [202/5000], Step [36/47], Loss: 0.1312\n",
      "Epoch [202/5000], Step [45/47], Loss: 0.0754\n",
      "Epoch [202/5000], Avg. Train Sample Loss: 0.1140, Avg. Validate Sample Loss: 0.1024,                             L2 Loss: 0.0164\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [203/5000] -------------------- \n",
      "Epoch [203/5000], Step [1/47], Loss: 0.1280\n",
      "Epoch [203/5000], Step [9/47], Loss: 0.1035\n",
      "Epoch [203/5000], Step [18/47], Loss: 0.2061\n",
      "Epoch [203/5000], Step [27/47], Loss: 0.1579\n",
      "Epoch [203/5000], Step [36/47], Loss: 0.2184\n",
      "Epoch [203/5000], Step [45/47], Loss: 0.1438\n",
      "Epoch [203/5000], Avg. Train Sample Loss: 0.1276, Avg. Validate Sample Loss: 0.1190,                             L2 Loss: 0.0219\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [204/5000] -------------------- \n",
      "Epoch [204/5000], Step [1/47], Loss: 0.1342\n",
      "Epoch [204/5000], Step [9/47], Loss: 0.1021\n",
      "Epoch [204/5000], Step [18/47], Loss: 0.1092\n",
      "Epoch [204/5000], Step [27/47], Loss: 0.0494\n",
      "Epoch [204/5000], Step [36/47], Loss: 0.1054\n",
      "Epoch [204/5000], Step [45/47], Loss: 0.1623\n",
      "Epoch [204/5000], Avg. Train Sample Loss: 0.1086, Avg. Validate Sample Loss: 0.1056,                             L2 Loss: 0.0171\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [205/5000] -------------------- \n",
      "Epoch [205/5000], Step [1/47], Loss: 0.1279\n",
      "Epoch [205/5000], Step [9/47], Loss: 0.0845\n",
      "Epoch [205/5000], Step [18/47], Loss: 0.0397\n",
      "Epoch [205/5000], Step [27/47], Loss: 0.0965\n",
      "Epoch [205/5000], Step [36/47], Loss: 0.1320\n",
      "Epoch [205/5000], Step [45/47], Loss: 0.1822\n",
      "Epoch [205/5000], Avg. Train Sample Loss: 0.1114, Avg. Validate Sample Loss: 0.1043,                             L2 Loss: 0.0208\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [206/5000] -------------------- \n",
      "Epoch [206/5000], Step [1/47], Loss: 0.1068\n",
      "Epoch [206/5000], Step [9/47], Loss: 0.0967\n",
      "Epoch [206/5000], Step [18/47], Loss: 0.1996\n",
      "Epoch [206/5000], Step [27/47], Loss: 0.1162\n",
      "Epoch [206/5000], Step [36/47], Loss: 0.1321\n",
      "Epoch [206/5000], Step [45/47], Loss: 0.0973\n",
      "Epoch [206/5000], Avg. Train Sample Loss: 0.1117, Avg. Validate Sample Loss: 0.0943,                             L2 Loss: 0.0186\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [207/5000] -------------------- \n",
      "Epoch [207/5000], Step [1/47], Loss: 0.0484\n",
      "Epoch [207/5000], Step [9/47], Loss: 0.0373\n",
      "Epoch [207/5000], Step [18/47], Loss: 0.1599\n",
      "Epoch [207/5000], Step [27/47], Loss: 0.2034\n",
      "Epoch [207/5000], Step [36/47], Loss: 0.0906\n",
      "Epoch [207/5000], Step [45/47], Loss: 0.1632\n",
      "Epoch [207/5000], Avg. Train Sample Loss: 0.1061, Avg. Validate Sample Loss: 0.1024,                             L2 Loss: 0.0165\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [208/5000] -------------------- \n",
      "Epoch [208/5000], Step [1/47], Loss: 0.1401\n",
      "Epoch [208/5000], Step [9/47], Loss: 0.0951\n",
      "Epoch [208/5000], Step [18/47], Loss: 0.1717\n",
      "Epoch [208/5000], Step [27/47], Loss: 0.1609\n",
      "Epoch [208/5000], Step [36/47], Loss: 0.0916\n",
      "Epoch [208/5000], Step [45/47], Loss: 0.1153\n",
      "Epoch [208/5000], Avg. Train Sample Loss: 0.1093, Avg. Validate Sample Loss: 0.1071,                             L2 Loss: 0.0189\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [209/5000] -------------------- \n",
      "Epoch [209/5000], Step [1/47], Loss: 0.0662\n",
      "Epoch [209/5000], Step [9/47], Loss: 0.1305\n",
      "Epoch [209/5000], Step [18/47], Loss: 0.1204\n",
      "Epoch [209/5000], Step [27/47], Loss: 0.0516\n",
      "Epoch [209/5000], Step [36/47], Loss: 0.0656\n",
      "Epoch [209/5000], Step [45/47], Loss: 0.0510\n",
      "Epoch [209/5000], Avg. Train Sample Loss: 0.1039, Avg. Validate Sample Loss: 0.1133,                             L2 Loss: 0.0154\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [210/5000] -------------------- \n",
      "Epoch [210/5000], Step [1/47], Loss: 0.1173\n",
      "Epoch [210/5000], Step [9/47], Loss: 0.0791\n",
      "Epoch [210/5000], Step [18/47], Loss: 0.1820\n",
      "Epoch [210/5000], Step [27/47], Loss: 0.0985\n",
      "Epoch [210/5000], Step [36/47], Loss: 0.1142\n",
      "Epoch [210/5000], Step [45/47], Loss: 0.2155\n",
      "Epoch [210/5000], Avg. Train Sample Loss: 0.1019, Avg. Validate Sample Loss: 0.1097,                             L2 Loss: 0.0111\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [211/5000] -------------------- \n",
      "Epoch [211/5000], Step [1/47], Loss: 0.2045\n",
      "Epoch [211/5000], Step [9/47], Loss: 0.0467\n",
      "Epoch [211/5000], Step [18/47], Loss: 0.0548\n",
      "Epoch [211/5000], Step [27/47], Loss: 0.0599\n",
      "Epoch [211/5000], Step [36/47], Loss: 0.0528\n",
      "Epoch [211/5000], Step [45/47], Loss: 0.2373\n",
      "Epoch [211/5000], Avg. Train Sample Loss: 0.1019, Avg. Validate Sample Loss: 0.0874,                             L2 Loss: 0.0117\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [212/5000] -------------------- \n",
      "Epoch [212/5000], Step [1/47], Loss: 0.0869\n",
      "Epoch [212/5000], Step [9/47], Loss: 0.1062\n",
      "Epoch [212/5000], Step [18/47], Loss: 0.1588\n",
      "Epoch [212/5000], Step [27/47], Loss: 0.1062\n",
      "Epoch [212/5000], Step [36/47], Loss: 0.0954\n",
      "Epoch [212/5000], Step [45/47], Loss: 0.0603\n",
      "Epoch [212/5000], Avg. Train Sample Loss: 0.1065, Avg. Validate Sample Loss: 0.1246,                             L2 Loss: 0.0169\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [213/5000] -------------------- \n",
      "Epoch [213/5000], Step [1/47], Loss: 0.0557\n",
      "Epoch [213/5000], Step [9/47], Loss: 0.0719\n",
      "Epoch [213/5000], Step [18/47], Loss: 0.1414\n",
      "Epoch [213/5000], Step [27/47], Loss: 0.0531\n",
      "Epoch [213/5000], Step [36/47], Loss: 0.0671\n",
      "Epoch [213/5000], Step [45/47], Loss: 0.1398\n",
      "Epoch [213/5000], Avg. Train Sample Loss: 0.1039, Avg. Validate Sample Loss: 0.1143,                             L2 Loss: 0.0187\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [214/5000] -------------------- \n",
      "Epoch [214/5000], Step [1/47], Loss: 0.1809\n",
      "Epoch [214/5000], Step [9/47], Loss: 0.0673\n",
      "Epoch [214/5000], Step [18/47], Loss: 0.1772\n",
      "Epoch [214/5000], Step [27/47], Loss: 0.0690\n",
      "Epoch [214/5000], Step [36/47], Loss: 0.1503\n",
      "Epoch [214/5000], Step [45/47], Loss: 0.1198\n",
      "Epoch [214/5000], Avg. Train Sample Loss: 0.0982, Avg. Validate Sample Loss: 0.0831,                             L2 Loss: 0.0119\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [215/5000] -------------------- \n",
      "Epoch [215/5000], Step [1/47], Loss: 0.2493\n",
      "Epoch [215/5000], Step [9/47], Loss: 0.1400\n",
      "Epoch [215/5000], Step [18/47], Loss: 0.1412\n",
      "Epoch [215/5000], Step [27/47], Loss: 0.1310\n",
      "Epoch [215/5000], Step [36/47], Loss: 0.1208\n",
      "Epoch [215/5000], Step [45/47], Loss: 0.1544\n",
      "Epoch [215/5000], Avg. Train Sample Loss: 0.0983, Avg. Validate Sample Loss: 0.0874,                             L2 Loss: 0.0149\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [216/5000] -------------------- \n",
      "Epoch [216/5000], Step [1/47], Loss: 0.0762\n",
      "Epoch [216/5000], Step [9/47], Loss: 0.1274\n",
      "Epoch [216/5000], Step [18/47], Loss: 0.0634\n",
      "Epoch [216/5000], Step [27/47], Loss: 0.0709\n",
      "Epoch [216/5000], Step [36/47], Loss: 0.0526\n",
      "Epoch [216/5000], Step [45/47], Loss: 0.1031\n",
      "Epoch [216/5000], Avg. Train Sample Loss: 0.0956, Avg. Validate Sample Loss: 0.0882,                             L2 Loss: 0.0168\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [217/5000] -------------------- \n",
      "Epoch [217/5000], Step [1/47], Loss: 0.0899\n",
      "Epoch [217/5000], Step [9/47], Loss: 0.1131\n",
      "Epoch [217/5000], Step [18/47], Loss: 0.0969\n",
      "Epoch [217/5000], Step [27/47], Loss: 0.0623\n",
      "Epoch [217/5000], Step [36/47], Loss: 0.1107\n",
      "Epoch [217/5000], Step [45/47], Loss: 0.0774\n",
      "Epoch [217/5000], Avg. Train Sample Loss: 0.0952, Avg. Validate Sample Loss: 0.0889,                             L2 Loss: 0.0155\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [218/5000] -------------------- \n",
      "Epoch [218/5000], Step [1/47], Loss: 0.1316\n",
      "Epoch [218/5000], Step [9/47], Loss: 0.0882\n",
      "Epoch [218/5000], Step [18/47], Loss: 0.1165\n",
      "Epoch [218/5000], Step [27/47], Loss: 0.0475\n",
      "Epoch [218/5000], Step [36/47], Loss: 0.0521\n",
      "Epoch [218/5000], Step [45/47], Loss: 0.1311\n",
      "Epoch [218/5000], Avg. Train Sample Loss: 0.0924, Avg. Validate Sample Loss: 0.0862,                             L2 Loss: 0.0150\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [219/5000] -------------------- \n",
      "Epoch [219/5000], Step [1/47], Loss: 0.0709\n",
      "Epoch [219/5000], Step [9/47], Loss: 0.0364\n",
      "Epoch [219/5000], Step [18/47], Loss: 0.0907\n",
      "Epoch [219/5000], Step [27/47], Loss: 0.0506\n",
      "Epoch [219/5000], Step [36/47], Loss: 0.0890\n",
      "Epoch [219/5000], Step [45/47], Loss: 0.1338\n",
      "Epoch [219/5000], Avg. Train Sample Loss: 0.0931, Avg. Validate Sample Loss: 0.0833,                             L2 Loss: 0.0154\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [220/5000] -------------------- \n",
      "Epoch [220/5000], Step [1/47], Loss: 0.0388\n",
      "Epoch [220/5000], Step [9/47], Loss: 0.0981\n",
      "Epoch [220/5000], Step [18/47], Loss: 0.0764\n",
      "Epoch [220/5000], Step [27/47], Loss: 0.1618\n",
      "Epoch [220/5000], Step [36/47], Loss: 0.0904\n",
      "Epoch [220/5000], Step [45/47], Loss: 0.0831\n",
      "Epoch [220/5000], Avg. Train Sample Loss: 0.0937, Avg. Validate Sample Loss: 0.0731,                             L2 Loss: 0.0147\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [221/5000] -------------------- \n",
      "Epoch [221/5000], Step [1/47], Loss: 0.0459\n",
      "Epoch [221/5000], Step [9/47], Loss: 0.1063\n",
      "Epoch [221/5000], Step [18/47], Loss: 0.0509\n",
      "Epoch [221/5000], Step [27/47], Loss: 0.0749\n",
      "Epoch [221/5000], Step [36/47], Loss: 0.0622\n",
      "Epoch [221/5000], Step [45/47], Loss: 0.0512\n",
      "Epoch [221/5000], Avg. Train Sample Loss: 0.0903, Avg. Validate Sample Loss: 0.0748,                             L2 Loss: 0.0198\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [222/5000] -------------------- \n",
      "Epoch [222/5000], Step [1/47], Loss: 0.0916\n",
      "Epoch [222/5000], Step [9/47], Loss: 0.0804\n",
      "Epoch [222/5000], Step [18/47], Loss: 0.0704\n",
      "Epoch [222/5000], Step [27/47], Loss: 0.1104\n",
      "Epoch [222/5000], Step [36/47], Loss: 0.0718\n",
      "Epoch [222/5000], Step [45/47], Loss: 0.0237\n",
      "Epoch [222/5000], Avg. Train Sample Loss: 0.0914, Avg. Validate Sample Loss: 0.0767,                             L2 Loss: 0.0196\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [223/5000] -------------------- \n",
      "Epoch [223/5000], Step [1/47], Loss: 0.0560\n",
      "Epoch [223/5000], Step [9/47], Loss: 0.0974\n",
      "Epoch [223/5000], Step [18/47], Loss: 0.0547\n",
      "Epoch [223/5000], Step [27/47], Loss: 0.1049\n",
      "Epoch [223/5000], Step [36/47], Loss: 0.0382\n",
      "Epoch [223/5000], Step [45/47], Loss: 0.0474\n",
      "Epoch [223/5000], Avg. Train Sample Loss: 0.0922, Avg. Validate Sample Loss: 0.0753,                             L2 Loss: 0.0173\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [224/5000] -------------------- \n",
      "Epoch [224/5000], Step [1/47], Loss: 0.0328\n",
      "Epoch [224/5000], Step [9/47], Loss: 0.0868\n",
      "Epoch [224/5000], Step [18/47], Loss: 0.1057\n",
      "Epoch [224/5000], Step [27/47], Loss: 0.0416\n",
      "Epoch [224/5000], Step [36/47], Loss: 0.1131\n",
      "Epoch [224/5000], Step [45/47], Loss: 0.0300\n",
      "Epoch [224/5000], Avg. Train Sample Loss: 0.0877, Avg. Validate Sample Loss: 0.1156,                             L2 Loss: 0.0203\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [225/5000] -------------------- \n",
      "Epoch [225/5000], Step [1/47], Loss: 0.0940\n",
      "Epoch [225/5000], Step [9/47], Loss: 0.1833\n",
      "Epoch [225/5000], Step [18/47], Loss: 0.0966\n",
      "Epoch [225/5000], Step [27/47], Loss: 0.0705\n",
      "Epoch [225/5000], Step [36/47], Loss: 0.1085\n",
      "Epoch [225/5000], Step [45/47], Loss: 0.0527\n",
      "Epoch [225/5000], Avg. Train Sample Loss: 0.0900, Avg. Validate Sample Loss: 0.0709,                             L2 Loss: 0.0173\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [226/5000] -------------------- \n",
      "Epoch [226/5000], Step [1/47], Loss: 0.1285\n",
      "Epoch [226/5000], Step [9/47], Loss: 0.1087\n",
      "Epoch [226/5000], Step [18/47], Loss: 0.0411\n",
      "Epoch [226/5000], Step [27/47], Loss: 0.0535\n",
      "Epoch [226/5000], Step [36/47], Loss: 0.0289\n",
      "Epoch [226/5000], Step [45/47], Loss: 0.0461\n",
      "Epoch [226/5000], Avg. Train Sample Loss: 0.0848, Avg. Validate Sample Loss: 0.0731,                             L2 Loss: 0.0131\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [227/5000] -------------------- \n",
      "Epoch [227/5000], Step [1/47], Loss: 0.0467\n",
      "Epoch [227/5000], Step [9/47], Loss: 0.1052\n",
      "Epoch [227/5000], Step [18/47], Loss: 0.0416\n",
      "Epoch [227/5000], Step [27/47], Loss: 0.0438\n",
      "Epoch [227/5000], Step [36/47], Loss: 0.0547\n",
      "Epoch [227/5000], Step [45/47], Loss: 0.0507\n",
      "Epoch [227/5000], Avg. Train Sample Loss: 0.0834, Avg. Validate Sample Loss: 0.0773,                             L2 Loss: 0.0204\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [228/5000] -------------------- \n",
      "Epoch [228/5000], Step [1/47], Loss: 0.0446\n",
      "Epoch [228/5000], Step [9/47], Loss: 0.0678\n",
      "Epoch [228/5000], Step [18/47], Loss: 0.0658\n",
      "Epoch [228/5000], Step [27/47], Loss: 0.1378\n",
      "Epoch [228/5000], Step [36/47], Loss: 0.0312\n",
      "Epoch [228/5000], Step [45/47], Loss: 0.1384\n",
      "Epoch [228/5000], Avg. Train Sample Loss: 0.0840, Avg. Validate Sample Loss: 0.0985,                             L2 Loss: 0.0262\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [229/5000] -------------------- \n",
      "Epoch [229/5000], Step [1/47], Loss: 0.0591\n",
      "Epoch [229/5000], Step [9/47], Loss: 0.0703\n",
      "Epoch [229/5000], Step [18/47], Loss: 0.0314\n",
      "Epoch [229/5000], Step [27/47], Loss: 0.0542\n",
      "Epoch [229/5000], Step [36/47], Loss: 0.0838\n",
      "Epoch [229/5000], Step [45/47], Loss: 0.0708\n",
      "Epoch [229/5000], Avg. Train Sample Loss: 0.0877, Avg. Validate Sample Loss: 0.0895,                             L2 Loss: 0.0216\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [230/5000] -------------------- \n",
      "Epoch [230/5000], Step [1/47], Loss: 0.1009\n",
      "Epoch [230/5000], Step [9/47], Loss: 0.0797\n",
      "Epoch [230/5000], Step [18/47], Loss: 0.1225\n",
      "Epoch [230/5000], Step [27/47], Loss: 0.0805\n",
      "Epoch [230/5000], Step [36/47], Loss: 0.0577\n",
      "Epoch [230/5000], Step [45/47], Loss: 0.0361\n",
      "Epoch [230/5000], Avg. Train Sample Loss: 0.0796, Avg. Validate Sample Loss: 0.0730,                             L2 Loss: 0.0157\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [231/5000] -------------------- \n",
      "Epoch [231/5000], Step [1/47], Loss: 0.1198\n",
      "Epoch [231/5000], Step [9/47], Loss: 0.0642\n",
      "Epoch [231/5000], Step [18/47], Loss: 0.0567\n",
      "Epoch [231/5000], Step [27/47], Loss: 0.0598\n",
      "Epoch [231/5000], Step [36/47], Loss: 0.1466\n",
      "Epoch [231/5000], Step [45/47], Loss: 0.0735\n",
      "Epoch [231/5000], Avg. Train Sample Loss: 0.0832, Avg. Validate Sample Loss: 0.0743,                             L2 Loss: 0.0152\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [232/5000] -------------------- \n",
      "Epoch [232/5000], Step [1/47], Loss: 0.0936\n",
      "Epoch [232/5000], Step [9/47], Loss: 0.0509\n",
      "Epoch [232/5000], Step [18/47], Loss: 0.0991\n",
      "Epoch [232/5000], Step [27/47], Loss: 0.0357\n",
      "Epoch [232/5000], Step [36/47], Loss: 0.0590\n",
      "Epoch [232/5000], Step [45/47], Loss: 0.0274\n",
      "Epoch [232/5000], Avg. Train Sample Loss: 0.0772, Avg. Validate Sample Loss: 0.0729,                             L2 Loss: 0.0140\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [233/5000] -------------------- \n",
      "Epoch [233/5000], Step [1/47], Loss: 0.0339\n",
      "Epoch [233/5000], Step [9/47], Loss: 0.0404\n",
      "Epoch [233/5000], Step [18/47], Loss: 0.0702\n",
      "Epoch [233/5000], Step [27/47], Loss: 0.1327\n",
      "Epoch [233/5000], Step [36/47], Loss: 0.0914\n",
      "Epoch [233/5000], Step [45/47], Loss: 0.0293\n",
      "Epoch [233/5000], Avg. Train Sample Loss: 0.0762, Avg. Validate Sample Loss: 0.0783,                             L2 Loss: 0.0239\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [234/5000] -------------------- \n",
      "Epoch [234/5000], Step [1/47], Loss: 0.1076\n",
      "Epoch [234/5000], Step [9/47], Loss: 0.1184\n",
      "Epoch [234/5000], Step [18/47], Loss: 0.0630\n",
      "Epoch [234/5000], Step [27/47], Loss: 0.0734\n",
      "Epoch [234/5000], Step [36/47], Loss: 0.0279\n",
      "Epoch [234/5000], Step [45/47], Loss: 0.0614\n",
      "Epoch [234/5000], Avg. Train Sample Loss: 0.0731, Avg. Validate Sample Loss: 0.0979,                             L2 Loss: 0.0315\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [235/5000] -------------------- \n",
      "Epoch [235/5000], Step [1/47], Loss: 0.1502\n",
      "Epoch [235/5000], Step [9/47], Loss: 0.1152\n",
      "Epoch [235/5000], Step [18/47], Loss: 0.0861\n",
      "Epoch [235/5000], Step [27/47], Loss: 0.1752\n",
      "Epoch [235/5000], Step [36/47], Loss: 0.1629\n",
      "Epoch [235/5000], Step [45/47], Loss: 0.0903\n",
      "Epoch [235/5000], Avg. Train Sample Loss: 0.0985, Avg. Validate Sample Loss: 0.0646,                             L2 Loss: 0.0171\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [236/5000] -------------------- \n",
      "Epoch [236/5000], Step [1/47], Loss: 0.0400\n",
      "Epoch [236/5000], Step [9/47], Loss: 0.1185\n",
      "Epoch [236/5000], Step [18/47], Loss: 0.1150\n",
      "Epoch [236/5000], Step [27/47], Loss: 0.1314\n",
      "Epoch [236/5000], Step [36/47], Loss: 0.0248\n",
      "Epoch [236/5000], Step [45/47], Loss: 0.0674\n",
      "Epoch [236/5000], Avg. Train Sample Loss: 0.0712, Avg. Validate Sample Loss: 0.0596,                             L2 Loss: 0.0136\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [237/5000] -------------------- \n",
      "Epoch [237/5000], Step [1/47], Loss: 0.0861\n",
      "Epoch [237/5000], Step [9/47], Loss: 0.0396\n",
      "Epoch [237/5000], Step [18/47], Loss: 0.0734\n",
      "Epoch [237/5000], Step [27/47], Loss: 0.0671\n",
      "Epoch [237/5000], Step [36/47], Loss: 0.0506\n",
      "Epoch [237/5000], Step [45/47], Loss: 0.0759\n",
      "Epoch [237/5000], Avg. Train Sample Loss: 0.0708, Avg. Validate Sample Loss: 0.0899,                             L2 Loss: 0.0307\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [238/5000] -------------------- \n",
      "Epoch [238/5000], Step [1/47], Loss: 0.1332\n",
      "Epoch [238/5000], Step [9/47], Loss: 0.0769\n",
      "Epoch [238/5000], Step [18/47], Loss: 0.0927\n",
      "Epoch [238/5000], Step [27/47], Loss: 0.0439\n",
      "Epoch [238/5000], Step [36/47], Loss: 0.0788\n",
      "Epoch [238/5000], Step [45/47], Loss: 0.0487\n",
      "Epoch [238/5000], Avg. Train Sample Loss: 0.0699, Avg. Validate Sample Loss: 0.0744,                             L2 Loss: 0.0191\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [239/5000] -------------------- \n",
      "Epoch [239/5000], Step [1/47], Loss: 0.0402\n",
      "Epoch [239/5000], Step [9/47], Loss: 0.0379\n",
      "Epoch [239/5000], Step [18/47], Loss: 0.0668\n",
      "Epoch [239/5000], Step [27/47], Loss: 0.0472\n",
      "Epoch [239/5000], Step [36/47], Loss: 0.0735\n",
      "Epoch [239/5000], Step [45/47], Loss: 0.0369\n",
      "Epoch [239/5000], Avg. Train Sample Loss: 0.0707, Avg. Validate Sample Loss: 0.0630,                             L2 Loss: 0.0182\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [240/5000] -------------------- \n",
      "Epoch [240/5000], Step [1/47], Loss: 0.0423\n",
      "Epoch [240/5000], Step [9/47], Loss: 0.0855\n",
      "Epoch [240/5000], Step [18/47], Loss: 0.0533\n",
      "Epoch [240/5000], Step [27/47], Loss: 0.0566\n",
      "Epoch [240/5000], Step [36/47], Loss: 0.0330\n",
      "Epoch [240/5000], Step [45/47], Loss: 0.0763\n",
      "Epoch [240/5000], Avg. Train Sample Loss: 0.0611, Avg. Validate Sample Loss: 0.0636,                             L2 Loss: 0.0124\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [241/5000] -------------------- \n",
      "Epoch [241/5000], Step [1/47], Loss: 0.0520\n",
      "Epoch [241/5000], Step [9/47], Loss: 0.0670\n",
      "Epoch [241/5000], Step [18/47], Loss: 0.0583\n",
      "Epoch [241/5000], Step [27/47], Loss: 0.0775\n",
      "Epoch [241/5000], Step [36/47], Loss: 0.0594\n",
      "Epoch [241/5000], Step [45/47], Loss: 0.0881\n",
      "Epoch [241/5000], Avg. Train Sample Loss: 0.0609, Avg. Validate Sample Loss: 0.0799,                             L2 Loss: 0.0201\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [242/5000] -------------------- \n",
      "Epoch [242/5000], Step [1/47], Loss: 0.0441\n",
      "Epoch [242/5000], Step [9/47], Loss: 0.1142\n",
      "Epoch [242/5000], Step [18/47], Loss: 0.0885\n",
      "Epoch [242/5000], Step [27/47], Loss: 0.0619\n",
      "Epoch [242/5000], Step [36/47], Loss: 0.0422\n",
      "Epoch [242/5000], Step [45/47], Loss: 0.0690\n",
      "Epoch [242/5000], Avg. Train Sample Loss: 0.0845, Avg. Validate Sample Loss: 0.1080,                             L2 Loss: 0.0396\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [243/5000] -------------------- \n",
      "Epoch [243/5000], Step [1/47], Loss: 0.0995\n",
      "Epoch [243/5000], Step [9/47], Loss: 0.0650\n",
      "Epoch [243/5000], Step [18/47], Loss: 0.0844\n",
      "Epoch [243/5000], Step [27/47], Loss: 0.0614\n",
      "Epoch [243/5000], Step [36/47], Loss: 0.0553\n",
      "Epoch [243/5000], Step [45/47], Loss: 0.1239\n",
      "Epoch [243/5000], Avg. Train Sample Loss: 0.0768, Avg. Validate Sample Loss: 0.0490,                             L2 Loss: 0.0196\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [244/5000] -------------------- \n",
      "Epoch [244/5000], Step [1/47], Loss: 0.0266\n",
      "Epoch [244/5000], Step [9/47], Loss: 0.0408\n",
      "Epoch [244/5000], Step [18/47], Loss: 0.0484\n",
      "Epoch [244/5000], Step [27/47], Loss: 0.0739\n",
      "Epoch [244/5000], Step [36/47], Loss: 0.0946\n",
      "Epoch [244/5000], Step [45/47], Loss: 0.0225\n",
      "Epoch [244/5000], Avg. Train Sample Loss: 0.0577, Avg. Validate Sample Loss: 0.0458,                             L2 Loss: 0.0158\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [245/5000] -------------------- \n",
      "Epoch [245/5000], Step [1/47], Loss: 0.0844\n",
      "Epoch [245/5000], Step [9/47], Loss: 0.0392\n",
      "Epoch [245/5000], Step [18/47], Loss: 0.0579\n",
      "Epoch [245/5000], Step [27/47], Loss: 0.0713\n",
      "Epoch [245/5000], Step [36/47], Loss: 0.0251\n",
      "Epoch [245/5000], Step [45/47], Loss: 0.0603\n",
      "Epoch [245/5000], Avg. Train Sample Loss: 0.0583, Avg. Validate Sample Loss: 0.0515,                             L2 Loss: 0.0188\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [246/5000] -------------------- \n",
      "Epoch [246/5000], Step [1/47], Loss: 0.0501\n",
      "Epoch [246/5000], Step [9/47], Loss: 0.0698\n",
      "Epoch [246/5000], Step [18/47], Loss: 0.0471\n",
      "Epoch [246/5000], Step [27/47], Loss: 0.1342\n",
      "Epoch [246/5000], Step [36/47], Loss: 0.0487\n",
      "Epoch [246/5000], Step [45/47], Loss: 0.0228\n",
      "Epoch [246/5000], Avg. Train Sample Loss: 0.0691, Avg. Validate Sample Loss: 0.0545,                             L2 Loss: 0.0223\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [247/5000] -------------------- \n",
      "Epoch [247/5000], Step [1/47], Loss: 0.1369\n",
      "Epoch [247/5000], Step [9/47], Loss: 0.0700\n",
      "Epoch [247/5000], Step [18/47], Loss: 0.0469\n",
      "Epoch [247/5000], Step [27/47], Loss: 0.1223\n",
      "Epoch [247/5000], Step [36/47], Loss: 0.0274\n",
      "Epoch [247/5000], Step [45/47], Loss: 0.0505\n",
      "Epoch [247/5000], Avg. Train Sample Loss: 0.0559, Avg. Validate Sample Loss: 0.0639,                             L2 Loss: 0.0185\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [248/5000] -------------------- \n",
      "Epoch [248/5000], Step [1/47], Loss: 0.0780\n",
      "Epoch [248/5000], Step [9/47], Loss: 0.0376\n",
      "Epoch [248/5000], Step [18/47], Loss: 0.0560\n",
      "Epoch [248/5000], Step [27/47], Loss: 0.0445\n",
      "Epoch [248/5000], Step [36/47], Loss: 0.0692\n",
      "Epoch [248/5000], Step [45/47], Loss: 0.0408\n",
      "Epoch [248/5000], Avg. Train Sample Loss: 0.0550, Avg. Validate Sample Loss: 0.0435,                             L2 Loss: 0.0147\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [249/5000] -------------------- \n",
      "Epoch [249/5000], Step [1/47], Loss: 0.0580\n",
      "Epoch [249/5000], Step [9/47], Loss: 0.0633\n",
      "Epoch [249/5000], Step [18/47], Loss: 0.0273\n",
      "Epoch [249/5000], Step [27/47], Loss: 0.0559\n",
      "Epoch [249/5000], Step [36/47], Loss: 0.0890\n",
      "Epoch [249/5000], Step [45/47], Loss: 0.0447\n",
      "Epoch [249/5000], Avg. Train Sample Loss: 0.0562, Avg. Validate Sample Loss: 0.0534,                             L2 Loss: 0.0139\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [250/5000] -------------------- \n",
      "Epoch [250/5000], Step [1/47], Loss: 0.0326\n",
      "Epoch [250/5000], Step [9/47], Loss: 0.0709\n",
      "Epoch [250/5000], Step [18/47], Loss: 0.0694\n",
      "Epoch [250/5000], Step [27/47], Loss: 0.0432\n",
      "Epoch [250/5000], Step [36/47], Loss: 0.0358\n",
      "Epoch [250/5000], Step [45/47], Loss: 0.0711\n",
      "Epoch [250/5000], Avg. Train Sample Loss: 0.0504, Avg. Validate Sample Loss: 0.0538,                             L2 Loss: 0.0127\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [251/5000] -------------------- \n",
      "Epoch [251/5000], Step [1/47], Loss: 0.0634\n",
      "Epoch [251/5000], Step [9/47], Loss: 0.0890\n",
      "Epoch [251/5000], Step [18/47], Loss: 0.0519\n",
      "Epoch [251/5000], Step [27/47], Loss: 0.0564\n",
      "Epoch [251/5000], Step [36/47], Loss: 0.0759\n",
      "Epoch [251/5000], Step [45/47], Loss: 0.0434\n",
      "Epoch [251/5000], Avg. Train Sample Loss: 0.0476, Avg. Validate Sample Loss: 0.0407,                             L2 Loss: 0.0151\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [252/5000] -------------------- \n",
      "Epoch [252/5000], Step [1/47], Loss: 0.0481\n",
      "Epoch [252/5000], Step [9/47], Loss: 0.0569\n",
      "Epoch [252/5000], Step [18/47], Loss: 0.0493\n",
      "Epoch [252/5000], Step [27/47], Loss: 0.0764\n",
      "Epoch [252/5000], Step [36/47], Loss: 0.0312\n",
      "Epoch [252/5000], Step [45/47], Loss: 0.0603\n",
      "Epoch [252/5000], Avg. Train Sample Loss: 0.0519, Avg. Validate Sample Loss: 0.0691,                             L2 Loss: 0.0262\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [253/5000] -------------------- \n",
      "Epoch [253/5000], Step [1/47], Loss: 0.0977\n",
      "Epoch [253/5000], Step [9/47], Loss: 0.0777\n",
      "Epoch [253/5000], Step [18/47], Loss: 0.0745\n",
      "Epoch [253/5000], Step [27/47], Loss: 0.0244\n",
      "Epoch [253/5000], Step [36/47], Loss: 0.0374\n",
      "Epoch [253/5000], Step [45/47], Loss: 0.0276\n",
      "Epoch [253/5000], Avg. Train Sample Loss: 0.0504, Avg. Validate Sample Loss: 0.0471,                             L2 Loss: 0.0183\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [254/5000] -------------------- \n",
      "Epoch [254/5000], Step [1/47], Loss: 0.0489\n",
      "Epoch [254/5000], Step [9/47], Loss: 0.0730\n",
      "Epoch [254/5000], Step [18/47], Loss: 0.0718\n",
      "Epoch [254/5000], Step [27/47], Loss: 0.0325\n",
      "Epoch [254/5000], Step [36/47], Loss: 0.0878\n",
      "Epoch [254/5000], Step [45/47], Loss: 0.0430\n",
      "Epoch [254/5000], Avg. Train Sample Loss: 0.0538, Avg. Validate Sample Loss: 0.0493,                             L2 Loss: 0.0158\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [255/5000] -------------------- \n",
      "Epoch [255/5000], Step [1/47], Loss: 0.0631\n",
      "Epoch [255/5000], Step [9/47], Loss: 0.0630\n",
      "Epoch [255/5000], Step [18/47], Loss: 0.0900\n",
      "Epoch [255/5000], Step [27/47], Loss: 0.0746\n",
      "Epoch [255/5000], Step [36/47], Loss: 0.0267\n",
      "Epoch [255/5000], Step [45/47], Loss: 0.0683\n",
      "Epoch [255/5000], Avg. Train Sample Loss: 0.0563, Avg. Validate Sample Loss: 0.0968,                             L2 Loss: 0.0385\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [256/5000] -------------------- \n",
      "Epoch [256/5000], Step [1/47], Loss: 0.0881\n",
      "Epoch [256/5000], Step [9/47], Loss: 0.0873\n",
      "Epoch [256/5000], Step [18/47], Loss: 0.0308\n",
      "Epoch [256/5000], Step [27/47], Loss: 0.0846\n",
      "Epoch [256/5000], Step [36/47], Loss: 0.0433\n",
      "Epoch [256/5000], Step [45/47], Loss: 0.0889\n",
      "Epoch [256/5000], Avg. Train Sample Loss: 0.0529, Avg. Validate Sample Loss: 0.0477,                             L2 Loss: 0.0206\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [257/5000] -------------------- \n",
      "Epoch [257/5000], Step [1/47], Loss: 0.0546\n",
      "Epoch [257/5000], Step [9/47], Loss: 0.0390\n",
      "Epoch [257/5000], Step [18/47], Loss: 0.0710\n",
      "Epoch [257/5000], Step [27/47], Loss: 0.0373\n",
      "Epoch [257/5000], Step [36/47], Loss: 0.0727\n",
      "Epoch [257/5000], Step [45/47], Loss: 0.0495\n",
      "Epoch [257/5000], Avg. Train Sample Loss: 0.0469, Avg. Validate Sample Loss: 0.0659,                             L2 Loss: 0.0243\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [258/5000] -------------------- \n",
      "Epoch [258/5000], Step [1/47], Loss: 0.0524\n",
      "Epoch [258/5000], Step [9/47], Loss: 0.0424\n",
      "Epoch [258/5000], Step [18/47], Loss: 0.0506\n",
      "Epoch [258/5000], Step [27/47], Loss: 0.0533\n",
      "Epoch [258/5000], Step [36/47], Loss: 0.0427\n",
      "Epoch [258/5000], Step [45/47], Loss: 0.0320\n",
      "Epoch [258/5000], Avg. Train Sample Loss: 0.0453, Avg. Validate Sample Loss: 0.0397,                             L2 Loss: 0.0145\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [259/5000] -------------------- \n",
      "Epoch [259/5000], Step [1/47], Loss: 0.0171\n",
      "Epoch [259/5000], Step [9/47], Loss: 0.0335\n",
      "Epoch [259/5000], Step [18/47], Loss: 0.0237\n",
      "Epoch [259/5000], Step [27/47], Loss: 0.0726\n",
      "Epoch [259/5000], Step [36/47], Loss: 0.0610\n",
      "Epoch [259/5000], Step [45/47], Loss: 0.0649\n",
      "Epoch [259/5000], Avg. Train Sample Loss: 0.0413, Avg. Validate Sample Loss: 0.0415,                             L2 Loss: 0.0130\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [260/5000] -------------------- \n",
      "Epoch [260/5000], Step [1/47], Loss: 0.0436\n",
      "Epoch [260/5000], Step [9/47], Loss: 0.0337\n",
      "Epoch [260/5000], Step [18/47], Loss: 0.0226\n",
      "Epoch [260/5000], Step [27/47], Loss: 0.0459\n",
      "Epoch [260/5000], Step [36/47], Loss: 0.0396\n",
      "Epoch [260/5000], Step [45/47], Loss: 0.0757\n",
      "Epoch [260/5000], Avg. Train Sample Loss: 0.0429, Avg. Validate Sample Loss: 0.0390,                             L2 Loss: 0.0129\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [261/5000] -------------------- \n",
      "Epoch [261/5000], Step [1/47], Loss: 0.0264\n",
      "Epoch [261/5000], Step [9/47], Loss: 0.0733\n",
      "Epoch [261/5000], Step [18/47], Loss: 0.0266\n",
      "Epoch [261/5000], Step [27/47], Loss: 0.0410\n",
      "Epoch [261/5000], Step [36/47], Loss: 0.0257\n",
      "Epoch [261/5000], Step [45/47], Loss: 0.0309\n",
      "Epoch [261/5000], Avg. Train Sample Loss: 0.0423, Avg. Validate Sample Loss: 0.0349,                             L2 Loss: 0.0135\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [262/5000] -------------------- \n",
      "Epoch [262/5000], Step [1/47], Loss: 0.0152\n",
      "Epoch [262/5000], Step [9/47], Loss: 0.0366\n",
      "Epoch [262/5000], Step [18/47], Loss: 0.0296\n",
      "Epoch [262/5000], Step [27/47], Loss: 0.0474\n",
      "Epoch [262/5000], Step [36/47], Loss: 0.0277\n",
      "Epoch [262/5000], Step [45/47], Loss: 0.0833\n",
      "Epoch [262/5000], Avg. Train Sample Loss: 0.0494, Avg. Validate Sample Loss: 0.0636,                             L2 Loss: 0.0284\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [263/5000] -------------------- \n",
      "Epoch [263/5000], Step [1/47], Loss: 0.0663\n",
      "Epoch [263/5000], Step [9/47], Loss: 0.0417\n",
      "Epoch [263/5000], Step [18/47], Loss: 0.0397\n",
      "Epoch [263/5000], Step [27/47], Loss: 0.0559\n",
      "Epoch [263/5000], Step [36/47], Loss: 0.0340\n",
      "Epoch [263/5000], Step [45/47], Loss: 0.0318\n",
      "Epoch [263/5000], Avg. Train Sample Loss: 0.0419, Avg. Validate Sample Loss: 0.0398,                             L2 Loss: 0.0194\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [264/5000] -------------------- \n",
      "Epoch [264/5000], Step [1/47], Loss: 0.0392\n",
      "Epoch [264/5000], Step [9/47], Loss: 0.0629\n",
      "Epoch [264/5000], Step [18/47], Loss: 0.0415\n",
      "Epoch [264/5000], Step [27/47], Loss: 0.0335\n",
      "Epoch [264/5000], Step [36/47], Loss: 0.0247\n",
      "Epoch [264/5000], Step [45/47], Loss: 0.0230\n",
      "Epoch [264/5000], Avg. Train Sample Loss: 0.0394, Avg. Validate Sample Loss: 0.0346,                             L2 Loss: 0.0121\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [265/5000] -------------------- \n",
      "Epoch [265/5000], Step [1/47], Loss: 0.0454\n",
      "Epoch [265/5000], Step [9/47], Loss: 0.0539\n",
      "Epoch [265/5000], Step [18/47], Loss: 0.0242\n",
      "Epoch [265/5000], Step [27/47], Loss: 0.0325\n",
      "Epoch [265/5000], Step [36/47], Loss: 0.0661\n",
      "Epoch [265/5000], Step [45/47], Loss: 0.0180\n",
      "Epoch [265/5000], Avg. Train Sample Loss: 0.0425, Avg. Validate Sample Loss: 0.0458,                             L2 Loss: 0.0195\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [266/5000] -------------------- \n",
      "Epoch [266/5000], Step [1/47], Loss: 0.0369\n",
      "Epoch [266/5000], Step [9/47], Loss: 0.0288\n",
      "Epoch [266/5000], Step [18/47], Loss: 0.0185\n",
      "Epoch [266/5000], Step [27/47], Loss: 0.0235\n",
      "Epoch [266/5000], Step [36/47], Loss: 0.0462\n",
      "Epoch [266/5000], Step [45/47], Loss: 0.0559\n",
      "Epoch [266/5000], Avg. Train Sample Loss: 0.0355, Avg. Validate Sample Loss: 0.0357,                             L2 Loss: 0.0145\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [267/5000] -------------------- \n",
      "Epoch [267/5000], Step [1/47], Loss: 0.0326\n",
      "Epoch [267/5000], Step [9/47], Loss: 0.0189\n",
      "Epoch [267/5000], Step [18/47], Loss: 0.0576\n",
      "Epoch [267/5000], Step [27/47], Loss: 0.0327\n",
      "Epoch [267/5000], Step [36/47], Loss: 0.0605\n",
      "Epoch [267/5000], Step [45/47], Loss: 0.0325\n",
      "Epoch [267/5000], Avg. Train Sample Loss: 0.0381, Avg. Validate Sample Loss: 0.0363,                             L2 Loss: 0.0148\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [268/5000] -------------------- \n",
      "Epoch [268/5000], Step [1/47], Loss: 0.0213\n",
      "Epoch [268/5000], Step [9/47], Loss: 0.0520\n",
      "Epoch [268/5000], Step [18/47], Loss: 0.0228\n",
      "Epoch [268/5000], Step [27/47], Loss: 0.0156\n",
      "Epoch [268/5000], Step [36/47], Loss: 0.0387\n",
      "Epoch [268/5000], Step [45/47], Loss: 0.0344\n",
      "Epoch [268/5000], Avg. Train Sample Loss: 0.0345, Avg. Validate Sample Loss: 0.0473,                             L2 Loss: 0.0244\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [269/5000] -------------------- \n",
      "Epoch [269/5000], Step [1/47], Loss: 0.0514\n",
      "Epoch [269/5000], Step [9/47], Loss: 0.0359\n",
      "Epoch [269/5000], Step [18/47], Loss: 0.0517\n",
      "Epoch [269/5000], Step [27/47], Loss: 0.0210\n",
      "Epoch [269/5000], Step [36/47], Loss: 0.0390\n",
      "Epoch [269/5000], Step [45/47], Loss: 0.0451\n",
      "Epoch [269/5000], Avg. Train Sample Loss: 0.0378, Avg. Validate Sample Loss: 0.0301,                             L2 Loss: 0.0176\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [270/5000] -------------------- \n",
      "Epoch [270/5000], Step [1/47], Loss: 0.0271\n",
      "Epoch [270/5000], Step [9/47], Loss: 0.0201\n",
      "Epoch [270/5000], Step [18/47], Loss: 0.0372\n",
      "Epoch [270/5000], Step [27/47], Loss: 0.0296\n",
      "Epoch [270/5000], Step [36/47], Loss: 0.0925\n",
      "Epoch [270/5000], Step [45/47], Loss: 0.0150\n",
      "Epoch [270/5000], Avg. Train Sample Loss: 0.0411, Avg. Validate Sample Loss: 0.0460,                             L2 Loss: 0.0221\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [271/5000] -------------------- \n",
      "Epoch [271/5000], Step [1/47], Loss: 0.0306\n",
      "Epoch [271/5000], Step [9/47], Loss: 0.0282\n",
      "Epoch [271/5000], Step [18/47], Loss: 0.0191\n",
      "Epoch [271/5000], Step [27/47], Loss: 0.0297\n",
      "Epoch [271/5000], Step [36/47], Loss: 0.0212\n",
      "Epoch [271/5000], Step [45/47], Loss: 0.0505\n",
      "Epoch [271/5000], Avg. Train Sample Loss: 0.0329, Avg. Validate Sample Loss: 0.0332,                             L2 Loss: 0.0139\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [272/5000] -------------------- \n",
      "Epoch [272/5000], Step [1/47], Loss: 0.0331\n",
      "Epoch [272/5000], Step [9/47], Loss: 0.0203\n",
      "Epoch [272/5000], Step [18/47], Loss: 0.0153\n",
      "Epoch [272/5000], Step [27/47], Loss: 0.0547\n",
      "Epoch [272/5000], Step [36/47], Loss: 0.0505\n",
      "Epoch [272/5000], Step [45/47], Loss: 0.0231\n",
      "Epoch [272/5000], Avg. Train Sample Loss: 0.0345, Avg. Validate Sample Loss: 0.0296,                             L2 Loss: 0.0122\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [273/5000] -------------------- \n",
      "Epoch [273/5000], Step [1/47], Loss: 0.0136\n",
      "Epoch [273/5000], Step [9/47], Loss: 0.0398\n",
      "Epoch [273/5000], Step [18/47], Loss: 0.0166\n",
      "Epoch [273/5000], Step [27/47], Loss: 0.0131\n",
      "Epoch [273/5000], Step [36/47], Loss: 0.0143\n",
      "Epoch [273/5000], Step [45/47], Loss: 0.0446\n",
      "Epoch [273/5000], Avg. Train Sample Loss: 0.0318, Avg. Validate Sample Loss: 0.0279,                             L2 Loss: 0.0139\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [274/5000] -------------------- \n",
      "Epoch [274/5000], Step [1/47], Loss: 0.0217\n",
      "Epoch [274/5000], Step [9/47], Loss: 0.0189\n",
      "Epoch [274/5000], Step [18/47], Loss: 0.0417\n",
      "Epoch [274/5000], Step [27/47], Loss: 0.0164\n",
      "Epoch [274/5000], Step [36/47], Loss: 0.0326\n",
      "Epoch [274/5000], Step [45/47], Loss: 0.0135\n",
      "Epoch [274/5000], Avg. Train Sample Loss: 0.0306, Avg. Validate Sample Loss: 0.0341,                             L2 Loss: 0.0150\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [275/5000] -------------------- \n",
      "Epoch [275/5000], Step [1/47], Loss: 0.0440\n",
      "Epoch [275/5000], Step [9/47], Loss: 0.0162\n",
      "Epoch [275/5000], Step [18/47], Loss: 0.0241\n",
      "Epoch [275/5000], Step [27/47], Loss: 0.0272\n",
      "Epoch [275/5000], Step [36/47], Loss: 0.0286\n",
      "Epoch [275/5000], Step [45/47], Loss: 0.0405\n",
      "Epoch [275/5000], Avg. Train Sample Loss: 0.0303, Avg. Validate Sample Loss: 0.0301,                             L2 Loss: 0.0129\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [276/5000] -------------------- \n",
      "Epoch [276/5000], Step [1/47], Loss: 0.0500\n",
      "Epoch [276/5000], Step [9/47], Loss: 0.0253\n",
      "Epoch [276/5000], Step [18/47], Loss: 0.0364\n",
      "Epoch [276/5000], Step [27/47], Loss: 0.0296\n",
      "Epoch [276/5000], Step [36/47], Loss: 0.0398\n",
      "Epoch [276/5000], Step [45/47], Loss: 0.0505\n",
      "Epoch [276/5000], Avg. Train Sample Loss: 0.0285, Avg. Validate Sample Loss: 0.0391,                             L2 Loss: 0.0218\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [277/5000] -------------------- \n",
      "Epoch [277/5000], Step [1/47], Loss: 0.0389\n",
      "Epoch [277/5000], Step [9/47], Loss: 0.0314\n",
      "Epoch [277/5000], Step [18/47], Loss: 0.0344\n",
      "Epoch [277/5000], Step [27/47], Loss: 0.0394\n",
      "Epoch [277/5000], Step [36/47], Loss: 0.0185\n",
      "Epoch [277/5000], Step [45/47], Loss: 0.0873\n",
      "Epoch [277/5000], Avg. Train Sample Loss: 0.0343, Avg. Validate Sample Loss: 0.0297,                             L2 Loss: 0.0156\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [278/5000] -------------------- \n",
      "Epoch [278/5000], Step [1/47], Loss: 0.0398\n",
      "Epoch [278/5000], Step [9/47], Loss: 0.0868\n",
      "Epoch [278/5000], Step [18/47], Loss: 0.0265\n",
      "Epoch [278/5000], Step [27/47], Loss: 0.0324\n",
      "Epoch [278/5000], Step [36/47], Loss: 0.0468\n",
      "Epoch [278/5000], Step [45/47], Loss: 0.0275\n",
      "Epoch [278/5000], Avg. Train Sample Loss: 0.0362, Avg. Validate Sample Loss: 0.0247,                             L2 Loss: 0.0102\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [279/5000] -------------------- \n",
      "Epoch [279/5000], Step [1/47], Loss: 0.0262\n",
      "Epoch [279/5000], Step [9/47], Loss: 0.0619\n",
      "Epoch [279/5000], Step [18/47], Loss: 0.0371\n",
      "Epoch [279/5000], Step [27/47], Loss: 0.0452\n",
      "Epoch [279/5000], Step [36/47], Loss: 0.0812\n",
      "Epoch [279/5000], Step [45/47], Loss: 0.0617\n",
      "Epoch [279/5000], Avg. Train Sample Loss: 0.0517, Avg. Validate Sample Loss: 0.0298,                             L2 Loss: 0.0122\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [280/5000] -------------------- \n",
      "Epoch [280/5000], Step [1/47], Loss: 0.0202\n",
      "Epoch [280/5000], Step [9/47], Loss: 0.0173\n",
      "Epoch [280/5000], Step [18/47], Loss: 0.0327\n",
      "Epoch [280/5000], Step [27/47], Loss: 0.0193\n",
      "Epoch [280/5000], Step [36/47], Loss: 0.0257\n",
      "Epoch [280/5000], Step [45/47], Loss: 0.0240\n",
      "Epoch [280/5000], Avg. Train Sample Loss: 0.0262, Avg. Validate Sample Loss: 0.0279,                             L2 Loss: 0.0131\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [281/5000] -------------------- \n",
      "Epoch [281/5000], Step [1/47], Loss: 0.0271\n",
      "Epoch [281/5000], Step [9/47], Loss: 0.0305\n",
      "Epoch [281/5000], Step [18/47], Loss: 0.0219\n",
      "Epoch [281/5000], Step [27/47], Loss: 0.0166\n",
      "Epoch [281/5000], Step [36/47], Loss: 0.0257\n",
      "Epoch [281/5000], Step [45/47], Loss: 0.0230\n",
      "Epoch [281/5000], Avg. Train Sample Loss: 0.0253, Avg. Validate Sample Loss: 0.0253,                             L2 Loss: 0.0140\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [282/5000] -------------------- \n",
      "Epoch [282/5000], Step [1/47], Loss: 0.0214\n",
      "Epoch [282/5000], Step [9/47], Loss: 0.0307\n",
      "Epoch [282/5000], Step [18/47], Loss: 0.0672\n",
      "Epoch [282/5000], Step [27/47], Loss: 0.0287\n",
      "Epoch [282/5000], Step [36/47], Loss: 0.0106\n",
      "Epoch [282/5000], Step [45/47], Loss: 0.0130\n",
      "Epoch [282/5000], Avg. Train Sample Loss: 0.0289, Avg. Validate Sample Loss: 0.0289,                             L2 Loss: 0.0140\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [283/5000] -------------------- \n",
      "Epoch [283/5000], Step [1/47], Loss: 0.0371\n",
      "Epoch [283/5000], Step [9/47], Loss: 0.0473\n",
      "Epoch [283/5000], Step [18/47], Loss: 0.0339\n",
      "Epoch [283/5000], Step [27/47], Loss: 0.0415\n",
      "Epoch [283/5000], Step [36/47], Loss: 0.0190\n",
      "Epoch [283/5000], Step [45/47], Loss: 0.0296\n",
      "Epoch [283/5000], Avg. Train Sample Loss: 0.0278, Avg. Validate Sample Loss: 0.0221,                             L2 Loss: 0.0120\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [284/5000] -------------------- \n",
      "Epoch [284/5000], Step [1/47], Loss: 0.0151\n",
      "Epoch [284/5000], Step [9/47], Loss: 0.0187\n",
      "Epoch [284/5000], Step [18/47], Loss: 0.0134\n",
      "Epoch [284/5000], Step [27/47], Loss: 0.0214\n",
      "Epoch [284/5000], Step [36/47], Loss: 0.0246\n",
      "Epoch [284/5000], Step [45/47], Loss: 0.0222\n",
      "Epoch [284/5000], Avg. Train Sample Loss: 0.0233, Avg. Validate Sample Loss: 0.0208,                             L2 Loss: 0.0084\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [285/5000] -------------------- \n",
      "Epoch [285/5000], Step [1/47], Loss: 0.0107\n",
      "Epoch [285/5000], Step [9/47], Loss: 0.0136\n",
      "Epoch [285/5000], Step [18/47], Loss: 0.0190\n",
      "Epoch [285/5000], Step [27/47], Loss: 0.0325\n",
      "Epoch [285/5000], Step [36/47], Loss: 0.0289\n",
      "Epoch [285/5000], Step [45/47], Loss: 0.0279\n",
      "Epoch [285/5000], Avg. Train Sample Loss: 0.0230, Avg. Validate Sample Loss: 0.0230,                             L2 Loss: 0.0112\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [286/5000] -------------------- \n",
      "Epoch [286/5000], Step [1/47], Loss: 0.0152\n",
      "Epoch [286/5000], Step [9/47], Loss: 0.0113\n",
      "Epoch [286/5000], Step [18/47], Loss: 0.0121\n",
      "Epoch [286/5000], Step [27/47], Loss: 0.0458\n",
      "Epoch [286/5000], Step [36/47], Loss: 0.0425\n",
      "Epoch [286/5000], Step [45/47], Loss: 0.0369\n",
      "Epoch [286/5000], Avg. Train Sample Loss: 0.0335, Avg. Validate Sample Loss: 0.0371,                             L2 Loss: 0.0210\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [287/5000] -------------------- \n",
      "Epoch [287/5000], Step [1/47], Loss: 0.0398\n",
      "Epoch [287/5000], Step [9/47], Loss: 0.0167\n",
      "Epoch [287/5000], Step [18/47], Loss: 0.0179\n",
      "Epoch [287/5000], Step [27/47], Loss: 0.0196\n",
      "Epoch [287/5000], Step [36/47], Loss: 0.0121\n",
      "Epoch [287/5000], Step [45/47], Loss: 0.0395\n",
      "Epoch [287/5000], Avg. Train Sample Loss: 0.0245, Avg. Validate Sample Loss: 0.0234,                             L2 Loss: 0.0138\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [288/5000] -------------------- \n",
      "Epoch [288/5000], Step [1/47], Loss: 0.0239\n",
      "Epoch [288/5000], Step [9/47], Loss: 0.0090\n",
      "Epoch [288/5000], Step [18/47], Loss: 0.0102\n",
      "Epoch [288/5000], Step [27/47], Loss: 0.0103\n",
      "Epoch [288/5000], Step [36/47], Loss: 0.0207\n",
      "Epoch [288/5000], Step [45/47], Loss: 0.0249\n",
      "Epoch [288/5000], Avg. Train Sample Loss: 0.0221, Avg. Validate Sample Loss: 0.0280,                             L2 Loss: 0.0190\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [289/5000] -------------------- \n",
      "Epoch [289/5000], Step [1/47], Loss: 0.0336\n",
      "Epoch [289/5000], Step [9/47], Loss: 0.0474\n",
      "Epoch [289/5000], Step [18/47], Loss: 0.0238\n",
      "Epoch [289/5000], Step [27/47], Loss: 0.0274\n",
      "Epoch [289/5000], Step [36/47], Loss: 0.0232\n",
      "Epoch [289/5000], Step [45/47], Loss: 0.0365\n",
      "Epoch [289/5000], Avg. Train Sample Loss: 0.0247, Avg. Validate Sample Loss: 0.0199,                             L2 Loss: 0.0087\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [290/5000] -------------------- \n",
      "Epoch [290/5000], Step [1/47], Loss: 0.0247\n",
      "Epoch [290/5000], Step [9/47], Loss: 0.0258\n",
      "Epoch [290/5000], Step [18/47], Loss: 0.0262\n",
      "Epoch [290/5000], Step [27/47], Loss: 0.0212\n",
      "Epoch [290/5000], Step [36/47], Loss: 0.0090\n",
      "Epoch [290/5000], Step [45/47], Loss: 0.0276\n",
      "Epoch [290/5000], Avg. Train Sample Loss: 0.0222, Avg. Validate Sample Loss: 0.0212,                             L2 Loss: 0.0104\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [291/5000] -------------------- \n",
      "Epoch [291/5000], Step [1/47], Loss: 0.0110\n",
      "Epoch [291/5000], Step [9/47], Loss: 0.0269\n",
      "Epoch [291/5000], Step [18/47], Loss: 0.0204\n",
      "Epoch [291/5000], Step [27/47], Loss: 0.0141\n",
      "Epoch [291/5000], Step [36/47], Loss: 0.0419\n",
      "Epoch [291/5000], Step [45/47], Loss: 0.0326\n",
      "Epoch [291/5000], Avg. Train Sample Loss: 0.0239, Avg. Validate Sample Loss: 0.0196,                             L2 Loss: 0.0104\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [292/5000] -------------------- \n",
      "Epoch [292/5000], Step [1/47], Loss: 0.0249\n",
      "Epoch [292/5000], Step [9/47], Loss: 0.0241\n",
      "Epoch [292/5000], Step [18/47], Loss: 0.0158\n",
      "Epoch [292/5000], Step [27/47], Loss: 0.0201\n",
      "Epoch [292/5000], Step [36/47], Loss: 0.0434\n",
      "Epoch [292/5000], Step [45/47], Loss: 0.0417\n",
      "Epoch [292/5000], Avg. Train Sample Loss: 0.0264, Avg. Validate Sample Loss: 0.0487,                             L2 Loss: 0.0294\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [293/5000] -------------------- \n",
      "Epoch [293/5000], Step [1/47], Loss: 0.0571\n",
      "Epoch [293/5000], Step [9/47], Loss: 0.0293\n",
      "Epoch [293/5000], Step [18/47], Loss: 0.0191\n",
      "Epoch [293/5000], Step [27/47], Loss: 0.0310\n",
      "Epoch [293/5000], Step [36/47], Loss: 0.0163\n",
      "Epoch [293/5000], Step [45/47], Loss: 0.0225\n",
      "Epoch [293/5000], Avg. Train Sample Loss: 0.0258, Avg. Validate Sample Loss: 0.0134,                             L2 Loss: 0.0081\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [294/5000] -------------------- \n",
      "Epoch [294/5000], Step [1/47], Loss: 0.0252\n",
      "Epoch [294/5000], Step [9/47], Loss: 0.0381\n",
      "Epoch [294/5000], Step [18/47], Loss: 0.0238\n",
      "Epoch [294/5000], Step [27/47], Loss: 0.0098\n",
      "Epoch [294/5000], Step [36/47], Loss: 0.0139\n",
      "Epoch [294/5000], Step [45/47], Loss: 0.0463\n",
      "Epoch [294/5000], Avg. Train Sample Loss: 0.0244, Avg. Validate Sample Loss: 0.0487,                             L2 Loss: 0.0297\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [295/5000] -------------------- \n",
      "Epoch [295/5000], Step [1/47], Loss: 0.0407\n",
      "Epoch [295/5000], Step [9/47], Loss: 0.0242\n",
      "Epoch [295/5000], Step [18/47], Loss: 0.0147\n",
      "Epoch [295/5000], Step [27/47], Loss: 0.0458\n",
      "Epoch [295/5000], Step [36/47], Loss: 0.0286\n",
      "Epoch [295/5000], Step [45/47], Loss: 0.0107\n",
      "Epoch [295/5000], Avg. Train Sample Loss: 0.0305, Avg. Validate Sample Loss: 0.0312,                             L2 Loss: 0.0217\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [296/5000] -------------------- \n",
      "Epoch [296/5000], Step [1/47], Loss: 0.0370\n",
      "Epoch [296/5000], Step [9/47], Loss: 0.0157\n",
      "Epoch [296/5000], Step [18/47], Loss: 0.0288\n",
      "Epoch [296/5000], Step [27/47], Loss: 0.0348\n",
      "Epoch [296/5000], Step [36/47], Loss: 0.0287\n",
      "Epoch [296/5000], Step [45/47], Loss: 0.0073\n",
      "Epoch [296/5000], Avg. Train Sample Loss: 0.0197, Avg. Validate Sample Loss: 0.0226,                             L2 Loss: 0.0179\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [297/5000] -------------------- \n",
      "Epoch [297/5000], Step [1/47], Loss: 0.0203\n",
      "Epoch [297/5000], Step [9/47], Loss: 0.0153\n",
      "Epoch [297/5000], Step [18/47], Loss: 0.0211\n",
      "Epoch [297/5000], Step [27/47], Loss: 0.0158\n",
      "Epoch [297/5000], Step [36/47], Loss: 0.0136\n",
      "Epoch [297/5000], Step [45/47], Loss: 0.0119\n",
      "Epoch [297/5000], Avg. Train Sample Loss: 0.0230, Avg. Validate Sample Loss: 0.0206,                             L2 Loss: 0.0126\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [298/5000] -------------------- \n",
      "Epoch [298/5000], Step [1/47], Loss: 0.0139\n",
      "Epoch [298/5000], Step [9/47], Loss: 0.0148\n",
      "Epoch [298/5000], Step [18/47], Loss: 0.0268\n",
      "Epoch [298/5000], Step [27/47], Loss: 0.0163\n",
      "Epoch [298/5000], Step [36/47], Loss: 0.0198\n",
      "Epoch [298/5000], Step [45/47], Loss: 0.0131\n",
      "Epoch [298/5000], Avg. Train Sample Loss: 0.0195, Avg. Validate Sample Loss: 0.0145,                             L2 Loss: 0.0047\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [299/5000] -------------------- \n",
      "Epoch [299/5000], Step [1/47], Loss: 0.0164\n",
      "Epoch [299/5000], Step [9/47], Loss: 0.0463\n",
      "Epoch [299/5000], Step [18/47], Loss: 0.0460\n",
      "Epoch [299/5000], Step [27/47], Loss: 0.0444\n",
      "Epoch [299/5000], Step [36/47], Loss: 0.0151\n",
      "Epoch [299/5000], Step [45/47], Loss: 0.0246\n",
      "Epoch [299/5000], Avg. Train Sample Loss: 0.0209, Avg. Validate Sample Loss: 0.0155,                             L2 Loss: 0.0085\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [300/5000] -------------------- \n",
      "Epoch [300/5000], Step [1/47], Loss: 0.0128\n",
      "Epoch [300/5000], Step [9/47], Loss: 0.0156\n",
      "Epoch [300/5000], Step [18/47], Loss: 0.0245\n",
      "Epoch [300/5000], Step [27/47], Loss: 0.0117\n",
      "Epoch [300/5000], Step [36/47], Loss: 0.0138\n",
      "Epoch [300/5000], Step [45/47], Loss: 0.0233\n",
      "Epoch [300/5000], Avg. Train Sample Loss: 0.0170, Avg. Validate Sample Loss: 0.0167,                             L2 Loss: 0.0073\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [301/5000] -------------------- \n",
      "Epoch [301/5000], Step [1/47], Loss: 0.0143\n",
      "Epoch [301/5000], Step [9/47], Loss: 0.0205\n",
      "Epoch [301/5000], Step [18/47], Loss: 0.0202\n",
      "Epoch [301/5000], Step [27/47], Loss: 0.0167\n",
      "Epoch [301/5000], Step [36/47], Loss: 0.0282\n",
      "Epoch [301/5000], Step [45/47], Loss: 0.0268\n",
      "Epoch [301/5000], Avg. Train Sample Loss: 0.0195, Avg. Validate Sample Loss: 0.0173,                             L2 Loss: 0.0125\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [302/5000] -------------------- \n",
      "Epoch [302/5000], Step [1/47], Loss: 0.0170\n",
      "Epoch [302/5000], Step [9/47], Loss: 0.0327\n",
      "Epoch [302/5000], Step [18/47], Loss: 0.0103\n",
      "Epoch [302/5000], Step [27/47], Loss: 0.0084\n",
      "Epoch [302/5000], Step [36/47], Loss: 0.0135\n",
      "Epoch [302/5000], Step [45/47], Loss: 0.0349\n",
      "Epoch [302/5000], Avg. Train Sample Loss: 0.0189, Avg. Validate Sample Loss: 0.0280,                             L2 Loss: 0.0186\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [303/5000] -------------------- \n",
      "Epoch [303/5000], Step [1/47], Loss: 0.0210\n",
      "Epoch [303/5000], Step [9/47], Loss: 0.0248\n",
      "Epoch [303/5000], Step [18/47], Loss: 0.0151\n",
      "Epoch [303/5000], Step [27/47], Loss: 0.0082\n",
      "Epoch [303/5000], Step [36/47], Loss: 0.0153\n",
      "Epoch [303/5000], Step [45/47], Loss: 0.0124\n",
      "Epoch [303/5000], Avg. Train Sample Loss: 0.0189, Avg. Validate Sample Loss: 0.0143,                             L2 Loss: 0.0075\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [304/5000] -------------------- \n",
      "Epoch [304/5000], Step [1/47], Loss: 0.0170\n",
      "Epoch [304/5000], Step [9/47], Loss: 0.0231\n",
      "Epoch [304/5000], Step [18/47], Loss: 0.0280\n",
      "Epoch [304/5000], Step [27/47], Loss: 0.0350\n",
      "Epoch [304/5000], Step [36/47], Loss: 0.0201\n",
      "Epoch [304/5000], Step [45/47], Loss: 0.0167\n",
      "Epoch [304/5000], Avg. Train Sample Loss: 0.0236, Avg. Validate Sample Loss: 0.0187,                             L2 Loss: 0.0137\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [305/5000] -------------------- \n",
      "Epoch [305/5000], Step [1/47], Loss: 0.0125\n",
      "Epoch [305/5000], Step [9/47], Loss: 0.0118\n",
      "Epoch [305/5000], Step [18/47], Loss: 0.0152\n",
      "Epoch [305/5000], Step [27/47], Loss: 0.0141\n",
      "Epoch [305/5000], Step [36/47], Loss: 0.0190\n",
      "Epoch [305/5000], Step [45/47], Loss: 0.0174\n",
      "Epoch [305/5000], Avg. Train Sample Loss: 0.0170, Avg. Validate Sample Loss: 0.0218,                             L2 Loss: 0.0141\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [306/5000] -------------------- \n",
      "Epoch [306/5000], Step [1/47], Loss: 0.0124\n",
      "Epoch [306/5000], Step [9/47], Loss: 0.0166\n",
      "Epoch [306/5000], Step [18/47], Loss: 0.0202\n",
      "Epoch [306/5000], Step [27/47], Loss: 0.0419\n",
      "Epoch [306/5000], Step [36/47], Loss: 0.0169\n",
      "Epoch [306/5000], Step [45/47], Loss: 0.0297\n",
      "Epoch [306/5000], Avg. Train Sample Loss: 0.0204, Avg. Validate Sample Loss: 0.0129,                             L2 Loss: 0.0080\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [307/5000] -------------------- \n",
      "Epoch [307/5000], Step [1/47], Loss: 0.0142\n",
      "Epoch [307/5000], Step [9/47], Loss: 0.0396\n",
      "Epoch [307/5000], Step [18/47], Loss: 0.0312\n",
      "Epoch [307/5000], Step [27/47], Loss: 0.0201\n",
      "Epoch [307/5000], Step [36/47], Loss: 0.0121\n",
      "Epoch [307/5000], Step [45/47], Loss: 0.0189\n",
      "Epoch [307/5000], Avg. Train Sample Loss: 0.0182, Avg. Validate Sample Loss: 0.0115,                             L2 Loss: 0.0067\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [308/5000] -------------------- \n",
      "Epoch [308/5000], Step [1/47], Loss: 0.0124\n",
      "Epoch [308/5000], Step [9/47], Loss: 0.0157\n",
      "Epoch [308/5000], Step [18/47], Loss: 0.0093\n",
      "Epoch [308/5000], Step [27/47], Loss: 0.0168\n",
      "Epoch [308/5000], Step [36/47], Loss: 0.0097\n",
      "Epoch [308/5000], Step [45/47], Loss: 0.0164\n",
      "Epoch [308/5000], Avg. Train Sample Loss: 0.0151, Avg. Validate Sample Loss: 0.0142,                             L2 Loss: 0.0119\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [309/5000] -------------------- \n",
      "Epoch [309/5000], Step [1/47], Loss: 0.0229\n",
      "Epoch [309/5000], Step [9/47], Loss: 0.0123\n",
      "Epoch [309/5000], Step [18/47], Loss: 0.0178\n",
      "Epoch [309/5000], Step [27/47], Loss: 0.0156\n",
      "Epoch [309/5000], Step [36/47], Loss: 0.0136\n",
      "Epoch [309/5000], Step [45/47], Loss: 0.0201\n",
      "Epoch [309/5000], Avg. Train Sample Loss: 0.0139, Avg. Validate Sample Loss: 0.0141,                             L2 Loss: 0.0054\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [310/5000] -------------------- \n",
      "Epoch [310/5000], Step [1/47], Loss: 0.0135\n",
      "Epoch [310/5000], Step [9/47], Loss: 0.0086\n",
      "Epoch [310/5000], Step [18/47], Loss: 0.0215\n",
      "Epoch [310/5000], Step [27/47], Loss: 0.0306\n",
      "Epoch [310/5000], Step [36/47], Loss: 0.0214\n",
      "Epoch [310/5000], Step [45/47], Loss: 0.0171\n",
      "Epoch [310/5000], Avg. Train Sample Loss: 0.0170, Avg. Validate Sample Loss: 0.0143,                             L2 Loss: 0.0093\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [311/5000] -------------------- \n",
      "Epoch [311/5000], Step [1/47], Loss: 0.0136\n",
      "Epoch [311/5000], Step [9/47], Loss: 0.0125\n",
      "Epoch [311/5000], Step [18/47], Loss: 0.0200\n",
      "Epoch [311/5000], Step [27/47], Loss: 0.0355\n",
      "Epoch [311/5000], Step [36/47], Loss: 0.0119\n",
      "Epoch [311/5000], Step [45/47], Loss: 0.0097\n",
      "Epoch [311/5000], Avg. Train Sample Loss: 0.0253, Avg. Validate Sample Loss: 0.0188,                             L2 Loss: 0.0138\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [312/5000] -------------------- \n",
      "Epoch [312/5000], Step [1/47], Loss: 0.0237\n",
      "Epoch [312/5000], Step [9/47], Loss: 0.0037\n",
      "Epoch [312/5000], Step [18/47], Loss: 0.0196\n",
      "Epoch [312/5000], Step [27/47], Loss: 0.0145\n",
      "Epoch [312/5000], Step [36/47], Loss: 0.0175\n",
      "Epoch [312/5000], Step [45/47], Loss: 0.0206\n",
      "Epoch [312/5000], Avg. Train Sample Loss: 0.0147, Avg. Validate Sample Loss: 0.0163,                             L2 Loss: 0.0106\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [313/5000] -------------------- \n",
      "Epoch [313/5000], Step [1/47], Loss: 0.0147\n",
      "Epoch [313/5000], Step [9/47], Loss: 0.0137\n",
      "Epoch [313/5000], Step [18/47], Loss: 0.0295\n",
      "Epoch [313/5000], Step [27/47], Loss: 0.0120\n",
      "Epoch [313/5000], Step [36/47], Loss: 0.0241\n",
      "Epoch [313/5000], Step [45/47], Loss: 0.0174\n",
      "Epoch [313/5000], Avg. Train Sample Loss: 0.0155, Avg. Validate Sample Loss: 0.0233,                             L2 Loss: 0.0192\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [314/5000] -------------------- \n",
      "Epoch [314/5000], Step [1/47], Loss: 0.0200\n",
      "Epoch [314/5000], Step [9/47], Loss: 0.0123\n",
      "Epoch [314/5000], Step [18/47], Loss: 0.0215\n",
      "Epoch [314/5000], Step [27/47], Loss: 0.0200\n",
      "Epoch [314/5000], Step [36/47], Loss: 0.0137\n",
      "Epoch [314/5000], Step [45/47], Loss: 0.0218\n",
      "Epoch [314/5000], Avg. Train Sample Loss: 0.0137, Avg. Validate Sample Loss: 0.0155,                             L2 Loss: 0.0115\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [315/5000] -------------------- \n",
      "Epoch [315/5000], Step [1/47], Loss: 0.0097\n",
      "Epoch [315/5000], Step [9/47], Loss: 0.0155\n",
      "Epoch [315/5000], Step [18/47], Loss: 0.0130\n",
      "Epoch [315/5000], Step [27/47], Loss: 0.0124\n",
      "Epoch [315/5000], Step [36/47], Loss: 0.0084\n",
      "Epoch [315/5000], Step [45/47], Loss: 0.0136\n",
      "Epoch [315/5000], Avg. Train Sample Loss: 0.0145, Avg. Validate Sample Loss: 0.0111,                             L2 Loss: 0.0064\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [316/5000] -------------------- \n",
      "Epoch [316/5000], Step [1/47], Loss: 0.0084\n",
      "Epoch [316/5000], Step [9/47], Loss: 0.0170\n",
      "Epoch [316/5000], Step [18/47], Loss: 0.0175\n",
      "Epoch [316/5000], Step [27/47], Loss: 0.0158\n",
      "Epoch [316/5000], Step [36/47], Loss: 0.0122\n",
      "Epoch [316/5000], Step [45/47], Loss: 0.0102\n",
      "Epoch [316/5000], Avg. Train Sample Loss: 0.0149, Avg. Validate Sample Loss: 0.0128,                             L2 Loss: 0.0100\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [317/5000] -------------------- \n",
      "Epoch [317/5000], Step [1/47], Loss: 0.0163\n",
      "Epoch [317/5000], Step [9/47], Loss: 0.0070\n",
      "Epoch [317/5000], Step [18/47], Loss: 0.0319\n",
      "Epoch [317/5000], Step [27/47], Loss: 0.0135\n",
      "Epoch [317/5000], Step [36/47], Loss: 0.0196\n",
      "Epoch [317/5000], Step [45/47], Loss: 0.0183\n",
      "Epoch [317/5000], Avg. Train Sample Loss: 0.0225, Avg. Validate Sample Loss: 0.0243,                             L2 Loss: 0.0181\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [318/5000] -------------------- \n",
      "Epoch [318/5000], Step [1/47], Loss: 0.0210\n",
      "Epoch [318/5000], Step [9/47], Loss: 0.0087\n",
      "Epoch [318/5000], Step [18/47], Loss: 0.0293\n",
      "Epoch [318/5000], Step [27/47], Loss: 0.0138\n",
      "Epoch [318/5000], Step [36/47], Loss: 0.0244\n",
      "Epoch [318/5000], Step [45/47], Loss: 0.0134\n",
      "Epoch [318/5000], Avg. Train Sample Loss: 0.0161, Avg. Validate Sample Loss: 0.0132,                             L2 Loss: 0.0098\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [319/5000] -------------------- \n",
      "Epoch [319/5000], Step [1/47], Loss: 0.0162\n",
      "Epoch [319/5000], Step [9/47], Loss: 0.0125\n",
      "Epoch [319/5000], Step [18/47], Loss: 0.0225\n",
      "Epoch [319/5000], Step [27/47], Loss: 0.0177\n",
      "Epoch [319/5000], Step [36/47], Loss: 0.0086\n",
      "Epoch [319/5000], Step [45/47], Loss: 0.0213\n",
      "Epoch [319/5000], Avg. Train Sample Loss: 0.0132, Avg. Validate Sample Loss: 0.0165,                             L2 Loss: 0.0144\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [320/5000] -------------------- \n",
      "Epoch [320/5000], Step [1/47], Loss: 0.0143\n",
      "Epoch [320/5000], Step [9/47], Loss: 0.0116\n",
      "Epoch [320/5000], Step [18/47], Loss: 0.0087\n",
      "Epoch [320/5000], Step [27/47], Loss: 0.0167\n",
      "Epoch [320/5000], Step [36/47], Loss: 0.0136\n",
      "Epoch [320/5000], Step [45/47], Loss: 0.0245\n",
      "Epoch [320/5000], Avg. Train Sample Loss: 0.0153, Avg. Validate Sample Loss: 0.0205,                             L2 Loss: 0.0153\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [321/5000] -------------------- \n",
      "Epoch [321/5000], Step [1/47], Loss: 0.0169\n",
      "Epoch [321/5000], Step [9/47], Loss: 0.0045\n",
      "Epoch [321/5000], Step [18/47], Loss: 0.0111\n",
      "Epoch [321/5000], Step [27/47], Loss: 0.0095\n",
      "Epoch [321/5000], Step [36/47], Loss: 0.0146\n",
      "Epoch [321/5000], Step [45/47], Loss: 0.0719\n",
      "Epoch [321/5000], Avg. Train Sample Loss: 0.0216, Avg. Validate Sample Loss: 0.0518,                             L2 Loss: 0.0321\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [322/5000] -------------------- \n",
      "Epoch [322/5000], Step [1/47], Loss: 0.0507\n",
      "Epoch [322/5000], Step [9/47], Loss: 0.0151\n",
      "Epoch [322/5000], Step [18/47], Loss: 0.0293\n",
      "Epoch [322/5000], Step [27/47], Loss: 0.0135\n",
      "Epoch [322/5000], Step [36/47], Loss: 0.0270\n",
      "Epoch [322/5000], Step [45/47], Loss: 0.0047\n",
      "Epoch [322/5000], Avg. Train Sample Loss: 0.0172, Avg. Validate Sample Loss: 0.0129,                             L2 Loss: 0.0102\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [323/5000] -------------------- \n",
      "Epoch [323/5000], Step [1/47], Loss: 0.0112\n",
      "Epoch [323/5000], Step [9/47], Loss: 0.0194\n",
      "Epoch [323/5000], Step [18/47], Loss: 0.0169\n",
      "Epoch [323/5000], Step [27/47], Loss: 0.0106\n",
      "Epoch [323/5000], Step [36/47], Loss: 0.0084\n",
      "Epoch [323/5000], Step [45/47], Loss: 0.0081\n",
      "Epoch [323/5000], Avg. Train Sample Loss: 0.0137, Avg. Validate Sample Loss: 0.0127,                             L2 Loss: 0.0106\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [324/5000] -------------------- \n",
      "Epoch [324/5000], Step [1/47], Loss: 0.0149\n",
      "Epoch [324/5000], Step [9/47], Loss: 0.0116\n",
      "Epoch [324/5000], Step [18/47], Loss: 0.0111\n",
      "Epoch [324/5000], Step [27/47], Loss: 0.0114\n",
      "Epoch [324/5000], Step [36/47], Loss: 0.0260\n",
      "Epoch [324/5000], Step [45/47], Loss: 0.0116\n",
      "Epoch [324/5000], Avg. Train Sample Loss: 0.0136, Avg. Validate Sample Loss: 0.0085,                             L2 Loss: 0.0069\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [325/5000] -------------------- \n",
      "Epoch [325/5000], Step [1/47], Loss: 0.0079\n",
      "Epoch [325/5000], Step [9/47], Loss: 0.0174\n",
      "Epoch [325/5000], Step [18/47], Loss: 0.0159\n",
      "Epoch [325/5000], Step [27/47], Loss: 0.0088\n",
      "Epoch [325/5000], Step [36/47], Loss: 0.0092\n",
      "Epoch [325/5000], Step [45/47], Loss: 0.0136\n",
      "Epoch [325/5000], Avg. Train Sample Loss: 0.0122, Avg. Validate Sample Loss: 0.0169,                             L2 Loss: 0.0127\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [326/5000] -------------------- \n",
      "Epoch [326/5000], Step [1/47], Loss: 0.0110\n",
      "Epoch [326/5000], Step [9/47], Loss: 0.0119\n",
      "Epoch [326/5000], Step [18/47], Loss: 0.0078\n",
      "Epoch [326/5000], Step [27/47], Loss: 0.0145\n",
      "Epoch [326/5000], Step [36/47], Loss: 0.0212\n",
      "Epoch [326/5000], Step [45/47], Loss: 0.0153\n",
      "Epoch [326/5000], Avg. Train Sample Loss: 0.0128, Avg. Validate Sample Loss: 0.0147,                             L2 Loss: 0.0139\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [327/5000] -------------------- \n",
      "Epoch [327/5000], Step [1/47], Loss: 0.0212\n",
      "Epoch [327/5000], Step [9/47], Loss: 0.0171\n",
      "Epoch [327/5000], Step [18/47], Loss: 0.0114\n",
      "Epoch [327/5000], Step [27/47], Loss: 0.0094\n",
      "Epoch [327/5000], Step [36/47], Loss: 0.0354\n",
      "Epoch [327/5000], Step [45/47], Loss: 0.0119\n",
      "Epoch [327/5000], Avg. Train Sample Loss: 0.0184, Avg. Validate Sample Loss: 0.0388,                             L2 Loss: 0.0285\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [328/5000] -------------------- \n",
      "Epoch [328/5000], Step [1/47], Loss: 0.0319\n",
      "Epoch [328/5000], Step [9/47], Loss: 0.0306\n",
      "Epoch [328/5000], Step [18/47], Loss: 0.0157\n",
      "Epoch [328/5000], Step [27/47], Loss: 0.0099\n",
      "Epoch [328/5000], Step [36/47], Loss: 0.0297\n",
      "Epoch [328/5000], Step [45/47], Loss: 0.0257\n",
      "Epoch [328/5000], Avg. Train Sample Loss: 0.0202, Avg. Validate Sample Loss: 0.0164,                             L2 Loss: 0.0132\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [329/5000] -------------------- \n",
      "Epoch [329/5000], Step [1/47], Loss: 0.0118\n",
      "Epoch [329/5000], Step [9/47], Loss: 0.0103\n",
      "Epoch [329/5000], Step [18/47], Loss: 0.0085\n",
      "Epoch [329/5000], Step [27/47], Loss: 0.0079\n",
      "Epoch [329/5000], Step [36/47], Loss: 0.0114\n",
      "Epoch [329/5000], Step [45/47], Loss: 0.0077\n",
      "Epoch [329/5000], Avg. Train Sample Loss: 0.0120, Avg. Validate Sample Loss: 0.0095,                             L2 Loss: 0.0048\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [330/5000] -------------------- \n",
      "Epoch [330/5000], Step [1/47], Loss: 0.0045\n",
      "Epoch [330/5000], Step [9/47], Loss: 0.0068\n",
      "Epoch [330/5000], Step [18/47], Loss: 0.0101\n",
      "Epoch [330/5000], Step [27/47], Loss: 0.0082\n",
      "Epoch [330/5000], Step [36/47], Loss: 0.0075\n",
      "Epoch [330/5000], Step [45/47], Loss: 0.0087\n",
      "Epoch [330/5000], Avg. Train Sample Loss: 0.0092, Avg. Validate Sample Loss: 0.0079,                             L2 Loss: 0.0042\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [331/5000] -------------------- \n",
      "Epoch [331/5000], Step [1/47], Loss: 0.0030\n",
      "Epoch [331/5000], Step [9/47], Loss: 0.0146\n",
      "Epoch [331/5000], Step [18/47], Loss: 0.0198\n",
      "Epoch [331/5000], Step [27/47], Loss: 0.0051\n",
      "Epoch [331/5000], Step [36/47], Loss: 0.0117\n",
      "Epoch [331/5000], Step [45/47], Loss: 0.0104\n",
      "Epoch [331/5000], Avg. Train Sample Loss: 0.0107, Avg. Validate Sample Loss: 0.0089,                             L2 Loss: 0.0082\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [332/5000] -------------------- \n",
      "Epoch [332/5000], Step [1/47], Loss: 0.0067\n",
      "Epoch [332/5000], Step [9/47], Loss: 0.0129\n",
      "Epoch [332/5000], Step [18/47], Loss: 0.0127\n",
      "Epoch [332/5000], Step [27/47], Loss: 0.0156\n",
      "Epoch [332/5000], Step [36/47], Loss: 0.0137\n",
      "Epoch [332/5000], Step [45/47], Loss: 0.0055\n",
      "Epoch [332/5000], Avg. Train Sample Loss: 0.0095, Avg. Validate Sample Loss: 0.0135,                             L2 Loss: 0.0105\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [333/5000] -------------------- \n",
      "Epoch [333/5000], Step [1/47], Loss: 0.0093\n",
      "Epoch [333/5000], Step [9/47], Loss: 0.0198\n",
      "Epoch [333/5000], Step [18/47], Loss: 0.0065\n",
      "Epoch [333/5000], Step [27/47], Loss: 0.0227\n",
      "Epoch [333/5000], Step [36/47], Loss: 0.0050\n",
      "Epoch [333/5000], Step [45/47], Loss: 0.0177\n",
      "Epoch [333/5000], Avg. Train Sample Loss: 0.0125, Avg. Validate Sample Loss: 0.0154,                             L2 Loss: 0.0142\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [334/5000] -------------------- \n",
      "Epoch [334/5000], Step [1/47], Loss: 0.0180\n",
      "Epoch [334/5000], Step [9/47], Loss: 0.0141\n",
      "Epoch [334/5000], Step [18/47], Loss: 0.0084\n",
      "Epoch [334/5000], Step [27/47], Loss: 0.0095\n",
      "Epoch [334/5000], Step [36/47], Loss: 0.0088\n",
      "Epoch [334/5000], Step [45/47], Loss: 0.0081\n",
      "Epoch [334/5000], Avg. Train Sample Loss: 0.0100, Avg. Validate Sample Loss: 0.0091,                             L2 Loss: 0.0069\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [335/5000] -------------------- \n",
      "Epoch [335/5000], Step [1/47], Loss: 0.0083\n",
      "Epoch [335/5000], Step [9/47], Loss: 0.0164\n",
      "Epoch [335/5000], Step [18/47], Loss: 0.0155\n",
      "Epoch [335/5000], Step [27/47], Loss: 0.0105\n",
      "Epoch [335/5000], Step [36/47], Loss: 0.0090\n",
      "Epoch [335/5000], Step [45/47], Loss: 0.0058\n",
      "Epoch [335/5000], Avg. Train Sample Loss: 0.0117, Avg. Validate Sample Loss: 0.0083,                             L2 Loss: 0.0075\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [336/5000] -------------------- \n",
      "Epoch [336/5000], Step [1/47], Loss: 0.0090\n",
      "Epoch [336/5000], Step [9/47], Loss: 0.0072\n",
      "Epoch [336/5000], Step [18/47], Loss: 0.0249\n",
      "Epoch [336/5000], Step [27/47], Loss: 0.0134\n",
      "Epoch [336/5000], Step [36/47], Loss: 0.0070\n",
      "Epoch [336/5000], Step [45/47], Loss: 0.0194\n",
      "Epoch [336/5000], Avg. Train Sample Loss: 0.0114, Avg. Validate Sample Loss: 0.0108,                             L2 Loss: 0.0116\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [337/5000] -------------------- \n",
      "Epoch [337/5000], Step [1/47], Loss: 0.0208\n",
      "Epoch [337/5000], Step [9/47], Loss: 0.0136\n",
      "Epoch [337/5000], Step [18/47], Loss: 0.0209\n",
      "Epoch [337/5000], Step [27/47], Loss: 0.0120\n",
      "Epoch [337/5000], Step [36/47], Loss: 0.0143\n",
      "Epoch [337/5000], Step [45/47], Loss: 0.0064\n",
      "Epoch [337/5000], Avg. Train Sample Loss: 0.0122, Avg. Validate Sample Loss: 0.0082,                             L2 Loss: 0.0074\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [338/5000] -------------------- \n",
      "Epoch [338/5000], Step [1/47], Loss: 0.0071\n",
      "Epoch [338/5000], Step [9/47], Loss: 0.0084\n",
      "Epoch [338/5000], Step [18/47], Loss: 0.0103\n",
      "Epoch [338/5000], Step [27/47], Loss: 0.0121\n",
      "Epoch [338/5000], Step [36/47], Loss: 0.0149\n",
      "Epoch [338/5000], Step [45/47], Loss: 0.0174\n",
      "Epoch [338/5000], Avg. Train Sample Loss: 0.0099, Avg. Validate Sample Loss: 0.0079,                             L2 Loss: 0.0031\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [339/5000] -------------------- \n",
      "Epoch [339/5000], Step [1/47], Loss: 0.0061\n",
      "Epoch [339/5000], Step [9/47], Loss: 0.0046\n",
      "Epoch [339/5000], Step [18/47], Loss: 0.0078\n",
      "Epoch [339/5000], Step [27/47], Loss: 0.0117\n",
      "Epoch [339/5000], Step [36/47], Loss: 0.0198\n",
      "Epoch [339/5000], Step [45/47], Loss: 0.0087\n",
      "Epoch [339/5000], Avg. Train Sample Loss: 0.0111, Avg. Validate Sample Loss: 0.0102,                             L2 Loss: 0.0100\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [340/5000] -------------------- \n",
      "Epoch [340/5000], Step [1/47], Loss: 0.0115\n",
      "Epoch [340/5000], Step [9/47], Loss: 0.0070\n",
      "Epoch [340/5000], Step [18/47], Loss: 0.0120\n",
      "Epoch [340/5000], Step [27/47], Loss: 0.0073\n",
      "Epoch [340/5000], Step [36/47], Loss: 0.0076\n",
      "Epoch [340/5000], Step [45/47], Loss: 0.0060\n",
      "Epoch [340/5000], Avg. Train Sample Loss: 0.0091, Avg. Validate Sample Loss: 0.0090,                             L2 Loss: 0.0089\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [341/5000] -------------------- \n",
      "Epoch [341/5000], Step [1/47], Loss: 0.0113\n",
      "Epoch [341/5000], Step [9/47], Loss: 0.0127\n",
      "Epoch [341/5000], Step [18/47], Loss: 0.0061\n",
      "Epoch [341/5000], Step [27/47], Loss: 0.0181\n",
      "Epoch [341/5000], Step [36/47], Loss: 0.0074\n",
      "Epoch [341/5000], Step [45/47], Loss: 0.0221\n",
      "Epoch [341/5000], Avg. Train Sample Loss: 0.0140, Avg. Validate Sample Loss: 0.0266,                             L2 Loss: 0.0202\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [342/5000] -------------------- \n",
      "Epoch [342/5000], Step [1/47], Loss: 0.0195\n",
      "Epoch [342/5000], Step [9/47], Loss: 0.0226\n",
      "Epoch [342/5000], Step [18/47], Loss: 0.0373\n",
      "Epoch [342/5000], Step [27/47], Loss: 0.0126\n",
      "Epoch [342/5000], Step [36/47], Loss: 0.0269\n",
      "Epoch [342/5000], Step [45/47], Loss: 0.0053\n",
      "Epoch [342/5000], Avg. Train Sample Loss: 0.0245, Avg. Validate Sample Loss: 0.0092,                             L2 Loss: 0.0081\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [343/5000] -------------------- \n",
      "Epoch [343/5000], Step [1/47], Loss: 0.0138\n",
      "Epoch [343/5000], Step [9/47], Loss: 0.0055\n",
      "Epoch [343/5000], Step [18/47], Loss: 0.0176\n",
      "Epoch [343/5000], Step [27/47], Loss: 0.0081\n",
      "Epoch [343/5000], Step [36/47], Loss: 0.0100\n",
      "Epoch [343/5000], Step [45/47], Loss: 0.0071\n",
      "Epoch [343/5000], Avg. Train Sample Loss: 0.0118, Avg. Validate Sample Loss: 0.0084,                             L2 Loss: 0.0076\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [344/5000] -------------------- \n",
      "Epoch [344/5000], Step [1/47], Loss: 0.0113\n",
      "Epoch [344/5000], Step [9/47], Loss: 0.0174\n",
      "Epoch [344/5000], Step [18/47], Loss: 0.0103\n",
      "Epoch [344/5000], Step [27/47], Loss: 0.0110\n",
      "Epoch [344/5000], Step [36/47], Loss: 0.0117\n",
      "Epoch [344/5000], Step [45/47], Loss: 0.0078\n",
      "Epoch [344/5000], Avg. Train Sample Loss: 0.0086, Avg. Validate Sample Loss: 0.0060,                             L2 Loss: 0.0034\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [345/5000] -------------------- \n",
      "Epoch [345/5000], Step [1/47], Loss: 0.0055\n",
      "Epoch [345/5000], Step [9/47], Loss: 0.0359\n",
      "Epoch [345/5000], Step [18/47], Loss: 0.0233\n",
      "Epoch [345/5000], Step [27/47], Loss: 0.0292\n",
      "Epoch [345/5000], Step [36/47], Loss: 0.0081\n",
      "Epoch [345/5000], Step [45/47], Loss: 0.0092\n",
      "Epoch [345/5000], Avg. Train Sample Loss: 0.0203, Avg. Validate Sample Loss: 0.0071,                             L2 Loss: 0.0080\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [346/5000] -------------------- \n",
      "Epoch [346/5000], Step [1/47], Loss: 0.0078\n",
      "Epoch [346/5000], Step [9/47], Loss: 0.0123\n",
      "Epoch [346/5000], Step [18/47], Loss: 0.0086\n",
      "Epoch [346/5000], Step [27/47], Loss: 0.0103\n",
      "Epoch [346/5000], Step [36/47], Loss: 0.0073\n",
      "Epoch [346/5000], Step [45/47], Loss: 0.0039\n",
      "Epoch [346/5000], Avg. Train Sample Loss: 0.0090, Avg. Validate Sample Loss: 0.0100,                             L2 Loss: 0.0102\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [347/5000] -------------------- \n",
      "Epoch [347/5000], Step [1/47], Loss: 0.0103\n",
      "Epoch [347/5000], Step [9/47], Loss: 0.0065\n",
      "Epoch [347/5000], Step [18/47], Loss: 0.0053\n",
      "Epoch [347/5000], Step [27/47], Loss: 0.0094\n",
      "Epoch [347/5000], Step [36/47], Loss: 0.0091\n",
      "Epoch [347/5000], Step [45/47], Loss: 0.0048\n",
      "Epoch [347/5000], Avg. Train Sample Loss: 0.0114, Avg. Validate Sample Loss: 0.0200,                             L2 Loss: 0.0173\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [348/5000] -------------------- \n",
      "Epoch [348/5000], Step [1/47], Loss: 0.0190\n",
      "Epoch [348/5000], Step [9/47], Loss: 0.0307\n",
      "Epoch [348/5000], Step [18/47], Loss: 0.0075\n",
      "Epoch [348/5000], Step [27/47], Loss: 0.0111\n",
      "Epoch [348/5000], Step [36/47], Loss: 0.0062\n",
      "Epoch [348/5000], Step [45/47], Loss: 0.0048\n",
      "Epoch [348/5000], Avg. Train Sample Loss: 0.0107, Avg. Validate Sample Loss: 0.0076,                             L2 Loss: 0.0084\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [349/5000] -------------------- \n",
      "Epoch [349/5000], Step [1/47], Loss: 0.0068\n",
      "Epoch [349/5000], Step [9/47], Loss: 0.0082\n",
      "Epoch [349/5000], Step [18/47], Loss: 0.0082\n",
      "Epoch [349/5000], Step [27/47], Loss: 0.0084\n",
      "Epoch [349/5000], Step [36/47], Loss: 0.0071\n",
      "Epoch [349/5000], Step [45/47], Loss: 0.0072\n",
      "Epoch [349/5000], Avg. Train Sample Loss: 0.0082, Avg. Validate Sample Loss: 0.0060,                             L2 Loss: 0.0058\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [350/5000] -------------------- \n",
      "Epoch [350/5000], Step [1/47], Loss: 0.0045\n",
      "Epoch [350/5000], Step [9/47], Loss: 0.0071\n",
      "Epoch [350/5000], Step [18/47], Loss: 0.0053\n",
      "Epoch [350/5000], Step [27/47], Loss: 0.0235\n",
      "Epoch [350/5000], Step [36/47], Loss: 0.0045\n",
      "Epoch [350/5000], Step [45/47], Loss: 0.0060\n",
      "Epoch [350/5000], Avg. Train Sample Loss: 0.0108, Avg. Validate Sample Loss: 0.0100,                             L2 Loss: 0.0107\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [351/5000] -------------------- \n",
      "Epoch [351/5000], Step [1/47], Loss: 0.0063\n",
      "Epoch [351/5000], Step [9/47], Loss: 0.0093\n",
      "Epoch [351/5000], Step [18/47], Loss: 0.0106\n",
      "Epoch [351/5000], Step [27/47], Loss: 0.0059\n",
      "Epoch [351/5000], Step [36/47], Loss: 0.0082\n",
      "Epoch [351/5000], Step [45/47], Loss: 0.0055\n",
      "Epoch [351/5000], Avg. Train Sample Loss: 0.0118, Avg. Validate Sample Loss: 0.0059,                             L2 Loss: 0.0041\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [352/5000] -------------------- \n",
      "Epoch [352/5000], Step [1/47], Loss: 0.0071\n",
      "Epoch [352/5000], Step [9/47], Loss: 0.0173\n",
      "Epoch [352/5000], Step [18/47], Loss: 0.0075\n",
      "Epoch [352/5000], Step [27/47], Loss: 0.0051\n",
      "Epoch [352/5000], Step [36/47], Loss: 0.0152\n",
      "Epoch [352/5000], Step [45/47], Loss: 0.0088\n",
      "Epoch [352/5000], Avg. Train Sample Loss: 0.0089, Avg. Validate Sample Loss: 0.0101,                             L2 Loss: 0.0093\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [353/5000] -------------------- \n",
      "Epoch [353/5000], Step [1/47], Loss: 0.0085\n",
      "Epoch [353/5000], Step [9/47], Loss: 0.0055\n",
      "Epoch [353/5000], Step [18/47], Loss: 0.0080\n",
      "Epoch [353/5000], Step [27/47], Loss: 0.0157\n",
      "Epoch [353/5000], Step [36/47], Loss: 0.0073\n",
      "Epoch [353/5000], Step [45/47], Loss: 0.0078\n",
      "Epoch [353/5000], Avg. Train Sample Loss: 0.0095, Avg. Validate Sample Loss: 0.0069,                             L2 Loss: 0.0060\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [354/5000] -------------------- \n",
      "Epoch [354/5000], Step [1/47], Loss: 0.0068\n",
      "Epoch [354/5000], Step [9/47], Loss: 0.0093\n",
      "Epoch [354/5000], Step [18/47], Loss: 0.0066\n",
      "Epoch [354/5000], Step [27/47], Loss: 0.0079\n",
      "Epoch [354/5000], Step [36/47], Loss: 0.0064\n",
      "Epoch [354/5000], Step [45/47], Loss: 0.0071\n",
      "Epoch [354/5000], Avg. Train Sample Loss: 0.0088, Avg. Validate Sample Loss: 0.0062,                             L2 Loss: 0.0037\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [355/5000] -------------------- \n",
      "Epoch [355/5000], Step [1/47], Loss: 0.0113\n",
      "Epoch [355/5000], Step [9/47], Loss: 0.0072\n",
      "Epoch [355/5000], Step [18/47], Loss: 0.0085\n",
      "Epoch [355/5000], Step [27/47], Loss: 0.0060\n",
      "Epoch [355/5000], Step [36/47], Loss: 0.0048\n",
      "Epoch [355/5000], Step [45/47], Loss: 0.0090\n",
      "Epoch [355/5000], Avg. Train Sample Loss: 0.0091, Avg. Validate Sample Loss: 0.0114,                             L2 Loss: 0.0118\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [356/5000] -------------------- \n",
      "Epoch [356/5000], Step [1/47], Loss: 0.0084\n",
      "Epoch [356/5000], Step [9/47], Loss: 0.0081\n",
      "Epoch [356/5000], Step [18/47], Loss: 0.0179\n",
      "Epoch [356/5000], Step [27/47], Loss: 0.0161\n",
      "Epoch [356/5000], Step [36/47], Loss: 0.0097\n",
      "Epoch [356/5000], Step [45/47], Loss: 0.0142\n",
      "Epoch [356/5000], Avg. Train Sample Loss: 0.0203, Avg. Validate Sample Loss: 0.0129,                             L2 Loss: 0.0109\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [357/5000] -------------------- \n",
      "Epoch [357/5000], Step [1/47], Loss: 0.0098\n",
      "Epoch [357/5000], Step [9/47], Loss: 0.0076\n",
      "Epoch [357/5000], Step [18/47], Loss: 0.0320\n",
      "Epoch [357/5000], Step [27/47], Loss: 0.0190\n",
      "Epoch [357/5000], Step [36/47], Loss: 0.0044\n",
      "Epoch [357/5000], Step [45/47], Loss: 0.0147\n",
      "Epoch [357/5000], Avg. Train Sample Loss: 0.0133, Avg. Validate Sample Loss: 0.0094,                             L2 Loss: 0.0104\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [358/5000] -------------------- \n",
      "Epoch [358/5000], Step [1/47], Loss: 0.0133\n",
      "Epoch [358/5000], Step [9/47], Loss: 0.0068\n",
      "Epoch [358/5000], Step [18/47], Loss: 0.0050\n",
      "Epoch [358/5000], Step [27/47], Loss: 0.0256\n",
      "Epoch [358/5000], Step [36/47], Loss: 0.0050\n",
      "Epoch [358/5000], Step [45/47], Loss: 0.0258\n",
      "Epoch [358/5000], Avg. Train Sample Loss: 0.0109, Avg. Validate Sample Loss: 0.0193,                             L2 Loss: 0.0181\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [359/5000] -------------------- \n",
      "Epoch [359/5000], Step [1/47], Loss: 0.0229\n",
      "Epoch [359/5000], Step [9/47], Loss: 0.0098\n",
      "Epoch [359/5000], Step [18/47], Loss: 0.0067\n",
      "Epoch [359/5000], Step [27/47], Loss: 0.0061\n",
      "Epoch [359/5000], Step [36/47], Loss: 0.0042\n",
      "Epoch [359/5000], Step [45/47], Loss: 0.0068\n",
      "Epoch [359/5000], Avg. Train Sample Loss: 0.0095, Avg. Validate Sample Loss: 0.0061,                             L2 Loss: 0.0065\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [360/5000] -------------------- \n",
      "Epoch [360/5000], Step [1/47], Loss: 0.0053\n",
      "Epoch [360/5000], Step [9/47], Loss: 0.0072\n",
      "Epoch [360/5000], Step [18/47], Loss: 0.0052\n",
      "Epoch [360/5000], Step [27/47], Loss: 0.0109\n",
      "Epoch [360/5000], Step [36/47], Loss: 0.0088\n",
      "Epoch [360/5000], Step [45/47], Loss: 0.0051\n",
      "Epoch [360/5000], Avg. Train Sample Loss: 0.0071, Avg. Validate Sample Loss: 0.0060,                             L2 Loss: 0.0053\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [361/5000] -------------------- \n",
      "Epoch [361/5000], Step [1/47], Loss: 0.0072\n",
      "Epoch [361/5000], Step [9/47], Loss: 0.0163\n",
      "Epoch [361/5000], Step [18/47], Loss: 0.0491\n",
      "Epoch [361/5000], Step [27/47], Loss: 0.0082\n",
      "Epoch [361/5000], Step [36/47], Loss: 0.0073\n",
      "Epoch [361/5000], Step [45/47], Loss: 0.0045\n",
      "Epoch [361/5000], Avg. Train Sample Loss: 0.0127, Avg. Validate Sample Loss: 0.0123,                             L2 Loss: 0.0117\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [362/5000] -------------------- \n",
      "Epoch [362/5000], Step [1/47], Loss: 0.0118\n",
      "Epoch [362/5000], Step [9/47], Loss: 0.0129\n",
      "Epoch [362/5000], Step [18/47], Loss: 0.0349\n",
      "Epoch [362/5000], Step [27/47], Loss: 0.0362\n",
      "Epoch [362/5000], Step [36/47], Loss: 0.0101\n",
      "Epoch [362/5000], Step [45/47], Loss: 0.0081\n",
      "Epoch [362/5000], Avg. Train Sample Loss: 0.0162, Avg. Validate Sample Loss: 0.0066,                             L2 Loss: 0.0069\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [363/5000] -------------------- \n",
      "Epoch [363/5000], Step [1/47], Loss: 0.0066\n",
      "Epoch [363/5000], Step [9/47], Loss: 0.0049\n",
      "Epoch [363/5000], Step [18/47], Loss: 0.0082\n",
      "Epoch [363/5000], Step [27/47], Loss: 0.0166\n",
      "Epoch [363/5000], Step [36/47], Loss: 0.0127\n",
      "Epoch [363/5000], Step [45/47], Loss: 0.0143\n",
      "Epoch [363/5000], Avg. Train Sample Loss: 0.0114, Avg. Validate Sample Loss: 0.0255,                             L2 Loss: 0.0206\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [364/5000] -------------------- \n",
      "Epoch [364/5000], Step [1/47], Loss: 0.0205\n",
      "Epoch [364/5000], Step [9/47], Loss: 0.0103\n",
      "Epoch [364/5000], Step [18/47], Loss: 0.0056\n",
      "Epoch [364/5000], Step [27/47], Loss: 0.0107\n",
      "Epoch [364/5000], Step [36/47], Loss: 0.0037\n",
      "Epoch [364/5000], Step [45/47], Loss: 0.0097\n",
      "Epoch [364/5000], Avg. Train Sample Loss: 0.0103, Avg. Validate Sample Loss: 0.0058,                             L2 Loss: 0.0044\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [365/5000] -------------------- \n",
      "Epoch [365/5000], Step [1/47], Loss: 0.0054\n",
      "Epoch [365/5000], Step [9/47], Loss: 0.0084\n",
      "Epoch [365/5000], Step [18/47], Loss: 0.0088\n",
      "Epoch [365/5000], Step [27/47], Loss: 0.0095\n",
      "Epoch [365/5000], Step [36/47], Loss: 0.0223\n",
      "Epoch [365/5000], Step [45/47], Loss: 0.0163\n",
      "Epoch [365/5000], Avg. Train Sample Loss: 0.0090, Avg. Validate Sample Loss: 0.0066,                             L2 Loss: 0.0087\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [366/5000] -------------------- \n",
      "Epoch [366/5000], Step [1/47], Loss: 0.0066\n",
      "Epoch [366/5000], Step [9/47], Loss: 0.0161\n",
      "Epoch [366/5000], Step [18/47], Loss: 0.0068\n",
      "Epoch [366/5000], Step [27/47], Loss: 0.0173\n",
      "Epoch [366/5000], Step [36/47], Loss: 0.0066\n",
      "Epoch [366/5000], Step [45/47], Loss: 0.0048\n",
      "Epoch [366/5000], Avg. Train Sample Loss: 0.0095, Avg. Validate Sample Loss: 0.0069,                             L2 Loss: 0.0042\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [367/5000] -------------------- \n",
      "Epoch [367/5000], Step [1/47], Loss: 0.0085\n",
      "Epoch [367/5000], Step [9/47], Loss: 0.0063\n",
      "Epoch [367/5000], Step [18/47], Loss: 0.0052\n",
      "Epoch [367/5000], Step [27/47], Loss: 0.0051\n",
      "Epoch [367/5000], Step [36/47], Loss: 0.0067\n",
      "Epoch [367/5000], Step [45/47], Loss: 0.0087\n",
      "Epoch [367/5000], Avg. Train Sample Loss: 0.0068, Avg. Validate Sample Loss: 0.0094,                             L2 Loss: 0.0099\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [368/5000] -------------------- \n",
      "Epoch [368/5000], Step [1/47], Loss: 0.0058\n",
      "Epoch [368/5000], Step [9/47], Loss: 0.0121\n",
      "Epoch [368/5000], Step [18/47], Loss: 0.0330\n",
      "Epoch [368/5000], Step [27/47], Loss: 0.0595\n",
      "Epoch [368/5000], Step [36/47], Loss: 0.0051\n",
      "Epoch [368/5000], Step [45/47], Loss: 0.0320\n",
      "Epoch [368/5000], Avg. Train Sample Loss: 0.0250, Avg. Validate Sample Loss: 0.0094,                             L2 Loss: 0.0059\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [369/5000] -------------------- \n",
      "Epoch [369/5000], Step [1/47], Loss: 0.0087\n",
      "Epoch [369/5000], Step [9/47], Loss: 0.0050\n",
      "Epoch [369/5000], Step [18/47], Loss: 0.0070\n",
      "Epoch [369/5000], Step [27/47], Loss: 0.0027\n",
      "Epoch [369/5000], Step [36/47], Loss: 0.0056\n",
      "Epoch [369/5000], Step [45/47], Loss: 0.0108\n",
      "Epoch [369/5000], Avg. Train Sample Loss: 0.0073, Avg. Validate Sample Loss: 0.0068,                             L2 Loss: 0.0066\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [370/5000] -------------------- \n",
      "Epoch [370/5000], Step [1/47], Loss: 0.0094\n",
      "Epoch [370/5000], Step [9/47], Loss: 0.0081\n",
      "Epoch [370/5000], Step [18/47], Loss: 0.0155\n",
      "Epoch [370/5000], Step [27/47], Loss: 0.0129\n",
      "Epoch [370/5000], Step [36/47], Loss: 0.0045\n",
      "Epoch [370/5000], Step [45/47], Loss: 0.0083\n",
      "Epoch [370/5000], Avg. Train Sample Loss: 0.0078, Avg. Validate Sample Loss: 0.0116,                             L2 Loss: 0.0121\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [371/5000] -------------------- \n",
      "Epoch [371/5000], Step [1/47], Loss: 0.0150\n",
      "Epoch [371/5000], Step [9/47], Loss: 0.0033\n",
      "Epoch [371/5000], Step [18/47], Loss: 0.0096\n",
      "Epoch [371/5000], Step [27/47], Loss: 0.0085\n",
      "Epoch [371/5000], Step [36/47], Loss: 0.0070\n",
      "Epoch [371/5000], Step [45/47], Loss: 0.0072\n",
      "Epoch [371/5000], Avg. Train Sample Loss: 0.0075, Avg. Validate Sample Loss: 0.0069,                             L2 Loss: 0.0078\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [372/5000] -------------------- \n",
      "Epoch [372/5000], Step [1/47], Loss: 0.0099\n",
      "Epoch [372/5000], Step [9/47], Loss: 0.0051\n",
      "Epoch [372/5000], Step [18/47], Loss: 0.0151\n",
      "Epoch [372/5000], Step [27/47], Loss: 0.0034\n",
      "Epoch [372/5000], Step [36/47], Loss: 0.0053\n",
      "Epoch [372/5000], Step [45/47], Loss: 0.0059\n",
      "Epoch [372/5000], Avg. Train Sample Loss: 0.0108, Avg. Validate Sample Loss: 0.0052,                             L2 Loss: 0.0042\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [373/5000] -------------------- \n",
      "Epoch [373/5000], Step [1/47], Loss: 0.0052\n",
      "Epoch [373/5000], Step [9/47], Loss: 0.0045\n",
      "Epoch [373/5000], Step [18/47], Loss: 0.0122\n",
      "Epoch [373/5000], Step [27/47], Loss: 0.0045\n",
      "Epoch [373/5000], Step [36/47], Loss: 0.0047\n",
      "Epoch [373/5000], Step [45/47], Loss: 0.0051\n",
      "Epoch [373/5000], Avg. Train Sample Loss: 0.0065, Avg. Validate Sample Loss: 0.0058,                             L2 Loss: 0.0051\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [374/5000] -------------------- \n",
      "Epoch [374/5000], Step [1/47], Loss: 0.0047\n",
      "Epoch [374/5000], Step [9/47], Loss: 0.0049\n",
      "Epoch [374/5000], Step [18/47], Loss: 0.0060\n",
      "Epoch [374/5000], Step [27/47], Loss: 0.0040\n",
      "Epoch [374/5000], Step [36/47], Loss: 0.0103\n",
      "Epoch [374/5000], Step [45/47], Loss: 0.0061\n",
      "Epoch [374/5000], Avg. Train Sample Loss: 0.0064, Avg. Validate Sample Loss: 0.0051,                             L2 Loss: 0.0046\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [375/5000] -------------------- \n",
      "Epoch [375/5000], Step [1/47], Loss: 0.0118\n",
      "Epoch [375/5000], Step [9/47], Loss: 0.0030\n",
      "Epoch [375/5000], Step [18/47], Loss: 0.0171\n",
      "Epoch [375/5000], Step [27/47], Loss: 0.0338\n",
      "Epoch [375/5000], Step [36/47], Loss: 0.0166\n",
      "Epoch [375/5000], Step [45/47], Loss: 0.0040\n",
      "Epoch [375/5000], Avg. Train Sample Loss: 0.0159, Avg. Validate Sample Loss: 0.0058,                             L2 Loss: 0.0084\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [376/5000] -------------------- \n",
      "Epoch [376/5000], Step [1/47], Loss: 0.0047\n",
      "Epoch [376/5000], Step [9/47], Loss: 0.0049\n",
      "Epoch [376/5000], Step [18/47], Loss: 0.0058\n",
      "Epoch [376/5000], Step [27/47], Loss: 0.0052\n",
      "Epoch [376/5000], Step [36/47], Loss: 0.0063\n",
      "Epoch [376/5000], Step [45/47], Loss: 0.0070\n",
      "Epoch [376/5000], Avg. Train Sample Loss: 0.0069, Avg. Validate Sample Loss: 0.0056,                             L2 Loss: 0.0024\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [377/5000] -------------------- \n",
      "Epoch [377/5000], Step [1/47], Loss: 0.0059\n",
      "Epoch [377/5000], Step [9/47], Loss: 0.0070\n",
      "Epoch [377/5000], Step [18/47], Loss: 0.0054\n",
      "Epoch [377/5000], Step [27/47], Loss: 0.0048\n",
      "Epoch [377/5000], Step [36/47], Loss: 0.0047\n",
      "Epoch [377/5000], Step [45/47], Loss: 0.0053\n",
      "Epoch [377/5000], Avg. Train Sample Loss: 0.0086, Avg. Validate Sample Loss: 0.0087,                             L2 Loss: 0.0094\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [378/5000] -------------------- \n",
      "Epoch [378/5000], Step [1/47], Loss: 0.0091\n",
      "Epoch [378/5000], Step [9/47], Loss: 0.0136\n",
      "Epoch [378/5000], Step [18/47], Loss: 0.0041\n",
      "Epoch [378/5000], Step [27/47], Loss: 0.0268\n",
      "Epoch [378/5000], Step [36/47], Loss: 0.0122\n",
      "Epoch [378/5000], Step [45/47], Loss: 0.0064\n",
      "Epoch [378/5000], Avg. Train Sample Loss: 0.0149, Avg. Validate Sample Loss: 0.0075,                             L2 Loss: 0.0111\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [379/5000] -------------------- \n",
      "Epoch [379/5000], Step [1/47], Loss: 0.0055\n",
      "Epoch [379/5000], Step [9/47], Loss: 0.0216\n",
      "Epoch [379/5000], Step [18/47], Loss: 0.0322\n",
      "Epoch [379/5000], Step [27/47], Loss: 0.0113\n",
      "Epoch [379/5000], Step [36/47], Loss: 0.0320\n",
      "Epoch [379/5000], Step [45/47], Loss: 0.0136\n",
      "Epoch [379/5000], Avg. Train Sample Loss: 0.0195, Avg. Validate Sample Loss: 0.0049,                             L2 Loss: 0.0030\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [380/5000] -------------------- \n",
      "Epoch [380/5000], Step [1/47], Loss: 0.0057\n",
      "Epoch [380/5000], Step [9/47], Loss: 0.0103\n",
      "Epoch [380/5000], Step [18/47], Loss: 0.0183\n",
      "Epoch [380/5000], Step [27/47], Loss: 0.0061\n",
      "Epoch [380/5000], Step [36/47], Loss: 0.0082\n",
      "Epoch [380/5000], Step [45/47], Loss: 0.0067\n",
      "Epoch [380/5000], Avg. Train Sample Loss: 0.0094, Avg. Validate Sample Loss: 0.0059,                             L2 Loss: 0.0041\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [381/5000] -------------------- \n",
      "Epoch [381/5000], Step [1/47], Loss: 0.0052\n",
      "Epoch [381/5000], Step [9/47], Loss: 0.0059\n",
      "Epoch [381/5000], Step [18/47], Loss: 0.0041\n",
      "Epoch [381/5000], Step [27/47], Loss: 0.0080\n",
      "Epoch [381/5000], Step [36/47], Loss: 0.0061\n",
      "Epoch [381/5000], Step [45/47], Loss: 0.0051\n",
      "Epoch [381/5000], Avg. Train Sample Loss: 0.0060, Avg. Validate Sample Loss: 0.0092,                             L2 Loss: 0.0104\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [382/5000] -------------------- \n",
      "Epoch [382/5000], Step [1/47], Loss: 0.0101\n",
      "Epoch [382/5000], Step [9/47], Loss: 0.0048\n",
      "Epoch [382/5000], Step [18/47], Loss: 0.0083\n",
      "Epoch [382/5000], Step [27/47], Loss: 0.0042\n",
      "Epoch [382/5000], Step [36/47], Loss: 0.0090\n",
      "Epoch [382/5000], Step [45/47], Loss: 0.0041\n",
      "Epoch [382/5000], Avg. Train Sample Loss: 0.0055, Avg. Validate Sample Loss: 0.0053,                             L2 Loss: 0.0068\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [383/5000] -------------------- \n",
      "Epoch [383/5000], Step [1/47], Loss: 0.0031\n",
      "Epoch [383/5000], Step [9/47], Loss: 0.0076\n",
      "Epoch [383/5000], Step [18/47], Loss: 0.0097\n",
      "Epoch [383/5000], Step [27/47], Loss: 0.0072\n",
      "Epoch [383/5000], Step [36/47], Loss: 0.0160\n",
      "Epoch [383/5000], Step [45/47], Loss: 0.0051\n",
      "Epoch [383/5000], Avg. Train Sample Loss: 0.0067, Avg. Validate Sample Loss: 0.0052,                             L2 Loss: 0.0033\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [384/5000] -------------------- \n",
      "Epoch [384/5000], Step [1/47], Loss: 0.0037\n",
      "Epoch [384/5000], Step [9/47], Loss: 0.0086\n",
      "Epoch [384/5000], Step [18/47], Loss: 0.0244\n",
      "Epoch [384/5000], Step [27/47], Loss: 0.0205\n",
      "Epoch [384/5000], Step [36/47], Loss: 0.0164\n",
      "Epoch [384/5000], Step [45/47], Loss: 0.0254\n",
      "Epoch [384/5000], Avg. Train Sample Loss: 0.0121, Avg. Validate Sample Loss: 0.0101,                             L2 Loss: 0.0105\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [385/5000] -------------------- \n",
      "Epoch [385/5000], Step [1/47], Loss: 0.0063\n",
      "Epoch [385/5000], Step [9/47], Loss: 0.0048\n",
      "Epoch [385/5000], Step [18/47], Loss: 0.0079\n",
      "Epoch [385/5000], Step [27/47], Loss: 0.0119\n",
      "Epoch [385/5000], Step [36/47], Loss: 0.0172\n",
      "Epoch [385/5000], Step [45/47], Loss: 0.0087\n",
      "Epoch [385/5000], Avg. Train Sample Loss: 0.0087, Avg. Validate Sample Loss: 0.0072,                             L2 Loss: 0.0071\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [386/5000] -------------------- \n",
      "Epoch [386/5000], Step [1/47], Loss: 0.0046\n",
      "Epoch [386/5000], Step [9/47], Loss: 0.0059\n",
      "Epoch [386/5000], Step [18/47], Loss: 0.0077\n",
      "Epoch [386/5000], Step [27/47], Loss: 0.0190\n",
      "Epoch [386/5000], Step [36/47], Loss: 0.0284\n",
      "Epoch [386/5000], Step [45/47], Loss: 0.0265\n",
      "Epoch [386/5000], Avg. Train Sample Loss: 0.0111, Avg. Validate Sample Loss: 0.0083,                             L2 Loss: 0.0094\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [387/5000] -------------------- \n",
      "Epoch [387/5000], Step [1/47], Loss: 0.0077\n",
      "Epoch [387/5000], Step [9/47], Loss: 0.0122\n",
      "Epoch [387/5000], Step [18/47], Loss: 0.0051\n",
      "Epoch [387/5000], Step [27/47], Loss: 0.0052\n",
      "Epoch [387/5000], Step [36/47], Loss: 0.0118\n",
      "Epoch [387/5000], Step [45/47], Loss: 0.0185\n",
      "Epoch [387/5000], Avg. Train Sample Loss: 0.0086, Avg. Validate Sample Loss: 0.0212,                             L2 Loss: 0.0195\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [388/5000] -------------------- \n",
      "Epoch [388/5000], Step [1/47], Loss: 0.0161\n",
      "Epoch [388/5000], Step [9/47], Loss: 0.0082\n",
      "Epoch [388/5000], Step [18/47], Loss: 0.0130\n",
      "Epoch [388/5000], Step [27/47], Loss: 0.0078\n",
      "Epoch [388/5000], Step [36/47], Loss: 0.0129\n",
      "Epoch [388/5000], Step [45/47], Loss: 0.0239\n",
      "Epoch [388/5000], Avg. Train Sample Loss: 0.0111, Avg. Validate Sample Loss: 0.0119,                             L2 Loss: 0.0135\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [389/5000] -------------------- \n",
      "Epoch [389/5000], Step [1/47], Loss: 0.0107\n",
      "Epoch [389/5000], Step [9/47], Loss: 0.0028\n",
      "Epoch [389/5000], Step [18/47], Loss: 0.0049\n",
      "Epoch [389/5000], Step [27/47], Loss: 0.0051\n",
      "Epoch [389/5000], Step [36/47], Loss: 0.0042\n",
      "Epoch [389/5000], Step [45/47], Loss: 0.0156\n",
      "Epoch [389/5000], Avg. Train Sample Loss: 0.0067, Avg. Validate Sample Loss: 0.0213,                             L2 Loss: 0.0191\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [390/5000] -------------------- \n",
      "Epoch [390/5000], Step [1/47], Loss: 0.0274\n",
      "Epoch [390/5000], Step [9/47], Loss: 0.0053\n",
      "Epoch [390/5000], Step [18/47], Loss: 0.0208\n",
      "Epoch [390/5000], Step [27/47], Loss: 0.0127\n",
      "Epoch [390/5000], Step [36/47], Loss: 0.0097\n",
      "Epoch [390/5000], Step [45/47], Loss: 0.0064\n",
      "Epoch [390/5000], Avg. Train Sample Loss: 0.0141, Avg. Validate Sample Loss: 0.0142,                             L2 Loss: 0.0149\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [391/5000] -------------------- \n",
      "Epoch [391/5000], Step [1/47], Loss: 0.0116\n",
      "Epoch [391/5000], Step [9/47], Loss: 0.0121\n",
      "Epoch [391/5000], Step [18/47], Loss: 0.0106\n",
      "Epoch [391/5000], Step [27/47], Loss: 0.0097\n",
      "Epoch [391/5000], Step [36/47], Loss: 0.0093\n",
      "Epoch [391/5000], Step [45/47], Loss: 0.0059\n",
      "Epoch [391/5000], Avg. Train Sample Loss: 0.0083, Avg. Validate Sample Loss: 0.0059,                             L2 Loss: 0.0073\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [392/5000] -------------------- \n",
      "Epoch [392/5000], Step [1/47], Loss: 0.0044\n",
      "Epoch [392/5000], Step [9/47], Loss: 0.0038\n",
      "Epoch [392/5000], Step [18/47], Loss: 0.0060\n",
      "Epoch [392/5000], Step [27/47], Loss: 0.0078\n",
      "Epoch [392/5000], Step [36/47], Loss: 0.0048\n",
      "Epoch [392/5000], Step [45/47], Loss: 0.0036\n",
      "Epoch [392/5000], Avg. Train Sample Loss: 0.0060, Avg. Validate Sample Loss: 0.0079,                             L2 Loss: 0.0081\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [393/5000] -------------------- \n",
      "Epoch [393/5000], Step [1/47], Loss: 0.0045\n",
      "Epoch [393/5000], Step [9/47], Loss: 0.0030\n",
      "Epoch [393/5000], Step [18/47], Loss: 0.0072\n",
      "Epoch [393/5000], Step [27/47], Loss: 0.0059\n",
      "Epoch [393/5000], Step [36/47], Loss: 0.0062\n",
      "Epoch [393/5000], Step [45/47], Loss: 0.0135\n",
      "Epoch [393/5000], Avg. Train Sample Loss: 0.0062, Avg. Validate Sample Loss: 0.0217,                             L2 Loss: 0.0189\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [394/5000] -------------------- \n",
      "Epoch [394/5000], Step [1/47], Loss: 0.0190\n",
      "Epoch [394/5000], Step [9/47], Loss: 0.0166\n",
      "Epoch [394/5000], Step [18/47], Loss: 0.0349\n",
      "Epoch [394/5000], Step [27/47], Loss: 0.0239\n",
      "Epoch [394/5000], Step [36/47], Loss: 0.0156\n",
      "Epoch [394/5000], Step [45/47], Loss: 0.0149\n",
      "Epoch [394/5000], Avg. Train Sample Loss: 0.0211, Avg. Validate Sample Loss: 0.0045,                             L2 Loss: 0.0063\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [395/5000] -------------------- \n",
      "Epoch [395/5000], Step [1/47], Loss: 0.0055\n",
      "Epoch [395/5000], Step [9/47], Loss: 0.0045\n",
      "Epoch [395/5000], Step [18/47], Loss: 0.0076\n",
      "Epoch [395/5000], Step [27/47], Loss: 0.0109\n",
      "Epoch [395/5000], Step [36/47], Loss: 0.0099\n",
      "Epoch [395/5000], Step [45/47], Loss: 0.0037\n",
      "Epoch [395/5000], Avg. Train Sample Loss: 0.0084, Avg. Validate Sample Loss: 0.0063,                             L2 Loss: 0.0072\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [396/5000] -------------------- \n",
      "Epoch [396/5000], Step [1/47], Loss: 0.0056\n",
      "Epoch [396/5000], Step [9/47], Loss: 0.0032\n",
      "Epoch [396/5000], Step [18/47], Loss: 0.0155\n",
      "Epoch [396/5000], Step [27/47], Loss: 0.0055\n",
      "Epoch [396/5000], Step [36/47], Loss: 0.0058\n",
      "Epoch [396/5000], Step [45/47], Loss: 0.0126\n",
      "Epoch [396/5000], Avg. Train Sample Loss: 0.0070, Avg. Validate Sample Loss: 0.0079,                             L2 Loss: 0.0108\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [397/5000] -------------------- \n",
      "Epoch [397/5000], Step [1/47], Loss: 0.0071\n",
      "Epoch [397/5000], Step [9/47], Loss: 0.0074\n",
      "Epoch [397/5000], Step [18/47], Loss: 0.0090\n",
      "Epoch [397/5000], Step [27/47], Loss: 0.0036\n",
      "Epoch [397/5000], Step [36/47], Loss: 0.0044\n",
      "Epoch [397/5000], Step [45/47], Loss: 0.0144\n",
      "Epoch [397/5000], Avg. Train Sample Loss: 0.0058, Avg. Validate Sample Loss: 0.0049,                             L2 Loss: 0.0056\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [398/5000] -------------------- \n",
      "Epoch [398/5000], Step [1/47], Loss: 0.0036\n",
      "Epoch [398/5000], Step [9/47], Loss: 0.0069\n",
      "Epoch [398/5000], Step [18/47], Loss: 0.0139\n",
      "Epoch [398/5000], Step [27/47], Loss: 0.0026\n",
      "Epoch [398/5000], Step [36/47], Loss: 0.0040\n",
      "Epoch [398/5000], Step [45/47], Loss: 0.0064\n",
      "Epoch [398/5000], Avg. Train Sample Loss: 0.0067, Avg. Validate Sample Loss: 0.0083,                             L2 Loss: 0.0113\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [399/5000] -------------------- \n",
      "Epoch [399/5000], Step [1/47], Loss: 0.0066\n",
      "Epoch [399/5000], Step [9/47], Loss: 0.0072\n",
      "Epoch [399/5000], Step [18/47], Loss: 0.0060\n",
      "Epoch [399/5000], Step [27/47], Loss: 0.0098\n",
      "Epoch [399/5000], Step [36/47], Loss: 0.0098\n",
      "Epoch [399/5000], Step [45/47], Loss: 0.0036\n",
      "Epoch [399/5000], Avg. Train Sample Loss: 0.0073, Avg. Validate Sample Loss: 0.0049,                             L2 Loss: 0.0027\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [400/5000] -------------------- \n",
      "Epoch [400/5000], Step [1/47], Loss: 0.0040\n",
      "Epoch [400/5000], Step [9/47], Loss: 0.0139\n",
      "Epoch [400/5000], Step [18/47], Loss: 0.0048\n",
      "Epoch [400/5000], Step [27/47], Loss: 0.0164\n",
      "Epoch [400/5000], Step [36/47], Loss: 0.0145\n",
      "Epoch [400/5000], Step [45/47], Loss: 0.0071\n",
      "Epoch [400/5000], Avg. Train Sample Loss: 0.0162, Avg. Validate Sample Loss: 0.0065,                             L2 Loss: 0.0074\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [401/5000] -------------------- \n",
      "Epoch [401/5000], Step [1/47], Loss: 0.0054\n",
      "Epoch [401/5000], Step [9/47], Loss: 0.0101\n",
      "Epoch [401/5000], Step [18/47], Loss: 0.0043\n",
      "Epoch [401/5000], Step [27/47], Loss: 0.0053\n",
      "Epoch [401/5000], Step [36/47], Loss: 0.0063\n",
      "Epoch [401/5000], Step [45/47], Loss: 0.0055\n",
      "Epoch [401/5000], Avg. Train Sample Loss: 0.0070, Avg. Validate Sample Loss: 0.0148,                             L2 Loss: 0.0155\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [402/5000] -------------------- \n",
      "Epoch [402/5000], Step [1/47], Loss: 0.0145\n",
      "Epoch [402/5000], Step [9/47], Loss: 0.0068\n",
      "Epoch [402/5000], Step [18/47], Loss: 0.0099\n",
      "Epoch [402/5000], Step [27/47], Loss: 0.0026\n",
      "Epoch [402/5000], Step [36/47], Loss: 0.0030\n",
      "Epoch [402/5000], Step [45/47], Loss: 0.0169\n",
      "Epoch [402/5000], Avg. Train Sample Loss: 0.0071, Avg. Validate Sample Loss: 0.0175,                             L2 Loss: 0.0178\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [403/5000] -------------------- \n",
      "Epoch [403/5000], Step [1/47], Loss: 0.0179\n",
      "Epoch [403/5000], Step [9/47], Loss: 0.0198\n",
      "Epoch [403/5000], Step [18/47], Loss: 0.0155\n",
      "Epoch [403/5000], Step [27/47], Loss: 0.0065\n",
      "Epoch [403/5000], Step [36/47], Loss: 0.0060\n",
      "Epoch [403/5000], Step [45/47], Loss: 0.0041\n",
      "Epoch [403/5000], Avg. Train Sample Loss: 0.0106, Avg. Validate Sample Loss: 0.0050,                             L2 Loss: 0.0060\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [404/5000] -------------------- \n",
      "Epoch [404/5000], Step [1/47], Loss: 0.0048\n",
      "Epoch [404/5000], Step [9/47], Loss: 0.0041\n",
      "Epoch [404/5000], Step [18/47], Loss: 0.0079\n",
      "Epoch [404/5000], Step [27/47], Loss: 0.0155\n",
      "Epoch [404/5000], Step [36/47], Loss: 0.0061\n",
      "Epoch [404/5000], Step [45/47], Loss: 0.0050\n",
      "Epoch [404/5000], Avg. Train Sample Loss: 0.0067, Avg. Validate Sample Loss: 0.0105,                             L2 Loss: 0.0121\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [405/5000] -------------------- \n",
      "Epoch [405/5000], Step [1/47], Loss: 0.0094\n",
      "Epoch [405/5000], Step [9/47], Loss: 0.0051\n",
      "Epoch [405/5000], Step [18/47], Loss: 0.0029\n",
      "Epoch [405/5000], Step [27/47], Loss: 0.0056\n",
      "Epoch [405/5000], Step [36/47], Loss: 0.0145\n",
      "Epoch [405/5000], Step [45/47], Loss: 0.0069\n",
      "Epoch [405/5000], Avg. Train Sample Loss: 0.0065, Avg. Validate Sample Loss: 0.0158,                             L2 Loss: 0.0159\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [406/5000] -------------------- \n",
      "Epoch [406/5000], Step [1/47], Loss: 0.0111\n",
      "Epoch [406/5000], Step [9/47], Loss: 0.0301\n",
      "Epoch [406/5000], Step [18/47], Loss: 0.0228\n",
      "Epoch [406/5000], Step [27/47], Loss: 0.0451\n",
      "Epoch [406/5000], Step [36/47], Loss: 0.0053\n",
      "Epoch [406/5000], Step [45/47], Loss: 0.0078\n",
      "Epoch [406/5000], Avg. Train Sample Loss: 0.0145, Avg. Validate Sample Loss: 0.0051,                             L2 Loss: 0.0064\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [407/5000] -------------------- \n",
      "Epoch [407/5000], Step [1/47], Loss: 0.0053\n",
      "Epoch [407/5000], Step [9/47], Loss: 0.0065\n",
      "Epoch [407/5000], Step [18/47], Loss: 0.0035\n",
      "Epoch [407/5000], Step [27/47], Loss: 0.0041\n",
      "Epoch [407/5000], Step [36/47], Loss: 0.0043\n",
      "Epoch [407/5000], Step [45/47], Loss: 0.0033\n",
      "Epoch [407/5000], Avg. Train Sample Loss: 0.0061, Avg. Validate Sample Loss: 0.0049,                             L2 Loss: 0.0050\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [408/5000] -------------------- \n",
      "Epoch [408/5000], Step [1/47], Loss: 0.0057\n",
      "Epoch [408/5000], Step [9/47], Loss: 0.0058\n",
      "Epoch [408/5000], Step [18/47], Loss: 0.0056\n",
      "Epoch [408/5000], Step [27/47], Loss: 0.0030\n",
      "Epoch [408/5000], Step [36/47], Loss: 0.0051\n",
      "Epoch [408/5000], Step [45/47], Loss: 0.0044\n",
      "Epoch [408/5000], Avg. Train Sample Loss: 0.0063, Avg. Validate Sample Loss: 0.0040,                             L2 Loss: 0.0052\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [409/5000] -------------------- \n",
      "Epoch [409/5000], Step [1/47], Loss: 0.0050\n",
      "Epoch [409/5000], Step [9/47], Loss: 0.0676\n",
      "Epoch [409/5000], Step [18/47], Loss: 0.0426\n",
      "Epoch [409/5000], Step [27/47], Loss: 0.0131\n",
      "Epoch [409/5000], Step [36/47], Loss: 0.0075\n",
      "Epoch [409/5000], Step [45/47], Loss: 0.0046\n",
      "Epoch [409/5000], Avg. Train Sample Loss: 0.0169, Avg. Validate Sample Loss: 0.0050,                             L2 Loss: 0.0064\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [410/5000] -------------------- \n",
      "Epoch [410/5000], Step [1/47], Loss: 0.0062\n",
      "Epoch [410/5000], Step [9/47], Loss: 0.0062\n",
      "Epoch [410/5000], Step [18/47], Loss: 0.0032\n",
      "Epoch [410/5000], Step [27/47], Loss: 0.0056\n",
      "Epoch [410/5000], Step [36/47], Loss: 0.0064\n",
      "Epoch [410/5000], Step [45/47], Loss: 0.0051\n",
      "Epoch [410/5000], Avg. Train Sample Loss: 0.0048, Avg. Validate Sample Loss: 0.0042,                             L2 Loss: 0.0044\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [411/5000] -------------------- \n",
      "Epoch [411/5000], Step [1/47], Loss: 0.0039\n",
      "Epoch [411/5000], Step [9/47], Loss: 0.0062\n",
      "Epoch [411/5000], Step [18/47], Loss: 0.0058\n",
      "Epoch [411/5000], Step [27/47], Loss: 0.0037\n",
      "Epoch [411/5000], Step [36/47], Loss: 0.0028\n",
      "Epoch [411/5000], Step [45/47], Loss: 0.0025\n",
      "Epoch [411/5000], Avg. Train Sample Loss: 0.0050, Avg. Validate Sample Loss: 0.0041,                             L2 Loss: 0.0052\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [412/5000] -------------------- \n",
      "Epoch [412/5000], Step [1/47], Loss: 0.0038\n",
      "Epoch [412/5000], Step [9/47], Loss: 0.0103\n",
      "Epoch [412/5000], Step [18/47], Loss: 0.0064\n",
      "Epoch [412/5000], Step [27/47], Loss: 0.0049\n",
      "Epoch [412/5000], Step [36/47], Loss: 0.0085\n",
      "Epoch [412/5000], Step [45/47], Loss: 0.0050\n",
      "Epoch [412/5000], Avg. Train Sample Loss: 0.0062, Avg. Validate Sample Loss: 0.0041,                             L2 Loss: 0.0017\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [413/5000] -------------------- \n",
      "Epoch [413/5000], Step [1/47], Loss: 0.0030\n",
      "Epoch [413/5000], Step [9/47], Loss: 0.0046\n",
      "Epoch [413/5000], Step [18/47], Loss: 0.0068\n",
      "Epoch [413/5000], Step [27/47], Loss: 0.0036\n",
      "Epoch [413/5000], Step [36/47], Loss: 0.0048\n",
      "Epoch [413/5000], Step [45/47], Loss: 0.0106\n",
      "Epoch [413/5000], Avg. Train Sample Loss: 0.0072, Avg. Validate Sample Loss: 0.0129,                             L2 Loss: 0.0148\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [414/5000] -------------------- \n",
      "Epoch [414/5000], Step [1/47], Loss: 0.0140\n",
      "Epoch [414/5000], Step [9/47], Loss: 0.0295\n",
      "Epoch [414/5000], Step [18/47], Loss: 0.0188\n",
      "Epoch [414/5000], Step [27/47], Loss: 0.0466\n",
      "Epoch [414/5000], Step [36/47], Loss: 0.0357\n",
      "Epoch [414/5000], Step [45/47], Loss: 0.0141\n",
      "Epoch [414/5000], Avg. Train Sample Loss: 0.0167, Avg. Validate Sample Loss: 0.0128,                             L2 Loss: 0.0139\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [415/5000] -------------------- \n",
      "Epoch [415/5000], Step [1/47], Loss: 0.0106\n",
      "Epoch [415/5000], Step [9/47], Loss: 0.0059\n",
      "Epoch [415/5000], Step [18/47], Loss: 0.0079\n",
      "Epoch [415/5000], Step [27/47], Loss: 0.0047\n",
      "Epoch [415/5000], Step [36/47], Loss: 0.0017\n",
      "Epoch [415/5000], Step [45/47], Loss: 0.0081\n",
      "Epoch [415/5000], Avg. Train Sample Loss: 0.0059, Avg. Validate Sample Loss: 0.0043,                             L2 Loss: 0.0040\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [416/5000] -------------------- \n",
      "Epoch [416/5000], Step [1/47], Loss: 0.0040\n",
      "Epoch [416/5000], Step [9/47], Loss: 0.0061\n",
      "Epoch [416/5000], Step [18/47], Loss: 0.0034\n",
      "Epoch [416/5000], Step [27/47], Loss: 0.0060\n",
      "Epoch [416/5000], Step [36/47], Loss: 0.0159\n",
      "Epoch [416/5000], Step [45/47], Loss: 0.0087\n",
      "Epoch [416/5000], Avg. Train Sample Loss: 0.0067, Avg. Validate Sample Loss: 0.0102,                             L2 Loss: 0.0115\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [417/5000] -------------------- \n",
      "Epoch [417/5000], Step [1/47], Loss: 0.0127\n",
      "Epoch [417/5000], Step [9/47], Loss: 0.0113\n",
      "Epoch [417/5000], Step [18/47], Loss: 0.0051\n",
      "Epoch [417/5000], Step [27/47], Loss: 0.0046\n",
      "Epoch [417/5000], Step [36/47], Loss: 0.0053\n",
      "Epoch [417/5000], Step [45/47], Loss: 0.0041\n",
      "Epoch [417/5000], Avg. Train Sample Loss: 0.0083, Avg. Validate Sample Loss: 0.0039,                             L2 Loss: 0.0031\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [418/5000] -------------------- \n",
      "Epoch [418/5000], Step [1/47], Loss: 0.0042\n",
      "Epoch [418/5000], Step [9/47], Loss: 0.0058\n",
      "Epoch [418/5000], Step [18/47], Loss: 0.0060\n",
      "Epoch [418/5000], Step [27/47], Loss: 0.0108\n",
      "Epoch [418/5000], Step [36/47], Loss: 0.0043\n",
      "Epoch [418/5000], Step [45/47], Loss: 0.0133\n",
      "Epoch [418/5000], Avg. Train Sample Loss: 0.0063, Avg. Validate Sample Loss: 0.0081,                             L2 Loss: 0.0105\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [419/5000] -------------------- \n",
      "Epoch [419/5000], Step [1/47], Loss: 0.0053\n",
      "Epoch [419/5000], Step [9/47], Loss: 0.0083\n",
      "Epoch [419/5000], Step [18/47], Loss: 0.0085\n",
      "Epoch [419/5000], Step [27/47], Loss: 0.0163\n",
      "Epoch [419/5000], Step [36/47], Loss: 0.0041\n",
      "Epoch [419/5000], Step [45/47], Loss: 0.0041\n",
      "Epoch [419/5000], Avg. Train Sample Loss: 0.0077, Avg. Validate Sample Loss: 0.0044,                             L2 Loss: 0.0046\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [420/5000] -------------------- \n",
      "Epoch [420/5000], Step [1/47], Loss: 0.0070\n",
      "Epoch [420/5000], Step [9/47], Loss: 0.0114\n",
      "Epoch [420/5000], Step [18/47], Loss: 0.0049\n",
      "Epoch [420/5000], Step [27/47], Loss: 0.0057\n",
      "Epoch [420/5000], Step [36/47], Loss: 0.0035\n",
      "Epoch [420/5000], Step [45/47], Loss: 0.0057\n",
      "Epoch [420/5000], Avg. Train Sample Loss: 0.0055, Avg. Validate Sample Loss: 0.0085,                             L2 Loss: 0.0101\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [421/5000] -------------------- \n",
      "Epoch [421/5000], Step [1/47], Loss: 0.0097\n",
      "Epoch [421/5000], Step [9/47], Loss: 0.0031\n",
      "Epoch [421/5000], Step [18/47], Loss: 0.0079\n",
      "Epoch [421/5000], Step [27/47], Loss: 0.0041\n",
      "Epoch [421/5000], Step [36/47], Loss: 0.0055\n",
      "Epoch [421/5000], Step [45/47], Loss: 0.0325\n",
      "Epoch [421/5000], Avg. Train Sample Loss: 0.0081, Avg. Validate Sample Loss: 0.0140,                             L2 Loss: 0.0152\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [422/5000] -------------------- \n",
      "Epoch [422/5000], Step [1/47], Loss: 0.0166\n",
      "Epoch [422/5000], Step [9/47], Loss: 0.0048\n",
      "Epoch [422/5000], Step [18/47], Loss: 0.0110\n",
      "Epoch [422/5000], Step [27/47], Loss: 0.0082\n",
      "Epoch [422/5000], Step [36/47], Loss: 0.0381\n",
      "Epoch [422/5000], Step [45/47], Loss: 0.0296\n",
      "Epoch [422/5000], Avg. Train Sample Loss: 0.0157, Avg. Validate Sample Loss: 0.0241,                             L2 Loss: 0.0221\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [423/5000] -------------------- \n",
      "Epoch [423/5000], Step [1/47], Loss: 0.0228\n",
      "Epoch [423/5000], Step [9/47], Loss: 0.0829\n",
      "Epoch [423/5000], Step [18/47], Loss: 0.1162\n",
      "Epoch [423/5000], Step [27/47], Loss: 0.0135\n",
      "Epoch [423/5000], Step [36/47], Loss: 0.0225\n",
      "Epoch [423/5000], Step [45/47], Loss: 0.0127\n",
      "Epoch [423/5000], Avg. Train Sample Loss: 0.0318, Avg. Validate Sample Loss: 0.0092,                             L2 Loss: 0.0111\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [424/5000] -------------------- \n",
      "Epoch [424/5000], Step [1/47], Loss: 0.0089\n",
      "Epoch [424/5000], Step [9/47], Loss: 0.0074\n",
      "Epoch [424/5000], Step [18/47], Loss: 0.0045\n",
      "Epoch [424/5000], Step [27/47], Loss: 0.0039\n",
      "Epoch [424/5000], Step [36/47], Loss: 0.0078\n",
      "Epoch [424/5000], Step [45/47], Loss: 0.0034\n",
      "Epoch [424/5000], Avg. Train Sample Loss: 0.0050, Avg. Validate Sample Loss: 0.0048,                             L2 Loss: 0.0024\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [425/5000] -------------------- \n",
      "Epoch [425/5000], Step [1/47], Loss: 0.0027\n",
      "Epoch [425/5000], Step [9/47], Loss: 0.0056\n",
      "Epoch [425/5000], Step [18/47], Loss: 0.0109\n",
      "Epoch [425/5000], Step [27/47], Loss: 0.0051\n",
      "Epoch [425/5000], Step [36/47], Loss: 0.0036\n",
      "Epoch [425/5000], Step [45/47], Loss: 0.0059\n",
      "Epoch [425/5000], Avg. Train Sample Loss: 0.0046, Avg. Validate Sample Loss: 0.0046,                             L2 Loss: 0.0021\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [426/5000] -------------------- \n",
      "Epoch [426/5000], Step [1/47], Loss: 0.0025\n",
      "Epoch [426/5000], Step [9/47], Loss: 0.0059\n",
      "Epoch [426/5000], Step [18/47], Loss: 0.0032\n",
      "Epoch [426/5000], Step [27/47], Loss: 0.0090\n",
      "Epoch [426/5000], Step [36/47], Loss: 0.0034\n",
      "Epoch [426/5000], Step [45/47], Loss: 0.0045\n",
      "Epoch [426/5000], Avg. Train Sample Loss: 0.0047, Avg. Validate Sample Loss: 0.0091,                             L2 Loss: 0.0112\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [427/5000] -------------------- \n",
      "Epoch [427/5000], Step [1/47], Loss: 0.0072\n",
      "Epoch [427/5000], Step [9/47], Loss: 0.0068\n",
      "Epoch [427/5000], Step [18/47], Loss: 0.0043\n",
      "Epoch [427/5000], Step [27/47], Loss: 0.0091\n",
      "Epoch [427/5000], Step [36/47], Loss: 0.0023\n",
      "Epoch [427/5000], Step [45/47], Loss: 0.0035\n",
      "Epoch [427/5000], Avg. Train Sample Loss: 0.0055, Avg. Validate Sample Loss: 0.0038,                             L2 Loss: 0.0038\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [428/5000] -------------------- \n",
      "Epoch [428/5000], Step [1/47], Loss: 0.0040\n",
      "Epoch [428/5000], Step [9/47], Loss: 0.0106\n",
      "Epoch [428/5000], Step [18/47], Loss: 0.0026\n",
      "Epoch [428/5000], Step [27/47], Loss: 0.0044\n",
      "Epoch [428/5000], Step [36/47], Loss: 0.0048\n",
      "Epoch [428/5000], Step [45/47], Loss: 0.0045\n",
      "Epoch [428/5000], Avg. Train Sample Loss: 0.0053, Avg. Validate Sample Loss: 0.0040,                             L2 Loss: 0.0033\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [429/5000] -------------------- \n",
      "Epoch [429/5000], Step [1/47], Loss: 0.0033\n",
      "Epoch [429/5000], Step [9/47], Loss: 0.0041\n",
      "Epoch [429/5000], Step [18/47], Loss: 0.0063\n",
      "Epoch [429/5000], Step [27/47], Loss: 0.0025\n",
      "Epoch [429/5000], Step [36/47], Loss: 0.0102\n",
      "Epoch [429/5000], Step [45/47], Loss: 0.0038\n",
      "Epoch [429/5000], Avg. Train Sample Loss: 0.0058, Avg. Validate Sample Loss: 0.0074,                             L2 Loss: 0.0089\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [430/5000] -------------------- \n",
      "Epoch [430/5000], Step [1/47], Loss: 0.0075\n",
      "Epoch [430/5000], Step [9/47], Loss: 0.0043\n",
      "Epoch [430/5000], Step [18/47], Loss: 0.0051\n",
      "Epoch [430/5000], Step [27/47], Loss: 0.0031\n",
      "Epoch [430/5000], Step [36/47], Loss: 0.0067\n",
      "Epoch [430/5000], Step [45/47], Loss: 0.0033\n",
      "Epoch [430/5000], Avg. Train Sample Loss: 0.0050, Avg. Validate Sample Loss: 0.0039,                             L2 Loss: 0.0050\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [431/5000] -------------------- \n",
      "Epoch [431/5000], Step [1/47], Loss: 0.0049\n",
      "Epoch [431/5000], Step [9/47], Loss: 0.0083\n",
      "Epoch [431/5000], Step [18/47], Loss: 0.0034\n",
      "Epoch [431/5000], Step [27/47], Loss: 0.0038\n",
      "Epoch [431/5000], Step [36/47], Loss: 0.0152\n",
      "Epoch [431/5000], Step [45/47], Loss: 0.0151\n",
      "Epoch [431/5000], Avg. Train Sample Loss: 0.0069, Avg. Validate Sample Loss: 0.0048,                             L2 Loss: 0.0059\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [432/5000] -------------------- \n",
      "Epoch [432/5000], Step [1/47], Loss: 0.0037\n",
      "Epoch [432/5000], Step [9/47], Loss: 0.0054\n",
      "Epoch [432/5000], Step [18/47], Loss: 0.0028\n",
      "Epoch [432/5000], Step [27/47], Loss: 0.0118\n",
      "Epoch [432/5000], Step [36/47], Loss: 0.0085\n",
      "Epoch [432/5000], Step [45/47], Loss: 0.0052\n",
      "Epoch [432/5000], Avg. Train Sample Loss: 0.0068, Avg. Validate Sample Loss: 0.0056,                             L2 Loss: 0.0083\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [433/5000] -------------------- \n",
      "Epoch [433/5000], Step [1/47], Loss: 0.0064\n",
      "Epoch [433/5000], Step [9/47], Loss: 0.0040\n",
      "Epoch [433/5000], Step [18/47], Loss: 0.0031\n",
      "Epoch [433/5000], Step [27/47], Loss: 0.0034\n",
      "Epoch [433/5000], Step [36/47], Loss: 0.0036\n",
      "Epoch [433/5000], Step [45/47], Loss: 0.0097\n",
      "Epoch [433/5000], Avg. Train Sample Loss: 0.0055, Avg. Validate Sample Loss: 0.0117,                             L2 Loss: 0.0139\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [434/5000] -------------------- \n",
      "Epoch [434/5000], Step [1/47], Loss: 0.0108\n",
      "Epoch [434/5000], Step [9/47], Loss: 0.0056\n",
      "Epoch [434/5000], Step [18/47], Loss: 0.0033\n",
      "Epoch [434/5000], Step [27/47], Loss: 0.0041\n",
      "Epoch [434/5000], Step [36/47], Loss: 0.0072\n",
      "Epoch [434/5000], Step [45/47], Loss: 0.0038\n",
      "Epoch [434/5000], Avg. Train Sample Loss: 0.0064, Avg. Validate Sample Loss: 0.0059,                             L2 Loss: 0.0095\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [435/5000] -------------------- \n",
      "Epoch [435/5000], Step [1/47], Loss: 0.0068\n",
      "Epoch [435/5000], Step [9/47], Loss: 0.0068\n",
      "Epoch [435/5000], Step [18/47], Loss: 0.0060\n",
      "Epoch [435/5000], Step [27/47], Loss: 0.0200\n",
      "Epoch [435/5000], Step [36/47], Loss: 0.0129\n",
      "Epoch [435/5000], Step [45/47], Loss: 0.0030\n",
      "Epoch [435/5000], Avg. Train Sample Loss: 0.0107, Avg. Validate Sample Loss: 0.0038,                             L2 Loss: 0.0049\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [436/5000] -------------------- \n",
      "Epoch [436/5000], Step [1/47], Loss: 0.0049\n",
      "Epoch [436/5000], Step [9/47], Loss: 0.0058\n",
      "Epoch [436/5000], Step [18/47], Loss: 0.0032\n",
      "Epoch [436/5000], Step [27/47], Loss: 0.0119\n",
      "Epoch [436/5000], Step [36/47], Loss: 0.0118\n",
      "Epoch [436/5000], Step [45/47], Loss: 0.0028\n",
      "Epoch [436/5000], Avg. Train Sample Loss: 0.0072, Avg. Validate Sample Loss: 0.0037,                             L2 Loss: 0.0031\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [437/5000] -------------------- \n",
      "Epoch [437/5000], Step [1/47], Loss: 0.0029\n",
      "Epoch [437/5000], Step [9/47], Loss: 0.0065\n",
      "Epoch [437/5000], Step [18/47], Loss: 0.0035\n",
      "Epoch [437/5000], Step [27/47], Loss: 0.0047\n",
      "Epoch [437/5000], Step [36/47], Loss: 0.0050\n",
      "Epoch [437/5000], Step [45/47], Loss: 0.0113\n",
      "Epoch [437/5000], Avg. Train Sample Loss: 0.0096, Avg. Validate Sample Loss: 0.0035,                             L2 Loss: 0.0025\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [438/5000] -------------------- \n",
      "Epoch [438/5000], Step [1/47], Loss: 0.0027\n",
      "Epoch [438/5000], Step [9/47], Loss: 0.0042\n",
      "Epoch [438/5000], Step [18/47], Loss: 0.0038\n",
      "Epoch [438/5000], Step [27/47], Loss: 0.0166\n",
      "Epoch [438/5000], Step [36/47], Loss: 0.0111\n",
      "Epoch [438/5000], Step [45/47], Loss: 0.0043\n",
      "Epoch [438/5000], Avg. Train Sample Loss: 0.0205, Avg. Validate Sample Loss: 0.0074,                             L2 Loss: 0.0127\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [439/5000] -------------------- \n",
      "Epoch [439/5000], Step [1/47], Loss: 0.0103\n",
      "Epoch [439/5000], Step [9/47], Loss: 0.0035\n",
      "Epoch [439/5000], Step [18/47], Loss: 0.0034\n",
      "Epoch [439/5000], Step [27/47], Loss: 0.0073\n",
      "Epoch [439/5000], Step [36/47], Loss: 0.0120\n",
      "Epoch [439/5000], Step [45/47], Loss: 0.0055\n",
      "Epoch [439/5000], Avg. Train Sample Loss: 0.0146, Avg. Validate Sample Loss: 0.0243,                             L2 Loss: 0.0223\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [440/5000] -------------------- \n",
      "Epoch [440/5000], Step [1/47], Loss: 0.0277\n",
      "Epoch [440/5000], Step [9/47], Loss: 0.0269\n",
      "Epoch [440/5000], Step [18/47], Loss: 0.0044\n",
      "Epoch [440/5000], Step [27/47], Loss: 0.0057\n",
      "Epoch [440/5000], Step [36/47], Loss: 0.0322\n",
      "Epoch [440/5000], Step [45/47], Loss: 0.0024\n",
      "Epoch [440/5000], Avg. Train Sample Loss: 0.0123, Avg. Validate Sample Loss: 0.0036,                             L2 Loss: 0.0049\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [441/5000] -------------------- \n",
      "Epoch [441/5000], Step [1/47], Loss: 0.0035\n",
      "Epoch [441/5000], Step [9/47], Loss: 0.0030\n",
      "Epoch [441/5000], Step [18/47], Loss: 0.0103\n",
      "Epoch [441/5000], Step [27/47], Loss: 0.0026\n",
      "Epoch [441/5000], Step [36/47], Loss: 0.0054\n",
      "Epoch [441/5000], Step [45/47], Loss: 0.0047\n",
      "Epoch [441/5000], Avg. Train Sample Loss: 0.0060, Avg. Validate Sample Loss: 0.0121,                             L2 Loss: 0.0151\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [442/5000] -------------------- \n",
      "Epoch [442/5000], Step [1/47], Loss: 0.0126\n",
      "Epoch [442/5000], Step [9/47], Loss: 0.0054\n",
      "Epoch [442/5000], Step [18/47], Loss: 0.0037\n",
      "Epoch [442/5000], Step [27/47], Loss: 0.0019\n",
      "Epoch [442/5000], Step [36/47], Loss: 0.0022\n",
      "Epoch [442/5000], Step [45/47], Loss: 0.0040\n",
      "Epoch [442/5000], Avg. Train Sample Loss: 0.0063, Avg. Validate Sample Loss: 0.0059,                             L2 Loss: 0.0073\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [443/5000] -------------------- \n",
      "Epoch [443/5000], Step [1/47], Loss: 0.0073\n",
      "Epoch [443/5000], Step [9/47], Loss: 0.0048\n",
      "Epoch [443/5000], Step [18/47], Loss: 0.0042\n",
      "Epoch [443/5000], Step [27/47], Loss: 0.0034\n",
      "Epoch [443/5000], Step [36/47], Loss: 0.0036\n",
      "Epoch [443/5000], Step [45/47], Loss: 0.0039\n",
      "Epoch [443/5000], Avg. Train Sample Loss: 0.0057, Avg. Validate Sample Loss: 0.0082,                             L2 Loss: 0.0108\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [444/5000] -------------------- \n",
      "Epoch [444/5000], Step [1/47], Loss: 0.0089\n",
      "Epoch [444/5000], Step [9/47], Loss: 0.0020\n",
      "Epoch [444/5000], Step [18/47], Loss: 0.0060\n",
      "Epoch [444/5000], Step [27/47], Loss: 0.0033\n",
      "Epoch [444/5000], Step [36/47], Loss: 0.0076\n",
      "Epoch [444/5000], Step [45/47], Loss: 0.0064\n",
      "Epoch [444/5000], Avg. Train Sample Loss: 0.0073, Avg. Validate Sample Loss: 0.0073,                             L2 Loss: 0.0106\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [445/5000] -------------------- \n",
      "Epoch [445/5000], Step [1/47], Loss: 0.0060\n",
      "Epoch [445/5000], Step [9/47], Loss: 0.0163\n",
      "Epoch [445/5000], Step [18/47], Loss: 0.0143\n",
      "Epoch [445/5000], Step [27/47], Loss: 0.0114\n",
      "Epoch [445/5000], Step [36/47], Loss: 0.0181\n",
      "Epoch [445/5000], Step [45/47], Loss: 0.0096\n",
      "Epoch [445/5000], Avg. Train Sample Loss: 0.0128, Avg. Validate Sample Loss: 0.0048,                             L2 Loss: 0.0059\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [446/5000] -------------------- \n",
      "Epoch [446/5000], Step [1/47], Loss: 0.0040\n",
      "Epoch [446/5000], Step [9/47], Loss: 0.0041\n",
      "Epoch [446/5000], Step [18/47], Loss: 0.0054\n",
      "Epoch [446/5000], Step [27/47], Loss: 0.0026\n",
      "Epoch [446/5000], Step [36/47], Loss: 0.0037\n",
      "Epoch [446/5000], Step [45/47], Loss: 0.0045\n",
      "Epoch [446/5000], Avg. Train Sample Loss: 0.0042, Avg. Validate Sample Loss: 0.0034,                             L2 Loss: 0.0060\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [447/5000] -------------------- \n",
      "Epoch [447/5000], Step [1/47], Loss: 0.0028\n",
      "Epoch [447/5000], Step [9/47], Loss: 0.0041\n",
      "Epoch [447/5000], Step [18/47], Loss: 0.0048\n",
      "Epoch [447/5000], Step [27/47], Loss: 0.0048\n",
      "Epoch [447/5000], Step [36/47], Loss: 0.0059\n",
      "Epoch [447/5000], Step [45/47], Loss: 0.0031\n",
      "Epoch [447/5000], Avg. Train Sample Loss: 0.0103, Avg. Validate Sample Loss: 0.0044,                             L2 Loss: 0.0040\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [448/5000] -------------------- \n",
      "Epoch [448/5000], Step [1/47], Loss: 0.0030\n",
      "Epoch [448/5000], Step [9/47], Loss: 0.0040\n",
      "Epoch [448/5000], Step [18/47], Loss: 0.0036\n",
      "Epoch [448/5000], Step [27/47], Loss: 0.0050\n",
      "Epoch [448/5000], Step [36/47], Loss: 0.0025\n",
      "Epoch [448/5000], Step [45/47], Loss: 0.0043\n",
      "Epoch [448/5000], Avg. Train Sample Loss: 0.0049, Avg. Validate Sample Loss: 0.0033,                             L2 Loss: 0.0034\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [449/5000] -------------------- \n",
      "Epoch [449/5000], Step [1/47], Loss: 0.0031\n",
      "Epoch [449/5000], Step [9/47], Loss: 0.0037\n",
      "Epoch [449/5000], Step [18/47], Loss: 0.0119\n",
      "Epoch [449/5000], Step [27/47], Loss: 0.0052\n",
      "Epoch [449/5000], Step [36/47], Loss: 0.0142\n",
      "Epoch [449/5000], Step [45/47], Loss: 0.0034\n",
      "Epoch [449/5000], Avg. Train Sample Loss: 0.0075, Avg. Validate Sample Loss: 0.0075,                             L2 Loss: 0.0106\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [450/5000] -------------------- \n",
      "Epoch [450/5000], Step [1/47], Loss: 0.0070\n",
      "Epoch [450/5000], Step [9/47], Loss: 0.0020\n",
      "Epoch [450/5000], Step [18/47], Loss: 0.0053\n",
      "Epoch [450/5000], Step [27/47], Loss: 0.0045\n",
      "Epoch [450/5000], Step [36/47], Loss: 0.0027\n",
      "Epoch [450/5000], Step [45/47], Loss: 0.0033\n",
      "Epoch [450/5000], Avg. Train Sample Loss: 0.0056, Avg. Validate Sample Loss: 0.0083,                             L2 Loss: 0.0112\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [451/5000] -------------------- \n",
      "Epoch [451/5000], Step [1/47], Loss: 0.0096\n",
      "Epoch [451/5000], Step [9/47], Loss: 0.0302\n",
      "Epoch [451/5000], Step [18/47], Loss: 0.0053\n",
      "Epoch [451/5000], Step [27/47], Loss: 0.0187\n",
      "Epoch [451/5000], Step [36/47], Loss: 0.0058\n",
      "Epoch [451/5000], Step [45/47], Loss: 0.0048\n",
      "Epoch [451/5000], Avg. Train Sample Loss: 0.0099, Avg. Validate Sample Loss: 0.0040,                             L2 Loss: 0.0059\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [452/5000] -------------------- \n",
      "Epoch [452/5000], Step [1/47], Loss: 0.0047\n",
      "Epoch [452/5000], Step [9/47], Loss: 0.0061\n",
      "Epoch [452/5000], Step [18/47], Loss: 0.0020\n",
      "Epoch [452/5000], Step [27/47], Loss: 0.0056\n",
      "Epoch [452/5000], Step [36/47], Loss: 0.0095\n",
      "Epoch [452/5000], Step [45/47], Loss: 0.0292\n",
      "Epoch [452/5000], Avg. Train Sample Loss: 0.0081, Avg. Validate Sample Loss: 0.0242,                             L2 Loss: 0.0214\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [453/5000] -------------------- \n",
      "Epoch [453/5000], Step [1/47], Loss: 0.0230\n",
      "Epoch [453/5000], Step [9/47], Loss: 0.0165\n",
      "Epoch [453/5000], Step [18/47], Loss: 0.0131\n",
      "Epoch [453/5000], Step [27/47], Loss: 0.0336\n",
      "Epoch [453/5000], Step [36/47], Loss: 0.0180\n",
      "Epoch [453/5000], Step [45/47], Loss: 0.0106\n",
      "Epoch [453/5000], Avg. Train Sample Loss: 0.0205, Avg. Validate Sample Loss: 0.0236,                             L2 Loss: 0.0215\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [454/5000] -------------------- \n",
      "Epoch [454/5000], Step [1/47], Loss: 0.0285\n",
      "Epoch [454/5000], Step [9/47], Loss: 0.0204\n",
      "Epoch [454/5000], Step [18/47], Loss: 0.0182\n",
      "Epoch [454/5000], Step [27/47], Loss: 0.0041\n",
      "Epoch [454/5000], Step [36/47], Loss: 0.0093\n",
      "Epoch [454/5000], Step [45/47], Loss: 0.0040\n",
      "Epoch [454/5000], Avg. Train Sample Loss: 0.0104, Avg. Validate Sample Loss: 0.0048,                             L2 Loss: 0.0064\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [455/5000] -------------------- \n",
      "Epoch [455/5000], Step [1/47], Loss: 0.0043\n",
      "Epoch [455/5000], Step [9/47], Loss: 0.0039\n",
      "Epoch [455/5000], Step [18/47], Loss: 0.0039\n",
      "Epoch [455/5000], Step [27/47], Loss: 0.0078\n",
      "Epoch [455/5000], Step [36/47], Loss: 0.0037\n",
      "Epoch [455/5000], Step [45/47], Loss: 0.0048\n",
      "Epoch [455/5000], Avg. Train Sample Loss: 0.0041, Avg. Validate Sample Loss: 0.0100,                             L2 Loss: 0.0130\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [456/5000] -------------------- \n",
      "Epoch [456/5000], Step [1/47], Loss: 0.0110\n",
      "Epoch [456/5000], Step [9/47], Loss: 0.0037\n",
      "Epoch [456/5000], Step [18/47], Loss: 0.0067\n",
      "Epoch [456/5000], Step [27/47], Loss: 0.0043\n",
      "Epoch [456/5000], Step [36/47], Loss: 0.0077\n",
      "Epoch [456/5000], Step [45/47], Loss: 0.0067\n",
      "Epoch [456/5000], Avg. Train Sample Loss: 0.0054, Avg. Validate Sample Loss: 0.0061,                             L2 Loss: 0.0089\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [457/5000] -------------------- \n",
      "Epoch [457/5000], Step [1/47], Loss: 0.0055\n",
      "Epoch [457/5000], Step [9/47], Loss: 0.0118\n",
      "Epoch [457/5000], Step [18/47], Loss: 0.0116\n",
      "Epoch [457/5000], Step [27/47], Loss: 0.0041\n",
      "Epoch [457/5000], Step [36/47], Loss: 0.0050\n",
      "Epoch [457/5000], Step [45/47], Loss: 0.0103\n",
      "Epoch [457/5000], Avg. Train Sample Loss: 0.0054, Avg. Validate Sample Loss: 0.0113,                             L2 Loss: 0.0133\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [458/5000] -------------------- \n",
      "Epoch [458/5000], Step [1/47], Loss: 0.0145\n",
      "Epoch [458/5000], Step [9/47], Loss: 0.0037\n",
      "Epoch [458/5000], Step [18/47], Loss: 0.0112\n",
      "Epoch [458/5000], Step [27/47], Loss: 0.0123\n",
      "Epoch [458/5000], Step [36/47], Loss: 0.0048\n",
      "Epoch [458/5000], Step [45/47], Loss: 0.0021\n",
      "Epoch [458/5000], Avg. Train Sample Loss: 0.0091, Avg. Validate Sample Loss: 0.0037,                             L2 Loss: 0.0058\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [459/5000] -------------------- \n",
      "Epoch [459/5000], Step [1/47], Loss: 0.0026\n",
      "Epoch [459/5000], Step [9/47], Loss: 0.0051\n",
      "Epoch [459/5000], Step [18/47], Loss: 0.0090\n",
      "Epoch [459/5000], Step [27/47], Loss: 0.0034\n",
      "Epoch [459/5000], Step [36/47], Loss: 0.0045\n",
      "Epoch [459/5000], Step [45/47], Loss: 0.0079\n",
      "Epoch [459/5000], Avg. Train Sample Loss: 0.0051, Avg. Validate Sample Loss: 0.0167,                             L2 Loss: 0.0179\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [460/5000] -------------------- \n",
      "Epoch [460/5000], Step [1/47], Loss: 0.0198\n",
      "Epoch [460/5000], Step [9/47], Loss: 0.0035\n",
      "Epoch [460/5000], Step [18/47], Loss: 0.0101\n",
      "Epoch [460/5000], Step [27/47], Loss: 0.0142\n",
      "Epoch [460/5000], Step [36/47], Loss: 0.0085\n",
      "Epoch [460/5000], Step [45/47], Loss: 0.0035\n",
      "Epoch [460/5000], Avg. Train Sample Loss: 0.0101, Avg. Validate Sample Loss: 0.0034,                             L2 Loss: 0.0031\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [461/5000] -------------------- \n",
      "Epoch [461/5000], Step [1/47], Loss: 0.0020\n",
      "Epoch [461/5000], Step [9/47], Loss: 0.0053\n",
      "Epoch [461/5000], Step [18/47], Loss: 0.0034\n",
      "Epoch [461/5000], Step [27/47], Loss: 0.0026\n",
      "Epoch [461/5000], Step [36/47], Loss: 0.0040\n",
      "Epoch [461/5000], Step [45/47], Loss: 0.0047\n",
      "Epoch [461/5000], Avg. Train Sample Loss: 0.0045, Avg. Validate Sample Loss: 0.0063,                             L2 Loss: 0.0093\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [462/5000] -------------------- \n",
      "Epoch [462/5000], Step [1/47], Loss: 0.0056\n",
      "Epoch [462/5000], Step [9/47], Loss: 0.0020\n",
      "Epoch [462/5000], Step [18/47], Loss: 0.0075\n",
      "Epoch [462/5000], Step [27/47], Loss: 0.0062\n",
      "Epoch [462/5000], Step [36/47], Loss: 0.0045\n",
      "Epoch [462/5000], Step [45/47], Loss: 0.0024\n",
      "Epoch [462/5000], Avg. Train Sample Loss: 0.0057, Avg. Validate Sample Loss: 0.0031,                             L2 Loss: 0.0047\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [463/5000] -------------------- \n",
      "Epoch [463/5000], Step [1/47], Loss: 0.0031\n",
      "Epoch [463/5000], Step [9/47], Loss: 0.0030\n",
      "Epoch [463/5000], Step [18/47], Loss: 0.0034\n",
      "Epoch [463/5000], Step [27/47], Loss: 0.0082\n",
      "Epoch [463/5000], Step [36/47], Loss: 0.0028\n",
      "Epoch [463/5000], Step [45/47], Loss: 0.0026\n",
      "Epoch [463/5000], Avg. Train Sample Loss: 0.0044, Avg. Validate Sample Loss: 0.0044,                             L2 Loss: 0.0056\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [464/5000] -------------------- \n",
      "Epoch [464/5000], Step [1/47], Loss: 0.0040\n",
      "Epoch [464/5000], Step [9/47], Loss: 0.0040\n",
      "Epoch [464/5000], Step [18/47], Loss: 0.0063\n",
      "Epoch [464/5000], Step [27/47], Loss: 0.0088\n",
      "Epoch [464/5000], Step [36/47], Loss: 0.0104\n",
      "Epoch [464/5000], Step [45/47], Loss: 0.0089\n",
      "Epoch [464/5000], Avg. Train Sample Loss: 0.0064, Avg. Validate Sample Loss: 0.0207,                             L2 Loss: 0.0198\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [465/5000] -------------------- \n",
      "Epoch [465/5000], Step [1/47], Loss: 0.0183\n",
      "Epoch [465/5000], Step [9/47], Loss: 0.0069\n",
      "Epoch [465/5000], Step [18/47], Loss: 0.0142\n",
      "Epoch [465/5000], Step [27/47], Loss: 0.0065\n",
      "Epoch [465/5000], Step [36/47], Loss: 0.0122\n",
      "Epoch [465/5000], Step [45/47], Loss: 0.0070\n",
      "Epoch [465/5000], Avg. Train Sample Loss: 0.0094, Avg. Validate Sample Loss: 0.0050,                             L2 Loss: 0.0047\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [466/5000] -------------------- \n",
      "Epoch [466/5000], Step [1/47], Loss: 0.0045\n",
      "Epoch [466/5000], Step [9/47], Loss: 0.0062\n",
      "Epoch [466/5000], Step [18/47], Loss: 0.0337\n",
      "Epoch [466/5000], Step [27/47], Loss: 0.0486\n",
      "Epoch [466/5000], Step [36/47], Loss: 0.0219\n",
      "Epoch [466/5000], Step [45/47], Loss: 0.0036\n",
      "Epoch [466/5000], Avg. Train Sample Loss: 0.0149, Avg. Validate Sample Loss: 0.0063,                             L2 Loss: 0.0091\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [467/5000] -------------------- \n",
      "Epoch [467/5000], Step [1/47], Loss: 0.0065\n",
      "Epoch [467/5000], Step [9/47], Loss: 0.0149\n",
      "Epoch [467/5000], Step [18/47], Loss: 0.0035\n",
      "Epoch [467/5000], Step [27/47], Loss: 0.0044\n",
      "Epoch [467/5000], Step [36/47], Loss: 0.0054\n",
      "Epoch [467/5000], Step [45/47], Loss: 0.0019\n",
      "Epoch [467/5000], Avg. Train Sample Loss: 0.0050, Avg. Validate Sample Loss: 0.0031,                             L2 Loss: 0.0023\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [468/5000] -------------------- \n",
      "Epoch [468/5000], Step [1/47], Loss: 0.0033\n",
      "Epoch [468/5000], Step [9/47], Loss: 0.0036\n",
      "Epoch [468/5000], Step [18/47], Loss: 0.0082\n",
      "Epoch [468/5000], Step [27/47], Loss: 0.0075\n",
      "Epoch [468/5000], Step [36/47], Loss: 0.0025\n",
      "Epoch [468/5000], Step [45/47], Loss: 0.0187\n",
      "Epoch [468/5000], Avg. Train Sample Loss: 0.0067, Avg. Validate Sample Loss: 0.0049,                             L2 Loss: 0.0075\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [469/5000] -------------------- \n",
      "Epoch [469/5000], Step [1/47], Loss: 0.0037\n",
      "Epoch [469/5000], Step [9/47], Loss: 0.0033\n",
      "Epoch [469/5000], Step [18/47], Loss: 0.0032\n",
      "Epoch [469/5000], Step [27/47], Loss: 0.0189\n",
      "Epoch [469/5000], Step [36/47], Loss: 0.0118\n",
      "Epoch [469/5000], Step [45/47], Loss: 0.0021\n",
      "Epoch [469/5000], Avg. Train Sample Loss: 0.0073, Avg. Validate Sample Loss: 0.0078,                             L2 Loss: 0.0105\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [470/5000] -------------------- \n",
      "Epoch [470/5000], Step [1/47], Loss: 0.0069\n",
      "Epoch [470/5000], Step [9/47], Loss: 0.0038\n",
      "Epoch [470/5000], Step [18/47], Loss: 0.0027\n",
      "Epoch [470/5000], Step [27/47], Loss: 0.0156\n",
      "Epoch [470/5000], Step [36/47], Loss: 0.0143\n",
      "Epoch [470/5000], Step [45/47], Loss: 0.0052\n",
      "Epoch [470/5000], Avg. Train Sample Loss: 0.0078, Avg. Validate Sample Loss: 0.0043,                             L2 Loss: 0.0067\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [471/5000] -------------------- \n",
      "Epoch [471/5000], Step [1/47], Loss: 0.0038\n",
      "Epoch [471/5000], Step [9/47], Loss: 0.0041\n",
      "Epoch [471/5000], Step [18/47], Loss: 0.0025\n",
      "Epoch [471/5000], Step [27/47], Loss: 0.0036\n",
      "Epoch [471/5000], Step [36/47], Loss: 0.0020\n",
      "Epoch [471/5000], Step [45/47], Loss: 0.0054\n",
      "Epoch [471/5000], Avg. Train Sample Loss: 0.0040, Avg. Validate Sample Loss: 0.0073,                             L2 Loss: 0.0105\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [472/5000] -------------------- \n",
      "Epoch [472/5000], Step [1/47], Loss: 0.0071\n",
      "Epoch [472/5000], Step [9/47], Loss: 0.0403\n",
      "Epoch [472/5000], Step [18/47], Loss: 0.0069\n",
      "Epoch [472/5000], Step [27/47], Loss: 0.0088\n",
      "Epoch [472/5000], Step [36/47], Loss: 0.0035\n",
      "Epoch [472/5000], Step [45/47], Loss: 0.0379\n",
      "Epoch [472/5000], Avg. Train Sample Loss: 0.0231, Avg. Validate Sample Loss: 0.0240,                             L2 Loss: 0.0234\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [473/5000] -------------------- \n",
      "Epoch [473/5000], Step [1/47], Loss: 0.0222\n",
      "Epoch [473/5000], Step [9/47], Loss: 0.0106\n",
      "Epoch [473/5000], Step [18/47], Loss: 0.0097\n",
      "Epoch [473/5000], Step [27/47], Loss: 0.0032\n",
      "Epoch [473/5000], Step [36/47], Loss: 0.0046\n",
      "Epoch [473/5000], Step [45/47], Loss: 0.0021\n",
      "Epoch [473/5000], Avg. Train Sample Loss: 0.0065, Avg. Validate Sample Loss: 0.0025,                             L2 Loss: 0.0036\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [474/5000] -------------------- \n",
      "Epoch [474/5000], Step [1/47], Loss: 0.0031\n",
      "Epoch [474/5000], Step [9/47], Loss: 0.0030\n",
      "Epoch [474/5000], Step [18/47], Loss: 0.0044\n",
      "Epoch [474/5000], Step [27/47], Loss: 0.0027\n",
      "Epoch [474/5000], Step [36/47], Loss: 0.0041\n",
      "Epoch [474/5000], Step [45/47], Loss: 0.0116\n",
      "Epoch [474/5000], Avg. Train Sample Loss: 0.0043, Avg. Validate Sample Loss: 0.0052,                             L2 Loss: 0.0076\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [475/5000] -------------------- \n",
      "Epoch [475/5000], Step [1/47], Loss: 0.0053\n",
      "Epoch [475/5000], Step [9/47], Loss: 0.0045\n",
      "Epoch [475/5000], Step [18/47], Loss: 0.0040\n",
      "Epoch [475/5000], Step [27/47], Loss: 0.0037\n",
      "Epoch [475/5000], Step [36/47], Loss: 0.0020\n",
      "Epoch [475/5000], Step [45/47], Loss: 0.0019\n",
      "Epoch [475/5000], Avg. Train Sample Loss: 0.0040, Avg. Validate Sample Loss: 0.0034,                             L2 Loss: 0.0048\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [476/5000] -------------------- \n",
      "Epoch [476/5000], Step [1/47], Loss: 0.0032\n",
      "Epoch [476/5000], Step [9/47], Loss: 0.0027\n",
      "Epoch [476/5000], Step [18/47], Loss: 0.0031\n",
      "Epoch [476/5000], Step [27/47], Loss: 0.0031\n",
      "Epoch [476/5000], Step [36/47], Loss: 0.0077\n",
      "Epoch [476/5000], Step [45/47], Loss: 0.0079\n",
      "Epoch [476/5000], Avg. Train Sample Loss: 0.0058, Avg. Validate Sample Loss: 0.0150,                             L2 Loss: 0.0168\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [477/5000] -------------------- \n",
      "Epoch [477/5000], Step [1/47], Loss: 0.0119\n",
      "Epoch [477/5000], Step [9/47], Loss: 0.0138\n",
      "Epoch [477/5000], Step [18/47], Loss: 0.0159\n",
      "Epoch [477/5000], Step [27/47], Loss: 0.0092\n",
      "Epoch [477/5000], Step [36/47], Loss: 0.0215\n",
      "Epoch [477/5000], Step [45/47], Loss: 0.0054\n",
      "Epoch [477/5000], Avg. Train Sample Loss: 0.0093, Avg. Validate Sample Loss: 0.0050,                             L2 Loss: 0.0075\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [478/5000] -------------------- \n",
      "Epoch [478/5000], Step [1/47], Loss: 0.0063\n",
      "Epoch [478/5000], Step [9/47], Loss: 0.0040\n",
      "Epoch [478/5000], Step [18/47], Loss: 0.0030\n",
      "Epoch [478/5000], Step [27/47], Loss: 0.0112\n",
      "Epoch [478/5000], Step [36/47], Loss: 0.0038\n",
      "Epoch [478/5000], Step [45/47], Loss: 0.0049\n",
      "Epoch [478/5000], Avg. Train Sample Loss: 0.0067, Avg. Validate Sample Loss: 0.0097,                             L2 Loss: 0.0131\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [479/5000] -------------------- \n",
      "Epoch [479/5000], Step [1/47], Loss: 0.0119\n",
      "Epoch [479/5000], Step [9/47], Loss: 0.0131\n",
      "Epoch [479/5000], Step [18/47], Loss: 0.0130\n",
      "Epoch [479/5000], Step [27/47], Loss: 0.0485\n",
      "Epoch [479/5000], Step [36/47], Loss: 0.0049\n",
      "Epoch [479/5000], Step [45/47], Loss: 0.0101\n",
      "Epoch [479/5000], Avg. Train Sample Loss: 0.0116, Avg. Validate Sample Loss: 0.0040,                             L2 Loss: 0.0071\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [480/5000] -------------------- \n",
      "Epoch [480/5000], Step [1/47], Loss: 0.0021\n",
      "Epoch [480/5000], Step [9/47], Loss: 0.0040\n",
      "Epoch [480/5000], Step [18/47], Loss: 0.0091\n",
      "Epoch [480/5000], Step [27/47], Loss: 0.0055\n",
      "Epoch [480/5000], Step [36/47], Loss: 0.0053\n",
      "Epoch [480/5000], Step [45/47], Loss: 0.0055\n",
      "Epoch [480/5000], Avg. Train Sample Loss: 0.0067, Avg. Validate Sample Loss: 0.0067,                             L2 Loss: 0.0100\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [481/5000] -------------------- \n",
      "Epoch [481/5000], Step [1/47], Loss: 0.0110\n",
      "Epoch [481/5000], Step [9/47], Loss: 0.0072\n",
      "Epoch [481/5000], Step [18/47], Loss: 0.0030\n",
      "Epoch [481/5000], Step [27/47], Loss: 0.0042\n",
      "Epoch [481/5000], Step [36/47], Loss: 0.0031\n",
      "Epoch [481/5000], Step [45/47], Loss: 0.0065\n",
      "Epoch [481/5000], Avg. Train Sample Loss: 0.0060, Avg. Validate Sample Loss: 0.0084,                             L2 Loss: 0.0120\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [482/5000] -------------------- \n",
      "Epoch [482/5000], Step [1/47], Loss: 0.0084\n",
      "Epoch [482/5000], Step [9/47], Loss: 0.0031\n",
      "Epoch [482/5000], Step [18/47], Loss: 0.0274\n",
      "Epoch [482/5000], Step [27/47], Loss: 0.0203\n",
      "Epoch [482/5000], Step [36/47], Loss: 0.0075\n",
      "Epoch [482/5000], Step [45/47], Loss: 0.0110\n",
      "Epoch [482/5000], Avg. Train Sample Loss: 0.0191, Avg. Validate Sample Loss: 0.0224,                             L2 Loss: 0.0211\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [483/5000] -------------------- \n",
      "Epoch [483/5000], Step [1/47], Loss: 0.0248\n",
      "Epoch [483/5000], Step [9/47], Loss: 0.0065\n",
      "Epoch [483/5000], Step [18/47], Loss: 0.0036\n",
      "Epoch [483/5000], Step [27/47], Loss: 0.0018\n",
      "Epoch [483/5000], Step [36/47], Loss: 0.0019\n",
      "Epoch [483/5000], Step [45/47], Loss: 0.0026\n",
      "Epoch [483/5000], Avg. Train Sample Loss: 0.0050, Avg. Validate Sample Loss: 0.0024,                             L2 Loss: 0.0030\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [484/5000] -------------------- \n",
      "Epoch [484/5000], Step [1/47], Loss: 0.0016\n",
      "Epoch [484/5000], Step [9/47], Loss: 0.0040\n",
      "Epoch [484/5000], Step [18/47], Loss: 0.0030\n",
      "Epoch [484/5000], Step [27/47], Loss: 0.0035\n",
      "Epoch [484/5000], Step [36/47], Loss: 0.0024\n",
      "Epoch [484/5000], Step [45/47], Loss: 0.0040\n",
      "Epoch [484/5000], Avg. Train Sample Loss: 0.0049, Avg. Validate Sample Loss: 0.0025,                             L2 Loss: 0.0020\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [485/5000] -------------------- \n",
      "Epoch [485/5000], Step [1/47], Loss: 0.0035\n",
      "Epoch [485/5000], Step [9/47], Loss: 0.0084\n",
      "Epoch [485/5000], Step [18/47], Loss: 0.0050\n",
      "Epoch [485/5000], Step [27/47], Loss: 0.0025\n",
      "Epoch [485/5000], Step [36/47], Loss: 0.0061\n",
      "Epoch [485/5000], Step [45/47], Loss: 0.0077\n",
      "Epoch [485/5000], Avg. Train Sample Loss: 0.0054, Avg. Validate Sample Loss: 0.0088,                             L2 Loss: 0.0115\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [486/5000] -------------------- \n",
      "Epoch [486/5000], Step [1/47], Loss: 0.0098\n",
      "Epoch [486/5000], Step [9/47], Loss: 0.0058\n",
      "Epoch [486/5000], Step [18/47], Loss: 0.0035\n",
      "Epoch [486/5000], Step [27/47], Loss: 0.0030\n",
      "Epoch [486/5000], Step [36/47], Loss: 0.0044\n",
      "Epoch [486/5000], Step [45/47], Loss: 0.0283\n",
      "Epoch [486/5000], Avg. Train Sample Loss: 0.0055, Avg. Validate Sample Loss: 0.0138,                             L2 Loss: 0.0160\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [487/5000] -------------------- \n",
      "Epoch [487/5000], Step [1/47], Loss: 0.0131\n",
      "Epoch [487/5000], Step [9/47], Loss: 0.0074\n",
      "Epoch [487/5000], Step [18/47], Loss: 0.0193\n",
      "Epoch [487/5000], Step [27/47], Loss: 0.0042\n",
      "Epoch [487/5000], Step [36/47], Loss: 0.0016\n",
      "Epoch [487/5000], Step [45/47], Loss: 0.0039\n",
      "Epoch [487/5000], Avg. Train Sample Loss: 0.0078, Avg. Validate Sample Loss: 0.0036,                             L2 Loss: 0.0058\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [488/5000] -------------------- \n",
      "Epoch [488/5000], Step [1/47], Loss: 0.0031\n",
      "Epoch [488/5000], Step [9/47], Loss: 0.0036\n",
      "Epoch [488/5000], Step [18/47], Loss: 0.0031\n",
      "Epoch [488/5000], Step [27/47], Loss: 0.0177\n",
      "Epoch [488/5000], Step [36/47], Loss: 0.0206\n",
      "Epoch [488/5000], Step [45/47], Loss: 0.0022\n",
      "Epoch [488/5000], Avg. Train Sample Loss: 0.0129, Avg. Validate Sample Loss: 0.0049,                             L2 Loss: 0.0079\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [489/5000] -------------------- \n",
      "Epoch [489/5000], Step [1/47], Loss: 0.0037\n",
      "Epoch [489/5000], Step [9/47], Loss: 0.0023\n",
      "Epoch [489/5000], Step [18/47], Loss: 0.0044\n",
      "Epoch [489/5000], Step [27/47], Loss: 0.0050\n",
      "Epoch [489/5000], Step [36/47], Loss: 0.0036\n",
      "Epoch [489/5000], Step [45/47], Loss: 0.0019\n",
      "Epoch [489/5000], Avg. Train Sample Loss: 0.0058, Avg. Validate Sample Loss: 0.0030,                             L2 Loss: 0.0050\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [490/5000] -------------------- \n",
      "Epoch [490/5000], Step [1/47], Loss: 0.0020\n",
      "Epoch [490/5000], Step [9/47], Loss: 0.0028\n",
      "Epoch [490/5000], Step [18/47], Loss: 0.0035\n",
      "Epoch [490/5000], Step [27/47], Loss: 0.0029\n",
      "Epoch [490/5000], Step [36/47], Loss: 0.0047\n",
      "Epoch [490/5000], Step [45/47], Loss: 0.0016\n",
      "Epoch [490/5000], Avg. Train Sample Loss: 0.0034, Avg. Validate Sample Loss: 0.0033,                             L2 Loss: 0.0044\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [491/5000] -------------------- \n",
      "Epoch [491/5000], Step [1/47], Loss: 0.0029\n",
      "Epoch [491/5000], Step [9/47], Loss: 0.0024\n",
      "Epoch [491/5000], Step [18/47], Loss: 0.0038\n",
      "Epoch [491/5000], Step [27/47], Loss: 0.0035\n",
      "Epoch [491/5000], Step [36/47], Loss: 0.0059\n",
      "Epoch [491/5000], Step [45/47], Loss: 0.0026\n",
      "Epoch [491/5000], Avg. Train Sample Loss: 0.0079, Avg. Validate Sample Loss: 0.0070,                             L2 Loss: 0.0102\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [492/5000] -------------------- \n",
      "Epoch [492/5000], Step [1/47], Loss: 0.0065\n",
      "Epoch [492/5000], Step [9/47], Loss: 0.0018\n",
      "Epoch [492/5000], Step [18/47], Loss: 0.0027\n",
      "Epoch [492/5000], Step [27/47], Loss: 0.0025\n",
      "Epoch [492/5000], Step [36/47], Loss: 0.0037\n",
      "Epoch [492/5000], Step [45/47], Loss: 0.0018\n",
      "Epoch [492/5000], Avg. Train Sample Loss: 0.0047, Avg. Validate Sample Loss: 0.0027,                             L2 Loss: 0.0028\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [493/5000] -------------------- \n",
      "Epoch [493/5000], Step [1/47], Loss: 0.0033\n",
      "Epoch [493/5000], Step [9/47], Loss: 0.0040\n",
      "Epoch [493/5000], Step [18/47], Loss: 0.0163\n",
      "Epoch [493/5000], Step [27/47], Loss: 0.0144\n",
      "Epoch [493/5000], Step [36/47], Loss: 0.0038\n",
      "Epoch [493/5000], Step [45/47], Loss: 0.0018\n",
      "Epoch [493/5000], Avg. Train Sample Loss: 0.0084, Avg. Validate Sample Loss: 0.0031,                             L2 Loss: 0.0028\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [494/5000] -------------------- \n",
      "Epoch [494/5000], Step [1/47], Loss: 0.0026\n",
      "Epoch [494/5000], Step [9/47], Loss: 0.0136\n",
      "Epoch [494/5000], Step [18/47], Loss: 0.0041\n",
      "Epoch [494/5000], Step [27/47], Loss: 0.0037\n",
      "Epoch [494/5000], Step [36/47], Loss: 0.0053\n",
      "Epoch [494/5000], Step [45/47], Loss: 0.0048\n",
      "Epoch [494/5000], Avg. Train Sample Loss: 0.0043, Avg. Validate Sample Loss: 0.0031,                             L2 Loss: 0.0032\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [495/5000] -------------------- \n",
      "Epoch [495/5000], Step [1/47], Loss: 0.0016\n",
      "Epoch [495/5000], Step [9/47], Loss: 0.0170\n",
      "Epoch [495/5000], Step [18/47], Loss: 0.0121\n",
      "Epoch [495/5000], Step [27/47], Loss: 0.0041\n",
      "Epoch [495/5000], Step [36/47], Loss: 0.0019\n",
      "Epoch [495/5000], Step [45/47], Loss: 0.0042\n",
      "Epoch [495/5000], Avg. Train Sample Loss: 0.0067, Avg. Validate Sample Loss: 0.0024,                             L2 Loss: 0.0032\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [496/5000] -------------------- \n",
      "Epoch [496/5000], Step [1/47], Loss: 0.0032\n",
      "Epoch [496/5000], Step [9/47], Loss: 0.0263\n",
      "Epoch [496/5000], Step [18/47], Loss: 0.0137\n",
      "Epoch [496/5000], Step [27/47], Loss: 0.0063\n",
      "Epoch [496/5000], Step [36/47], Loss: 0.0051\n",
      "Epoch [496/5000], Step [45/47], Loss: 0.0068\n",
      "Epoch [496/5000], Avg. Train Sample Loss: 0.0094, Avg. Validate Sample Loss: 0.0060,                             L2 Loss: 0.0099\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [497/5000] -------------------- \n",
      "Epoch [497/5000], Step [1/47], Loss: 0.0084\n",
      "Epoch [497/5000], Step [9/47], Loss: 0.0023\n",
      "Epoch [497/5000], Step [18/47], Loss: 0.0030\n",
      "Epoch [497/5000], Step [27/47], Loss: 0.0019\n",
      "Epoch [497/5000], Step [36/47], Loss: 0.0045\n",
      "Epoch [497/5000], Step [45/47], Loss: 0.0060\n",
      "Epoch [497/5000], Avg. Train Sample Loss: 0.0034, Avg. Validate Sample Loss: 0.0053,                             L2 Loss: 0.0082\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [498/5000] -------------------- \n",
      "Epoch [498/5000], Step [1/47], Loss: 0.0062\n",
      "Epoch [498/5000], Step [9/47], Loss: 0.0034\n",
      "Epoch [498/5000], Step [18/47], Loss: 0.0228\n",
      "Epoch [498/5000], Step [27/47], Loss: 0.0536\n",
      "Epoch [498/5000], Step [36/47], Loss: 0.0263\n",
      "Epoch [498/5000], Step [45/47], Loss: 0.0044\n",
      "Epoch [498/5000], Avg. Train Sample Loss: 0.0179, Avg. Validate Sample Loss: 0.0026,                             L2 Loss: 0.0031\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [499/5000] -------------------- \n",
      "Epoch [499/5000], Step [1/47], Loss: 0.0022\n",
      "Epoch [499/5000], Step [9/47], Loss: 0.0051\n",
      "Epoch [499/5000], Step [18/47], Loss: 0.0042\n",
      "Epoch [499/5000], Step [27/47], Loss: 0.0031\n",
      "Epoch [499/5000], Step [36/47], Loss: 0.0033\n",
      "Epoch [499/5000], Step [45/47], Loss: 0.0086\n",
      "Epoch [499/5000], Avg. Train Sample Loss: 0.0058, Avg. Validate Sample Loss: 0.0038,                             L2 Loss: 0.0059\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [500/5000] -------------------- \n",
      "Epoch [500/5000], Step [1/47], Loss: 0.0026\n",
      "Epoch [500/5000], Step [9/47], Loss: 0.0060\n",
      "Epoch [500/5000], Step [18/47], Loss: 0.0045\n",
      "Epoch [500/5000], Step [27/47], Loss: 0.0212\n",
      "Epoch [500/5000], Step [36/47], Loss: 0.0167\n",
      "Epoch [500/5000], Step [45/47], Loss: 0.0024\n",
      "Epoch [500/5000], Avg. Train Sample Loss: 0.0110, Avg. Validate Sample Loss: 0.0026,                             L2 Loss: 0.0033\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [501/5000] -------------------- \n",
      "Epoch [501/5000], Step [1/47], Loss: 0.0029\n",
      "Epoch [501/5000], Step [9/47], Loss: 0.0033\n",
      "Epoch [501/5000], Step [18/47], Loss: 0.0101\n",
      "Epoch [501/5000], Step [27/47], Loss: 0.0065\n",
      "Epoch [501/5000], Step [36/47], Loss: 0.0079\n",
      "Epoch [501/5000], Step [45/47], Loss: 0.0066\n",
      "Epoch [501/5000], Avg. Train Sample Loss: 0.0078, Avg. Validate Sample Loss: 0.0046,                             L2 Loss: 0.0074\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [502/5000] -------------------- \n",
      "Epoch [502/5000], Step [1/47], Loss: 0.0048\n",
      "Epoch [502/5000], Step [9/47], Loss: 0.0087\n",
      "Epoch [502/5000], Step [18/47], Loss: 0.0045\n",
      "Epoch [502/5000], Step [27/47], Loss: 0.0024\n",
      "Epoch [502/5000], Step [36/47], Loss: 0.0049\n",
      "Epoch [502/5000], Step [45/47], Loss: 0.0112\n",
      "Epoch [502/5000], Avg. Train Sample Loss: 0.0049, Avg. Validate Sample Loss: 0.0198,                             L2 Loss: 0.0212\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [503/5000] -------------------- \n",
      "Epoch [503/5000], Step [1/47], Loss: 0.0212\n",
      "Epoch [503/5000], Step [9/47], Loss: 0.0076\n",
      "Epoch [503/5000], Step [18/47], Loss: 0.0052\n",
      "Epoch [503/5000], Step [27/47], Loss: 0.0025\n",
      "Epoch [503/5000], Step [36/47], Loss: 0.0034\n",
      "Epoch [503/5000], Step [45/47], Loss: 0.0029\n",
      "Epoch [503/5000], Avg. Train Sample Loss: 0.0046, Avg. Validate Sample Loss: 0.0048,                             L2 Loss: 0.0085\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [504/5000] -------------------- \n",
      "Epoch [504/5000], Step [1/47], Loss: 0.0043\n",
      "Epoch [504/5000], Step [9/47], Loss: 0.0057\n",
      "Epoch [504/5000], Step [18/47], Loss: 0.0042\n",
      "Epoch [504/5000], Step [27/47], Loss: 0.0039\n",
      "Epoch [504/5000], Step [36/47], Loss: 0.0091\n",
      "Epoch [504/5000], Step [45/47], Loss: 0.0026\n",
      "Epoch [504/5000], Avg. Train Sample Loss: 0.0046, Avg. Validate Sample Loss: 0.0034,                             L2 Loss: 0.0045\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [505/5000] -------------------- \n",
      "Epoch [505/5000], Step [1/47], Loss: 0.0030\n",
      "Epoch [505/5000], Step [9/47], Loss: 0.0153\n",
      "Epoch [505/5000], Step [18/47], Loss: 0.0022\n",
      "Epoch [505/5000], Step [27/47], Loss: 0.0029\n",
      "Epoch [505/5000], Step [36/47], Loss: 0.0053\n",
      "Epoch [505/5000], Step [45/47], Loss: 0.0057\n",
      "Epoch [505/5000], Avg. Train Sample Loss: 0.0063, Avg. Validate Sample Loss: 0.0057,                             L2 Loss: 0.0093\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [506/5000] -------------------- \n",
      "Epoch [506/5000], Step [1/47], Loss: 0.0073\n",
      "Epoch [506/5000], Step [9/47], Loss: 0.0030\n",
      "Epoch [506/5000], Step [18/47], Loss: 0.0029\n",
      "Epoch [506/5000], Step [27/47], Loss: 0.0049\n",
      "Epoch [506/5000], Step [36/47], Loss: 0.0023\n",
      "Epoch [506/5000], Step [45/47], Loss: 0.0120\n",
      "Epoch [506/5000], Avg. Train Sample Loss: 0.0040, Avg. Validate Sample Loss: 0.0053,                             L2 Loss: 0.0082\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [507/5000] -------------------- \n",
      "Epoch [507/5000], Step [1/47], Loss: 0.0054\n",
      "Epoch [507/5000], Step [9/47], Loss: 0.0040\n",
      "Epoch [507/5000], Step [18/47], Loss: 0.0026\n",
      "Epoch [507/5000], Step [27/47], Loss: 0.0039\n",
      "Epoch [507/5000], Step [36/47], Loss: 0.0032\n",
      "Epoch [507/5000], Step [45/47], Loss: 0.0088\n",
      "Epoch [507/5000], Avg. Train Sample Loss: 0.0045, Avg. Validate Sample Loss: 0.0201,                             L2 Loss: 0.0198\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [508/5000] -------------------- \n",
      "Epoch [508/5000], Step [1/47], Loss: 0.0234\n",
      "Epoch [508/5000], Step [9/47], Loss: 0.0082\n",
      "Epoch [508/5000], Step [18/47], Loss: 0.0046\n",
      "Epoch [508/5000], Step [27/47], Loss: 0.0078\n",
      "Epoch [508/5000], Step [36/47], Loss: 0.0100\n",
      "Epoch [508/5000], Step [45/47], Loss: 0.0087\n",
      "Epoch [508/5000], Avg. Train Sample Loss: 0.0089, Avg. Validate Sample Loss: 0.0072,                             L2 Loss: 0.0111\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [509/5000] -------------------- \n",
      "Epoch [509/5000], Step [1/47], Loss: 0.0059\n",
      "Epoch [509/5000], Step [9/47], Loss: 0.0048\n",
      "Epoch [509/5000], Step [18/47], Loss: 0.0124\n",
      "Epoch [509/5000], Step [27/47], Loss: 0.0059\n",
      "Epoch [509/5000], Step [36/47], Loss: 0.0040\n",
      "Epoch [509/5000], Step [45/47], Loss: 0.0028\n",
      "Epoch [509/5000], Avg. Train Sample Loss: 0.0065, Avg. Validate Sample Loss: 0.0030,                             L2 Loss: 0.0047\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [510/5000] -------------------- \n",
      "Epoch [510/5000], Step [1/47], Loss: 0.0038\n",
      "Epoch [510/5000], Step [9/47], Loss: 0.0059\n",
      "Epoch [510/5000], Step [18/47], Loss: 0.0048\n",
      "Epoch [510/5000], Step [27/47], Loss: 0.0020\n",
      "Epoch [510/5000], Step [36/47], Loss: 0.0019\n",
      "Epoch [510/5000], Step [45/47], Loss: 0.0035\n",
      "Epoch [510/5000], Avg. Train Sample Loss: 0.0037, Avg. Validate Sample Loss: 0.0023,                             L2 Loss: 0.0033\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [511/5000] -------------------- \n",
      "Epoch [511/5000], Step [1/47], Loss: 0.0016\n",
      "Epoch [511/5000], Step [9/47], Loss: 0.0059\n",
      "Epoch [511/5000], Step [18/47], Loss: 0.0034\n",
      "Epoch [511/5000], Step [27/47], Loss: 0.0041\n",
      "Epoch [511/5000], Step [36/47], Loss: 0.0036\n",
      "Epoch [511/5000], Step [45/47], Loss: 0.0307\n",
      "Epoch [511/5000], Avg. Train Sample Loss: 0.0069, Avg. Validate Sample Loss: 0.0326,                             L2 Loss: 0.0264\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [512/5000] -------------------- \n",
      "Epoch [512/5000], Step [1/47], Loss: 0.0401\n",
      "Epoch [512/5000], Step [9/47], Loss: 0.0030\n",
      "Epoch [512/5000], Step [18/47], Loss: 0.0325\n",
      "Epoch [512/5000], Step [27/47], Loss: 0.0705\n",
      "Epoch [512/5000], Step [36/47], Loss: 0.0106\n",
      "Epoch [512/5000], Step [45/47], Loss: 0.0106\n",
      "Epoch [512/5000], Avg. Train Sample Loss: 0.0241, Avg. Validate Sample Loss: 0.0187,                             L2 Loss: 0.0208\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [513/5000] -------------------- \n",
      "Epoch [513/5000], Step [1/47], Loss: 0.0226\n",
      "Epoch [513/5000], Step [9/47], Loss: 0.0400\n",
      "Epoch [513/5000], Step [18/47], Loss: 0.0111\n",
      "Epoch [513/5000], Step [27/47], Loss: 0.0293\n",
      "Epoch [513/5000], Step [36/47], Loss: 0.0027\n",
      "Epoch [513/5000], Step [45/47], Loss: 0.0068\n",
      "Epoch [513/5000], Avg. Train Sample Loss: 0.0143, Avg. Validate Sample Loss: 0.0025,                             L2 Loss: 0.0026\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [514/5000] -------------------- \n",
      "Epoch [514/5000], Step [1/47], Loss: 0.0030\n",
      "Epoch [514/5000], Step [9/47], Loss: 0.0043\n",
      "Epoch [514/5000], Step [18/47], Loss: 0.0043\n",
      "Epoch [514/5000], Step [27/47], Loss: 0.0030\n",
      "Epoch [514/5000], Step [36/47], Loss: 0.0029\n",
      "Epoch [514/5000], Step [45/47], Loss: 0.0048\n",
      "Epoch [514/5000], Avg. Train Sample Loss: 0.0039, Avg. Validate Sample Loss: 0.0045,                             L2 Loss: 0.0075\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [515/5000] -------------------- \n",
      "Epoch [515/5000], Step [1/47], Loss: 0.0048\n",
      "Epoch [515/5000], Step [9/47], Loss: 0.0024\n",
      "Epoch [515/5000], Step [18/47], Loss: 0.0020\n",
      "Epoch [515/5000], Step [27/47], Loss: 0.0021\n",
      "Epoch [515/5000], Step [36/47], Loss: 0.0041\n",
      "Epoch [515/5000], Step [45/47], Loss: 0.0096\n",
      "Epoch [515/5000], Avg. Train Sample Loss: 0.0044, Avg. Validate Sample Loss: 0.0040,                             L2 Loss: 0.0068\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [516/5000] -------------------- \n",
      "Epoch [516/5000], Step [1/47], Loss: 0.0040\n",
      "Epoch [516/5000], Step [9/47], Loss: 0.0030\n",
      "Epoch [516/5000], Step [18/47], Loss: 0.0023\n",
      "Epoch [516/5000], Step [27/47], Loss: 0.0019\n",
      "Epoch [516/5000], Step [36/47], Loss: 0.0020\n",
      "Epoch [516/5000], Step [45/47], Loss: 0.0058\n",
      "Epoch [516/5000], Avg. Train Sample Loss: 0.0035, Avg. Validate Sample Loss: 0.0046,                             L2 Loss: 0.0075\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [517/5000] -------------------- \n",
      "Epoch [517/5000], Step [1/47], Loss: 0.0047\n",
      "Epoch [517/5000], Step [9/47], Loss: 0.0029\n",
      "Epoch [517/5000], Step [18/47], Loss: 0.0428\n",
      "Epoch [517/5000], Step [27/47], Loss: 0.0245\n",
      "Epoch [517/5000], Step [36/47], Loss: 0.0073\n",
      "Epoch [517/5000], Step [45/47], Loss: 0.0035\n",
      "Epoch [517/5000], Avg. Train Sample Loss: 0.0123, Avg. Validate Sample Loss: 0.0025,                             L2 Loss: 0.0020\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [518/5000] -------------------- \n",
      "Epoch [518/5000], Step [1/47], Loss: 0.0016\n",
      "Epoch [518/5000], Step [9/47], Loss: 0.0028\n",
      "Epoch [518/5000], Step [18/47], Loss: 0.0037\n",
      "Epoch [518/5000], Step [27/47], Loss: 0.0069\n",
      "Epoch [518/5000], Step [36/47], Loss: 0.0028\n",
      "Epoch [518/5000], Step [45/47], Loss: 0.0030\n",
      "Epoch [518/5000], Avg. Train Sample Loss: 0.0043, Avg. Validate Sample Loss: 0.0029,                             L2 Loss: 0.0035\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [519/5000] -------------------- \n",
      "Epoch [519/5000], Step [1/47], Loss: 0.0020\n",
      "Epoch [519/5000], Step [9/47], Loss: 0.0057\n",
      "Epoch [519/5000], Step [18/47], Loss: 0.0021\n",
      "Epoch [519/5000], Step [27/47], Loss: 0.0066\n",
      "Epoch [519/5000], Step [36/47], Loss: 0.0102\n",
      "Epoch [519/5000], Step [45/47], Loss: 0.0079\n",
      "Epoch [519/5000], Avg. Train Sample Loss: 0.0044, Avg. Validate Sample Loss: 0.0042,                             L2 Loss: 0.0072\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [520/5000] -------------------- \n",
      "Epoch [520/5000], Step [1/47], Loss: 0.0037\n",
      "Epoch [520/5000], Step [9/47], Loss: 0.0063\n",
      "Epoch [520/5000], Step [18/47], Loss: 0.0043\n",
      "Epoch [520/5000], Step [27/47], Loss: 0.0013\n",
      "Epoch [520/5000], Step [36/47], Loss: 0.0055\n",
      "Epoch [520/5000], Step [45/47], Loss: 0.0058\n",
      "Epoch [520/5000], Avg. Train Sample Loss: 0.0042, Avg. Validate Sample Loss: 0.0031,                             L2 Loss: 0.0051\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [521/5000] -------------------- \n",
      "Epoch [521/5000], Step [1/47], Loss: 0.0030\n",
      "Epoch [521/5000], Step [9/47], Loss: 0.0162\n",
      "Epoch [521/5000], Step [18/47], Loss: 0.0126\n",
      "Epoch [521/5000], Step [27/47], Loss: 0.0059\n",
      "Epoch [521/5000], Step [36/47], Loss: 0.0026\n",
      "Epoch [521/5000], Step [45/47], Loss: 0.0026\n",
      "Epoch [521/5000], Avg. Train Sample Loss: 0.0051, Avg. Validate Sample Loss: 0.0024,                             L2 Loss: 0.0026\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [522/5000] -------------------- \n",
      "Epoch [522/5000], Step [1/47], Loss: 0.0019\n",
      "Epoch [522/5000], Step [9/47], Loss: 0.0021\n",
      "Epoch [522/5000], Step [18/47], Loss: 0.0022\n",
      "Epoch [522/5000], Step [27/47], Loss: 0.0015\n",
      "Epoch [522/5000], Step [36/47], Loss: 0.0028\n",
      "Epoch [522/5000], Step [45/47], Loss: 0.0036\n",
      "Epoch [522/5000], Avg. Train Sample Loss: 0.0027, Avg. Validate Sample Loss: 0.0023,                             L2 Loss: 0.0029\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [523/5000] -------------------- \n",
      "Epoch [523/5000], Step [1/47], Loss: 0.0015\n",
      "Epoch [523/5000], Step [9/47], Loss: 0.0029\n",
      "Epoch [523/5000], Step [18/47], Loss: 0.0025\n",
      "Epoch [523/5000], Step [27/47], Loss: 0.0035\n",
      "Epoch [523/5000], Step [36/47], Loss: 0.0062\n",
      "Epoch [523/5000], Step [45/47], Loss: 0.0023\n",
      "Epoch [523/5000], Avg. Train Sample Loss: 0.0031, Avg. Validate Sample Loss: 0.0024,                             L2 Loss: 0.0048\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [524/5000] -------------------- \n",
      "Epoch [524/5000], Step [1/47], Loss: 0.0022\n",
      "Epoch [524/5000], Step [9/47], Loss: 0.0026\n",
      "Epoch [524/5000], Step [18/47], Loss: 0.0044\n",
      "Epoch [524/5000], Step [27/47], Loss: 0.0018\n",
      "Epoch [524/5000], Step [36/47], Loss: 0.0031\n",
      "Epoch [524/5000], Step [45/47], Loss: 0.0093\n",
      "Epoch [524/5000], Avg. Train Sample Loss: 0.0035, Avg. Validate Sample Loss: 0.0088,                             L2 Loss: 0.0126\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [525/5000] -------------------- \n",
      "Epoch [525/5000], Step [1/47], Loss: 0.0087\n",
      "Epoch [525/5000], Step [9/47], Loss: 0.0032\n",
      "Epoch [525/5000], Step [18/47], Loss: 0.0021\n",
      "Epoch [525/5000], Step [27/47], Loss: 0.0014\n",
      "Epoch [525/5000], Step [36/47], Loss: 0.0224\n",
      "Epoch [525/5000], Step [45/47], Loss: 0.0116\n",
      "Epoch [525/5000], Avg. Train Sample Loss: 0.0081, Avg. Validate Sample Loss: 0.0099,                             L2 Loss: 0.0133\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [526/5000] -------------------- \n",
      "Epoch [526/5000], Step [1/47], Loss: 0.0108\n",
      "Epoch [526/5000], Step [9/47], Loss: 0.0571\n",
      "Epoch [526/5000], Step [18/47], Loss: 0.0179\n",
      "Epoch [526/5000], Step [27/47], Loss: 0.0047\n",
      "Epoch [526/5000], Step [36/47], Loss: 0.0052\n",
      "Epoch [526/5000], Step [45/47], Loss: 0.0047\n",
      "Epoch [526/5000], Avg. Train Sample Loss: 0.0094, Avg. Validate Sample Loss: 0.0022,                             L2 Loss: 0.0017\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [527/5000] -------------------- \n",
      "Epoch [527/5000], Step [1/47], Loss: 0.0026\n",
      "Epoch [527/5000], Step [9/47], Loss: 0.0047\n",
      "Epoch [527/5000], Step [18/47], Loss: 0.0040\n",
      "Epoch [527/5000], Step [27/47], Loss: 0.0027\n",
      "Epoch [527/5000], Step [36/47], Loss: 0.0022\n",
      "Epoch [527/5000], Step [45/47], Loss: 0.0020\n",
      "Epoch [527/5000], Avg. Train Sample Loss: 0.0060, Avg. Validate Sample Loss: 0.0031,                             L2 Loss: 0.0052\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [528/5000] -------------------- \n",
      "Epoch [528/5000], Step [1/47], Loss: 0.0035\n",
      "Epoch [528/5000], Step [9/47], Loss: 0.0135\n",
      "Epoch [528/5000], Step [18/47], Loss: 0.0161\n",
      "Epoch [528/5000], Step [27/47], Loss: 0.0035\n",
      "Epoch [528/5000], Step [36/47], Loss: 0.0044\n",
      "Epoch [528/5000], Step [45/47], Loss: 0.0047\n",
      "Epoch [528/5000], Avg. Train Sample Loss: 0.0127, Avg. Validate Sample Loss: 0.0035,                             L2 Loss: 0.0074\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [529/5000] -------------------- \n",
      "Epoch [529/5000], Step [1/47], Loss: 0.0054\n",
      "Epoch [529/5000], Step [9/47], Loss: 0.0074\n",
      "Epoch [529/5000], Step [18/47], Loss: 0.0136\n",
      "Epoch [529/5000], Step [27/47], Loss: 0.0225\n",
      "Epoch [529/5000], Step [36/47], Loss: 0.0095\n",
      "Epoch [529/5000], Step [45/47], Loss: 0.0028\n",
      "Epoch [529/5000], Avg. Train Sample Loss: 0.0086, Avg. Validate Sample Loss: 0.0034,                             L2 Loss: 0.0054\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [530/5000] -------------------- \n",
      "Epoch [530/5000], Step [1/47], Loss: 0.0030\n",
      "Epoch [530/5000], Step [9/47], Loss: 0.0042\n",
      "Epoch [530/5000], Step [18/47], Loss: 0.0137\n",
      "Epoch [530/5000], Step [27/47], Loss: 0.0041\n",
      "Epoch [530/5000], Step [36/47], Loss: 0.0036\n",
      "Epoch [530/5000], Step [45/47], Loss: 0.0015\n",
      "Epoch [530/5000], Avg. Train Sample Loss: 0.0040, Avg. Validate Sample Loss: 0.0022,                             L2 Loss: 0.0036\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [531/5000] -------------------- \n",
      "Epoch [531/5000], Step [1/47], Loss: 0.0021\n",
      "Epoch [531/5000], Step [9/47], Loss: 0.0018\n",
      "Epoch [531/5000], Step [18/47], Loss: 0.0026\n",
      "Epoch [531/5000], Step [27/47], Loss: 0.0019\n",
      "Epoch [531/5000], Step [36/47], Loss: 0.0027\n",
      "Epoch [531/5000], Step [45/47], Loss: 0.0100\n",
      "Epoch [531/5000], Avg. Train Sample Loss: 0.0032, Avg. Validate Sample Loss: 0.0083,                             L2 Loss: 0.0122\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [532/5000] -------------------- \n",
      "Epoch [532/5000], Step [1/47], Loss: 0.0093\n",
      "Epoch [532/5000], Step [9/47], Loss: 0.0029\n",
      "Epoch [532/5000], Step [18/47], Loss: 0.0026\n",
      "Epoch [532/5000], Step [27/47], Loss: 0.0088\n",
      "Epoch [532/5000], Step [36/47], Loss: 0.0015\n",
      "Epoch [532/5000], Step [45/47], Loss: 0.0023\n",
      "Epoch [532/5000], Avg. Train Sample Loss: 0.0046, Avg. Validate Sample Loss: 0.0025,                             L2 Loss: 0.0026\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [533/5000] -------------------- \n",
      "Epoch [533/5000], Step [1/47], Loss: 0.0021\n",
      "Epoch [533/5000], Step [9/47], Loss: 0.0018\n",
      "Epoch [533/5000], Step [18/47], Loss: 0.0017\n",
      "Epoch [533/5000], Step [27/47], Loss: 0.0021\n",
      "Epoch [533/5000], Step [36/47], Loss: 0.0056\n",
      "Epoch [533/5000], Step [45/47], Loss: 0.0130\n",
      "Epoch [533/5000], Avg. Train Sample Loss: 0.0055, Avg. Validate Sample Loss: 0.0094,                             L2 Loss: 0.0128\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [534/5000] -------------------- \n",
      "Epoch [534/5000], Step [1/47], Loss: 0.0112\n",
      "Epoch [534/5000], Step [9/47], Loss: 0.0020\n",
      "Epoch [534/5000], Step [18/47], Loss: 0.0095\n",
      "Epoch [534/5000], Step [27/47], Loss: 0.0046\n",
      "Epoch [534/5000], Step [36/47], Loss: 0.0047\n",
      "Epoch [534/5000], Step [45/47], Loss: 0.0023\n",
      "Epoch [534/5000], Avg. Train Sample Loss: 0.0080, Avg. Validate Sample Loss: 0.0068,                             L2 Loss: 0.0107\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [535/5000] -------------------- \n",
      "Epoch [535/5000], Step [1/47], Loss: 0.0063\n",
      "Epoch [535/5000], Step [9/47], Loss: 0.0033\n",
      "Epoch [535/5000], Step [18/47], Loss: 0.0032\n",
      "Epoch [535/5000], Step [27/47], Loss: 0.0039\n",
      "Epoch [535/5000], Step [36/47], Loss: 0.0032\n",
      "Epoch [535/5000], Step [45/47], Loss: 0.0030\n",
      "Epoch [535/5000], Avg. Train Sample Loss: 0.0032, Avg. Validate Sample Loss: 0.0022,                             L2 Loss: 0.0026\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [536/5000] -------------------- \n",
      "Epoch [536/5000], Step [1/47], Loss: 0.0025\n",
      "Epoch [536/5000], Step [9/47], Loss: 0.0037\n",
      "Epoch [536/5000], Step [18/47], Loss: 0.0153\n",
      "Epoch [536/5000], Step [27/47], Loss: 0.0394\n",
      "Epoch [536/5000], Step [36/47], Loss: 0.0251\n",
      "Epoch [536/5000], Step [45/47], Loss: 0.0089\n",
      "Epoch [536/5000], Avg. Train Sample Loss: 0.0139, Avg. Validate Sample Loss: 0.0085,                             L2 Loss: 0.0123\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [537/5000] -------------------- \n",
      "Epoch [537/5000], Step [1/47], Loss: 0.0068\n",
      "Epoch [537/5000], Step [9/47], Loss: 0.0125\n",
      "Epoch [537/5000], Step [18/47], Loss: 0.0119\n",
      "Epoch [537/5000], Step [27/47], Loss: 0.0067\n",
      "Epoch [537/5000], Step [36/47], Loss: 0.0059\n",
      "Epoch [537/5000], Step [45/47], Loss: 0.0019\n",
      "Epoch [537/5000], Avg. Train Sample Loss: 0.0085, Avg. Validate Sample Loss: 0.0047,                             L2 Loss: 0.0090\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [538/5000] -------------------- \n",
      "Epoch [538/5000], Step [1/47], Loss: 0.0045\n",
      "Epoch [538/5000], Step [9/47], Loss: 0.0038\n",
      "Epoch [538/5000], Step [18/47], Loss: 0.0029\n",
      "Epoch [538/5000], Step [27/47], Loss: 0.0029\n",
      "Epoch [538/5000], Step [36/47], Loss: 0.0016\n",
      "Epoch [538/5000], Step [45/47], Loss: 0.0021\n",
      "Epoch [538/5000], Avg. Train Sample Loss: 0.0028, Avg. Validate Sample Loss: 0.0026,                             L2 Loss: 0.0057\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [539/5000] -------------------- \n",
      "Epoch [539/5000], Step [1/47], Loss: 0.0016\n",
      "Epoch [539/5000], Step [9/47], Loss: 0.0039\n",
      "Epoch [539/5000], Step [18/47], Loss: 0.0122\n",
      "Epoch [539/5000], Step [27/47], Loss: 0.0113\n",
      "Epoch [539/5000], Step [36/47], Loss: 0.0134\n",
      "Epoch [539/5000], Step [45/47], Loss: 0.0183\n",
      "Epoch [539/5000], Avg. Train Sample Loss: 0.0079, Avg. Validate Sample Loss: 0.0122,                             L2 Loss: 0.0152\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [540/5000] -------------------- \n",
      "Epoch [540/5000], Step [1/47], Loss: 0.0145\n",
      "Epoch [540/5000], Step [9/47], Loss: 0.0122\n",
      "Epoch [540/5000], Step [18/47], Loss: 0.0014\n",
      "Epoch [540/5000], Step [27/47], Loss: 0.0158\n",
      "Epoch [540/5000], Step [36/47], Loss: 0.0116\n",
      "Epoch [540/5000], Step [45/47], Loss: 0.0039\n",
      "Epoch [540/5000], Avg. Train Sample Loss: 0.0124, Avg. Validate Sample Loss: 0.0045,                             L2 Loss: 0.0082\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [541/5000] -------------------- \n",
      "Epoch [541/5000], Step [1/47], Loss: 0.0052\n",
      "Epoch [541/5000], Step [9/47], Loss: 0.0063\n",
      "Epoch [541/5000], Step [18/47], Loss: 0.0026\n",
      "Epoch [541/5000], Step [27/47], Loss: 0.0041\n",
      "Epoch [541/5000], Step [36/47], Loss: 0.0029\n",
      "Epoch [541/5000], Step [45/47], Loss: 0.0022\n",
      "Epoch [541/5000], Avg. Train Sample Loss: 0.0035, Avg. Validate Sample Loss: 0.0029,                             L2 Loss: 0.0045\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [542/5000] -------------------- \n",
      "Epoch [542/5000], Step [1/47], Loss: 0.0026\n",
      "Epoch [542/5000], Step [9/47], Loss: 0.0014\n",
      "Epoch [542/5000], Step [18/47], Loss: 0.0039\n",
      "Epoch [542/5000], Step [27/47], Loss: 0.0095\n",
      "Epoch [542/5000], Step [36/47], Loss: 0.0043\n",
      "Epoch [542/5000], Step [45/47], Loss: 0.0019\n",
      "Epoch [542/5000], Avg. Train Sample Loss: 0.0042, Avg. Validate Sample Loss: 0.0030,                             L2 Loss: 0.0048\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [543/5000] -------------------- \n",
      "Epoch [543/5000], Step [1/47], Loss: 0.0029\n",
      "Epoch [543/5000], Step [9/47], Loss: 0.0025\n",
      "Epoch [543/5000], Step [18/47], Loss: 0.0020\n",
      "Epoch [543/5000], Step [27/47], Loss: 0.0026\n",
      "Epoch [543/5000], Step [36/47], Loss: 0.0018\n",
      "Epoch [543/5000], Step [45/47], Loss: 0.0061\n",
      "Epoch [543/5000], Avg. Train Sample Loss: 0.0028, Avg. Validate Sample Loss: 0.0112,                             L2 Loss: 0.0146\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [544/5000] -------------------- \n",
      "Epoch [544/5000], Step [1/47], Loss: 0.0072\n",
      "Epoch [544/5000], Step [9/47], Loss: 0.0103\n",
      "Epoch [544/5000], Step [18/47], Loss: 0.0066\n",
      "Epoch [544/5000], Step [27/47], Loss: 0.0048\n",
      "Epoch [544/5000], Step [36/47], Loss: 0.0028\n",
      "Epoch [544/5000], Step [45/47], Loss: 0.0091\n",
      "Epoch [544/5000], Avg. Train Sample Loss: 0.0076, Avg. Validate Sample Loss: 0.0075,                             L2 Loss: 0.0115\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [545/5000] -------------------- \n",
      "Epoch [545/5000], Step [1/47], Loss: 0.0064\n",
      "Epoch [545/5000], Step [9/47], Loss: 0.0017\n",
      "Epoch [545/5000], Step [18/47], Loss: 0.0024\n",
      "Epoch [545/5000], Step [27/47], Loss: 0.0066\n",
      "Epoch [545/5000], Step [36/47], Loss: 0.0025\n",
      "Epoch [545/5000], Step [45/47], Loss: 0.0274\n",
      "Epoch [545/5000], Avg. Train Sample Loss: 0.0067, Avg. Validate Sample Loss: 0.0176,                             L2 Loss: 0.0196\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [546/5000] -------------------- \n",
      "Epoch [546/5000], Step [1/47], Loss: 0.0246\n",
      "Epoch [546/5000], Step [9/47], Loss: 0.0060\n",
      "Epoch [546/5000], Step [18/47], Loss: 0.0025\n",
      "Epoch [546/5000], Step [27/47], Loss: 0.0045\n",
      "Epoch [546/5000], Step [36/47], Loss: 0.0084\n",
      "Epoch [546/5000], Step [45/47], Loss: 0.0020\n",
      "Epoch [546/5000], Avg. Train Sample Loss: 0.0093, Avg. Validate Sample Loss: 0.0020,                             L2 Loss: 0.0026\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [547/5000] -------------------- \n",
      "Epoch [547/5000], Step [1/47], Loss: 0.0023\n",
      "Epoch [547/5000], Step [9/47], Loss: 0.0041\n",
      "Epoch [547/5000], Step [18/47], Loss: 0.0228\n",
      "Epoch [547/5000], Step [27/47], Loss: 0.0182\n",
      "Epoch [547/5000], Step [36/47], Loss: 0.0882\n",
      "Epoch [547/5000], Step [45/47], Loss: 0.0234\n",
      "Epoch [547/5000], Avg. Train Sample Loss: 0.0162, Avg. Validate Sample Loss: 0.0275,                             L2 Loss: 0.0248\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [548/5000] -------------------- \n",
      "Epoch [548/5000], Step [1/47], Loss: 0.0329\n",
      "Epoch [548/5000], Step [9/47], Loss: 0.0175\n",
      "Epoch [548/5000], Step [18/47], Loss: 0.0144\n",
      "Epoch [548/5000], Step [27/47], Loss: 0.0030\n",
      "Epoch [548/5000], Step [36/47], Loss: 0.0030\n",
      "Epoch [548/5000], Step [45/47], Loss: 0.0084\n",
      "Epoch [548/5000], Avg. Train Sample Loss: 0.0086, Avg. Validate Sample Loss: 0.0152,                             L2 Loss: 0.0178\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [549/5000] -------------------- \n",
      "Epoch [549/5000], Step [1/47], Loss: 0.0135\n",
      "Epoch [549/5000], Step [9/47], Loss: 0.0098\n",
      "Epoch [549/5000], Step [18/47], Loss: 0.0038\n",
      "Epoch [549/5000], Step [27/47], Loss: 0.0021\n",
      "Epoch [549/5000], Step [36/47], Loss: 0.0022\n",
      "Epoch [549/5000], Step [45/47], Loss: 0.0027\n",
      "Epoch [549/5000], Avg. Train Sample Loss: 0.0040, Avg. Validate Sample Loss: 0.0017,                             L2 Loss: 0.0024\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [550/5000] -------------------- \n",
      "Epoch [550/5000], Step [1/47], Loss: 0.0015\n",
      "Epoch [550/5000], Step [9/47], Loss: 0.0022\n",
      "Epoch [550/5000], Step [18/47], Loss: 0.0025\n",
      "Epoch [550/5000], Step [27/47], Loss: 0.0024\n",
      "Epoch [550/5000], Step [36/47], Loss: 0.0025\n",
      "Epoch [550/5000], Step [45/47], Loss: 0.0139\n",
      "Epoch [550/5000], Avg. Train Sample Loss: 0.0055, Avg. Validate Sample Loss: 0.0157,                             L2 Loss: 0.0180\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [551/5000] -------------------- \n",
      "Epoch [551/5000], Step [1/47], Loss: 0.0126\n",
      "Epoch [551/5000], Step [9/47], Loss: 0.0028\n",
      "Epoch [551/5000], Step [18/47], Loss: 0.0020\n",
      "Epoch [551/5000], Step [27/47], Loss: 0.0022\n",
      "Epoch [551/5000], Step [36/47], Loss: 0.0017\n",
      "Epoch [551/5000], Step [45/47], Loss: 0.0076\n",
      "Epoch [551/5000], Avg. Train Sample Loss: 0.0038, Avg. Validate Sample Loss: 0.0070,                             L2 Loss: 0.0104\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [552/5000] -------------------- \n",
      "Epoch [552/5000], Step [1/47], Loss: 0.0065\n",
      "Epoch [552/5000], Step [9/47], Loss: 0.0039\n",
      "Epoch [552/5000], Step [18/47], Loss: 0.0080\n",
      "Epoch [552/5000], Step [27/47], Loss: 0.0381\n",
      "Epoch [552/5000], Step [36/47], Loss: 0.0032\n",
      "Epoch [552/5000], Step [45/47], Loss: 0.0028\n",
      "Epoch [552/5000], Avg. Train Sample Loss: 0.0081, Avg. Validate Sample Loss: 0.0031,                             L2 Loss: 0.0056\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [553/5000] -------------------- \n",
      "Epoch [553/5000], Step [1/47], Loss: 0.0025\n",
      "Epoch [553/5000], Step [9/47], Loss: 0.0018\n",
      "Epoch [553/5000], Step [18/47], Loss: 0.0034\n",
      "Epoch [553/5000], Step [27/47], Loss: 0.0027\n",
      "Epoch [553/5000], Step [36/47], Loss: 0.0014\n",
      "Epoch [553/5000], Step [45/47], Loss: 0.0020\n",
      "Epoch [553/5000], Avg. Train Sample Loss: 0.0035, Avg. Validate Sample Loss: 0.0027,                             L2 Loss: 0.0040\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [554/5000] -------------------- \n",
      "Epoch [554/5000], Step [1/47], Loss: 0.0029\n",
      "Epoch [554/5000], Step [9/47], Loss: 0.0021\n",
      "Epoch [554/5000], Step [18/47], Loss: 0.0028\n",
      "Epoch [554/5000], Step [27/47], Loss: 0.0022\n",
      "Epoch [554/5000], Step [36/47], Loss: 0.0024\n",
      "Epoch [554/5000], Step [45/47], Loss: 0.0022\n",
      "Epoch [554/5000], Avg. Train Sample Loss: 0.0077, Avg. Validate Sample Loss: 0.0139,                             L2 Loss: 0.0165\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [555/5000] -------------------- \n",
      "Epoch [555/5000], Step [1/47], Loss: 0.0129\n",
      "Epoch [555/5000], Step [9/47], Loss: 0.0030\n",
      "Epoch [555/5000], Step [18/47], Loss: 0.0027\n",
      "Epoch [555/5000], Step [27/47], Loss: 0.0023\n",
      "Epoch [555/5000], Step [36/47], Loss: 0.0042\n",
      "Epoch [555/5000], Step [45/47], Loss: 0.0023\n",
      "Epoch [555/5000], Avg. Train Sample Loss: 0.0051, Avg. Validate Sample Loss: 0.0022,                             L2 Loss: 0.0029\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [556/5000] -------------------- \n",
      "Epoch [556/5000], Step [1/47], Loss: 0.0022\n",
      "Epoch [556/5000], Step [9/47], Loss: 0.0023\n",
      "Epoch [556/5000], Step [18/47], Loss: 0.0060\n",
      "Epoch [556/5000], Step [27/47], Loss: 0.0026\n",
      "Epoch [556/5000], Step [36/47], Loss: 0.0029\n",
      "Epoch [556/5000], Step [45/47], Loss: 0.0016\n",
      "Epoch [556/5000], Avg. Train Sample Loss: 0.0028, Avg. Validate Sample Loss: 0.0019,                             L2 Loss: 0.0022\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [557/5000] -------------------- \n",
      "Epoch [557/5000], Step [1/47], Loss: 0.0019\n",
      "Epoch [557/5000], Step [9/47], Loss: 0.0014\n",
      "Epoch [557/5000], Step [18/47], Loss: 0.0031\n",
      "Epoch [557/5000], Step [27/47], Loss: 0.0035\n",
      "Epoch [557/5000], Step [36/47], Loss: 0.0022\n",
      "Epoch [557/5000], Step [45/47], Loss: 0.0020\n",
      "Epoch [557/5000], Avg. Train Sample Loss: 0.0032, Avg. Validate Sample Loss: 0.0106,                             L2 Loss: 0.0142\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [558/5000] -------------------- \n",
      "Epoch [558/5000], Step [1/47], Loss: 0.0120\n",
      "Epoch [558/5000], Step [9/47], Loss: 0.0013\n",
      "Epoch [558/5000], Step [18/47], Loss: 0.0038\n",
      "Epoch [558/5000], Step [27/47], Loss: 0.0037\n",
      "Epoch [558/5000], Step [36/47], Loss: 0.0034\n",
      "Epoch [558/5000], Step [45/47], Loss: 0.0030\n",
      "Epoch [558/5000], Avg. Train Sample Loss: 0.0054, Avg. Validate Sample Loss: 0.0026,                             L2 Loss: 0.0018\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [559/5000] -------------------- \n",
      "Epoch [559/5000], Step [1/47], Loss: 0.0021\n",
      "Epoch [559/5000], Step [9/47], Loss: 0.0021\n",
      "Epoch [559/5000], Step [18/47], Loss: 0.0035\n",
      "Epoch [559/5000], Step [27/47], Loss: 0.0038\n",
      "Epoch [559/5000], Step [36/47], Loss: 0.0023\n",
      "Epoch [559/5000], Step [45/47], Loss: 0.0029\n",
      "Epoch [559/5000], Avg. Train Sample Loss: 0.0044, Avg. Validate Sample Loss: 0.0030,                             L2 Loss: 0.0058\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [560/5000] -------------------- \n",
      "Epoch [560/5000], Step [1/47], Loss: 0.0027\n",
      "Epoch [560/5000], Step [9/47], Loss: 0.0012\n",
      "Epoch [560/5000], Step [18/47], Loss: 0.0035\n",
      "Epoch [560/5000], Step [27/47], Loss: 0.0078\n",
      "Epoch [560/5000], Step [36/47], Loss: 0.0015\n",
      "Epoch [560/5000], Step [45/47], Loss: 0.0033\n",
      "Epoch [560/5000], Avg. Train Sample Loss: 0.0032, Avg. Validate Sample Loss: 0.0022,                             L2 Loss: 0.0030\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [561/5000] -------------------- \n",
      "Epoch [561/5000], Step [1/47], Loss: 0.0023\n",
      "Epoch [561/5000], Step [9/47], Loss: 0.0031\n",
      "Epoch [561/5000], Step [18/47], Loss: 0.0035\n",
      "Epoch [561/5000], Step [27/47], Loss: 0.0120\n",
      "Epoch [561/5000], Step [36/47], Loss: 0.0020\n",
      "Epoch [561/5000], Step [45/47], Loss: 0.0035\n",
      "Epoch [561/5000], Avg. Train Sample Loss: 0.0037, Avg. Validate Sample Loss: 0.0030,                             L2 Loss: 0.0057\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [562/5000] -------------------- \n",
      "Epoch [562/5000], Step [1/47], Loss: 0.0028\n",
      "Epoch [562/5000], Step [9/47], Loss: 0.0018\n",
      "Epoch [562/5000], Step [18/47], Loss: 0.0053\n",
      "Epoch [562/5000], Step [27/47], Loss: 0.0025\n",
      "Epoch [562/5000], Step [36/47], Loss: 0.0012\n",
      "Epoch [562/5000], Step [45/47], Loss: 0.0016\n",
      "Epoch [562/5000], Avg. Train Sample Loss: 0.0029, Avg. Validate Sample Loss: 0.0034,                             L2 Loss: 0.0057\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [563/5000] -------------------- \n",
      "Epoch [563/5000], Step [1/47], Loss: 0.0025\n",
      "Epoch [563/5000], Step [9/47], Loss: 0.0028\n",
      "Epoch [563/5000], Step [18/47], Loss: 0.0057\n",
      "Epoch [563/5000], Step [27/47], Loss: 0.0077\n",
      "Epoch [563/5000], Step [36/47], Loss: 0.0123\n",
      "Epoch [563/5000], Step [45/47], Loss: 0.0107\n",
      "Epoch [563/5000], Avg. Train Sample Loss: 0.0098, Avg. Validate Sample Loss: 0.0025,                             L2 Loss: 0.0042\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [564/5000] -------------------- \n",
      "Epoch [564/5000], Step [1/47], Loss: 0.0024\n",
      "Epoch [564/5000], Step [9/47], Loss: 0.0081\n",
      "Epoch [564/5000], Step [18/47], Loss: 0.0026\n",
      "Epoch [564/5000], Step [27/47], Loss: 0.0015\n",
      "Epoch [564/5000], Step [36/47], Loss: 0.0024\n",
      "Epoch [564/5000], Step [45/47], Loss: 0.0023\n",
      "Epoch [564/5000], Avg. Train Sample Loss: 0.0031, Avg. Validate Sample Loss: 0.0115,                             L2 Loss: 0.0145\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [565/5000] -------------------- \n",
      "Epoch [565/5000], Step [1/47], Loss: 0.0142\n",
      "Epoch [565/5000], Step [9/47], Loss: 0.0030\n",
      "Epoch [565/5000], Step [18/47], Loss: 0.0027\n",
      "Epoch [565/5000], Step [27/47], Loss: 0.0035\n",
      "Epoch [565/5000], Step [36/47], Loss: 0.0019\n",
      "Epoch [565/5000], Step [45/47], Loss: 0.0108\n",
      "Epoch [565/5000], Avg. Train Sample Loss: 0.0044, Avg. Validate Sample Loss: 0.0042,                             L2 Loss: 0.0075\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [566/5000] -------------------- \n",
      "Epoch [566/5000], Step [1/47], Loss: 0.0040\n",
      "Epoch [566/5000], Step [9/47], Loss: 0.0191\n",
      "Epoch [566/5000], Step [18/47], Loss: 0.0085\n",
      "Epoch [566/5000], Step [27/47], Loss: 0.0026\n",
      "Epoch [566/5000], Step [36/47], Loss: 0.0036\n",
      "Epoch [566/5000], Step [45/47], Loss: 0.0031\n",
      "Epoch [566/5000], Avg. Train Sample Loss: 0.0062, Avg. Validate Sample Loss: 0.0021,                             L2 Loss: 0.0022\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [567/5000] -------------------- \n",
      "Epoch [567/5000], Step [1/47], Loss: 0.0019\n",
      "Epoch [567/5000], Step [9/47], Loss: 0.0017\n",
      "Epoch [567/5000], Step [18/47], Loss: 0.0039\n",
      "Epoch [567/5000], Step [27/47], Loss: 0.0324\n",
      "Epoch [567/5000], Step [36/47], Loss: 0.0193\n",
      "Epoch [567/5000], Step [45/47], Loss: 0.0210\n",
      "Epoch [567/5000], Avg. Train Sample Loss: 0.0144, Avg. Validate Sample Loss: 0.0496,                             L2 Loss: 0.0340\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [568/5000] -------------------- \n",
      "Epoch [568/5000], Step [1/47], Loss: 0.0589\n",
      "Epoch [568/5000], Step [9/47], Loss: 0.0520\n",
      "Epoch [568/5000], Step [18/47], Loss: 0.1014\n",
      "Epoch [568/5000], Step [27/47], Loss: 0.0068\n",
      "Epoch [568/5000], Step [36/47], Loss: 0.0034\n",
      "Epoch [568/5000], Step [45/47], Loss: 0.0031\n",
      "Epoch [568/5000], Avg. Train Sample Loss: 0.0327, Avg. Validate Sample Loss: 0.0019,                             L2 Loss: 0.0018\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [569/5000] -------------------- \n",
      "Epoch [569/5000], Step [1/47], Loss: 0.0019\n",
      "Epoch [569/5000], Step [9/47], Loss: 0.0031\n",
      "Epoch [569/5000], Step [18/47], Loss: 0.0042\n",
      "Epoch [569/5000], Step [27/47], Loss: 0.0032\n",
      "Epoch [569/5000], Step [36/47], Loss: 0.0028\n",
      "Epoch [569/5000], Step [45/47], Loss: 0.0026\n",
      "Epoch [569/5000], Avg. Train Sample Loss: 0.0028, Avg. Validate Sample Loss: 0.0061,                             L2 Loss: 0.0095\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [570/5000] -------------------- \n",
      "Epoch [570/5000], Step [1/47], Loss: 0.0050\n",
      "Epoch [570/5000], Step [9/47], Loss: 0.0110\n",
      "Epoch [570/5000], Step [18/47], Loss: 0.0024\n",
      "Epoch [570/5000], Step [27/47], Loss: 0.0019\n",
      "Epoch [570/5000], Step [36/47], Loss: 0.0095\n",
      "Epoch [570/5000], Step [45/47], Loss: 0.0027\n",
      "Epoch [570/5000], Avg. Train Sample Loss: 0.0055, Avg. Validate Sample Loss: 0.0032,                             L2 Loss: 0.0050\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [571/5000] -------------------- \n",
      "Epoch [571/5000], Step [1/47], Loss: 0.0026\n",
      "Epoch [571/5000], Step [9/47], Loss: 0.0021\n",
      "Epoch [571/5000], Step [18/47], Loss: 0.0018\n",
      "Epoch [571/5000], Step [27/47], Loss: 0.0017\n",
      "Epoch [571/5000], Step [36/47], Loss: 0.0015\n",
      "Epoch [571/5000], Step [45/47], Loss: 0.0027\n",
      "Epoch [571/5000], Avg. Train Sample Loss: 0.0025, Avg. Validate Sample Loss: 0.0028,                             L2 Loss: 0.0045\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [572/5000] -------------------- \n",
      "Epoch [572/5000], Step [1/47], Loss: 0.0019\n",
      "Epoch [572/5000], Step [9/47], Loss: 0.0028\n",
      "Epoch [572/5000], Step [18/47], Loss: 0.0061\n",
      "Epoch [572/5000], Step [27/47], Loss: 0.0013\n",
      "Epoch [572/5000], Step [36/47], Loss: 0.0018\n",
      "Epoch [572/5000], Step [45/47], Loss: 0.0028\n",
      "Epoch [572/5000], Avg. Train Sample Loss: 0.0034, Avg. Validate Sample Loss: 0.0020,                             L2 Loss: 0.0036\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [573/5000] -------------------- \n",
      "Epoch [573/5000], Step [1/47], Loss: 0.0020\n",
      "Epoch [573/5000], Step [9/47], Loss: 0.0019\n",
      "Epoch [573/5000], Step [18/47], Loss: 0.0055\n",
      "Epoch [573/5000], Step [27/47], Loss: 0.0033\n",
      "Epoch [573/5000], Step [36/47], Loss: 0.0111\n",
      "Epoch [573/5000], Step [45/47], Loss: 0.0038\n",
      "Epoch [573/5000], Avg. Train Sample Loss: 0.0043, Avg. Validate Sample Loss: 0.0072,                             L2 Loss: 0.0114\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [574/5000] -------------------- \n",
      "Epoch [574/5000], Step [1/47], Loss: 0.0074\n",
      "Epoch [574/5000], Step [9/47], Loss: 0.0023\n",
      "Epoch [574/5000], Step [18/47], Loss: 0.0021\n",
      "Epoch [574/5000], Step [27/47], Loss: 0.0015\n",
      "Epoch [574/5000], Step [36/47], Loss: 0.0080\n",
      "Epoch [574/5000], Step [45/47], Loss: 0.0018\n",
      "Epoch [574/5000], Avg. Train Sample Loss: 0.0041, Avg. Validate Sample Loss: 0.0038,                             L2 Loss: 0.0068\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [575/5000] -------------------- \n",
      "Epoch [575/5000], Step [1/47], Loss: 0.0032\n",
      "Epoch [575/5000], Step [9/47], Loss: 0.0014\n",
      "Epoch [575/5000], Step [18/47], Loss: 0.0122\n",
      "Epoch [575/5000], Step [27/47], Loss: 0.0011\n",
      "Epoch [575/5000], Step [36/47], Loss: 0.0063\n",
      "Epoch [575/5000], Step [45/47], Loss: 0.0033\n",
      "Epoch [575/5000], Avg. Train Sample Loss: 0.0042, Avg. Validate Sample Loss: 0.0034,                             L2 Loss: 0.0075\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [576/5000] -------------------- \n",
      "Epoch [576/5000], Step [1/47], Loss: 0.0028\n",
      "Epoch [576/5000], Step [9/47], Loss: 0.0045\n",
      "Epoch [576/5000], Step [18/47], Loss: 0.0021\n",
      "Epoch [576/5000], Step [27/47], Loss: 0.0064\n",
      "Epoch [576/5000], Step [36/47], Loss: 0.0024\n",
      "Epoch [576/5000], Step [45/47], Loss: 0.0020\n",
      "Epoch [576/5000], Avg. Train Sample Loss: 0.0037, Avg. Validate Sample Loss: 0.0028,                             L2 Loss: 0.0054\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [577/5000] -------------------- \n",
      "Epoch [577/5000], Step [1/47], Loss: 0.0024\n",
      "Epoch [577/5000], Step [9/47], Loss: 0.0025\n",
      "Epoch [577/5000], Step [18/47], Loss: 0.0012\n",
      "Epoch [577/5000], Step [27/47], Loss: 0.0034\n",
      "Epoch [577/5000], Step [36/47], Loss: 0.0089\n",
      "Epoch [577/5000], Step [45/47], Loss: 0.0114\n",
      "Epoch [577/5000], Avg. Train Sample Loss: 0.0039, Avg. Validate Sample Loss: 0.0072,                             L2 Loss: 0.0113\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [578/5000] -------------------- \n",
      "Epoch [578/5000], Step [1/47], Loss: 0.0083\n",
      "Epoch [578/5000], Step [9/47], Loss: 0.0039\n",
      "Epoch [578/5000], Step [18/47], Loss: 0.0075\n",
      "Epoch [578/5000], Step [27/47], Loss: 0.0017\n",
      "Epoch [578/5000], Step [36/47], Loss: 0.0021\n",
      "Epoch [578/5000], Step [45/47], Loss: 0.0016\n",
      "Epoch [578/5000], Avg. Train Sample Loss: 0.0046, Avg. Validate Sample Loss: 0.0188,                             L2 Loss: 0.0195\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [579/5000] -------------------- \n",
      "Epoch [579/5000], Step [1/47], Loss: 0.0164\n",
      "Epoch [579/5000], Step [9/47], Loss: 0.0072\n",
      "Epoch [579/5000], Step [18/47], Loss: 0.0026\n",
      "Epoch [579/5000], Step [27/47], Loss: 0.0042\n",
      "Epoch [579/5000], Step [36/47], Loss: 0.0012\n",
      "Epoch [579/5000], Step [45/47], Loss: 0.0058\n",
      "Epoch [579/5000], Avg. Train Sample Loss: 0.0044, Avg. Validate Sample Loss: 0.0028,                             L2 Loss: 0.0049\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [580/5000] -------------------- \n",
      "Epoch [580/5000], Step [1/47], Loss: 0.0031\n",
      "Epoch [580/5000], Step [9/47], Loss: 0.0038\n",
      "Epoch [580/5000], Step [18/47], Loss: 0.0030\n",
      "Epoch [580/5000], Step [27/47], Loss: 0.0015\n",
      "Epoch [580/5000], Step [36/47], Loss: 0.0107\n",
      "Epoch [580/5000], Step [45/47], Loss: 0.0193\n",
      "Epoch [580/5000], Avg. Train Sample Loss: 0.0045, Avg. Validate Sample Loss: 0.0679,                             L2 Loss: 0.0382\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [581/5000] -------------------- \n",
      "Epoch [581/5000], Step [1/47], Loss: 0.0697\n",
      "Epoch [581/5000], Step [9/47], Loss: 0.0537\n",
      "Epoch [581/5000], Step [18/47], Loss: 0.0228\n",
      "Epoch [581/5000], Step [27/47], Loss: 0.0067\n",
      "Epoch [581/5000], Step [36/47], Loss: 0.0525\n",
      "Epoch [581/5000], Step [45/47], Loss: 0.0205\n",
      "Epoch [581/5000], Avg. Train Sample Loss: 0.0305, Avg. Validate Sample Loss: 0.0157,                             L2 Loss: 0.0171\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [582/5000] -------------------- \n",
      "Epoch [582/5000], Step [1/47], Loss: 0.0162\n",
      "Epoch [582/5000], Step [9/47], Loss: 0.0185\n",
      "Epoch [582/5000], Step [18/47], Loss: 0.0055\n",
      "Epoch [582/5000], Step [27/47], Loss: 0.0014\n",
      "Epoch [582/5000], Step [36/47], Loss: 0.0054\n",
      "Epoch [582/5000], Step [45/47], Loss: 0.0026\n",
      "Epoch [582/5000], Avg. Train Sample Loss: 0.0084, Avg. Validate Sample Loss: 0.0019,                             L2 Loss: 0.0035\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [583/5000] -------------------- \n",
      "Epoch [583/5000], Step [1/47], Loss: 0.0020\n",
      "Epoch [583/5000], Step [9/47], Loss: 0.0020\n",
      "Epoch [583/5000], Step [18/47], Loss: 0.0069\n",
      "Epoch [583/5000], Step [27/47], Loss: 0.0033\n",
      "Epoch [583/5000], Step [36/47], Loss: 0.0017\n",
      "Epoch [583/5000], Step [45/47], Loss: 0.0017\n",
      "Epoch [583/5000], Avg. Train Sample Loss: 0.0032, Avg. Validate Sample Loss: 0.0016,                             L2 Loss: 0.0014\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [584/5000] -------------------- \n",
      "Epoch [584/5000], Step [1/47], Loss: 0.0012\n",
      "Epoch [584/5000], Step [9/47], Loss: 0.0013\n",
      "Epoch [584/5000], Step [18/47], Loss: 0.0021\n",
      "Epoch [584/5000], Step [27/47], Loss: 0.0018\n",
      "Epoch [584/5000], Step [36/47], Loss: 0.0020\n",
      "Epoch [584/5000], Step [45/47], Loss: 0.0036\n",
      "Epoch [584/5000], Avg. Train Sample Loss: 0.0034, Avg. Validate Sample Loss: 0.0027,                             L2 Loss: 0.0053\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [585/5000] -------------------- \n",
      "Epoch [585/5000], Step [1/47], Loss: 0.0021\n",
      "Epoch [585/5000], Step [9/47], Loss: 0.0043\n",
      "Epoch [585/5000], Step [18/47], Loss: 0.0042\n",
      "Epoch [585/5000], Step [27/47], Loss: 0.0021\n",
      "Epoch [585/5000], Step [36/47], Loss: 0.0027\n",
      "Epoch [585/5000], Step [45/47], Loss: 0.0038\n",
      "Epoch [585/5000], Avg. Train Sample Loss: 0.0072, Avg. Validate Sample Loss: 0.0022,                             L2 Loss: 0.0029\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [586/5000] -------------------- \n",
      "Epoch [586/5000], Step [1/47], Loss: 0.0016\n",
      "Epoch [586/5000], Step [9/47], Loss: 0.0018\n",
      "Epoch [586/5000], Step [18/47], Loss: 0.0040\n",
      "Epoch [586/5000], Step [27/47], Loss: 0.0020\n",
      "Epoch [586/5000], Step [36/47], Loss: 0.0013\n",
      "Epoch [586/5000], Step [45/47], Loss: 0.0028\n",
      "Epoch [586/5000], Avg. Train Sample Loss: 0.0027, Avg. Validate Sample Loss: 0.0018,                             L2 Loss: 0.0022\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [587/5000] -------------------- \n",
      "Epoch [587/5000], Step [1/47], Loss: 0.0017\n",
      "Epoch [587/5000], Step [9/47], Loss: 0.0019\n",
      "Epoch [587/5000], Step [18/47], Loss: 0.0019\n",
      "Epoch [587/5000], Step [27/47], Loss: 0.0030\n",
      "Epoch [587/5000], Step [36/47], Loss: 0.0022\n",
      "Epoch [587/5000], Step [45/47], Loss: 0.0113\n",
      "Epoch [587/5000], Avg. Train Sample Loss: 0.0040, Avg. Validate Sample Loss: 0.0042,                             L2 Loss: 0.0076\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [588/5000] -------------------- \n",
      "Epoch [588/5000], Step [1/47], Loss: 0.0039\n",
      "Epoch [588/5000], Step [9/47], Loss: 0.0079\n",
      "Epoch [588/5000], Step [18/47], Loss: 0.0017\n",
      "Epoch [588/5000], Step [27/47], Loss: 0.0037\n",
      "Epoch [588/5000], Step [36/47], Loss: 0.0032\n",
      "Epoch [588/5000], Step [45/47], Loss: 0.0036\n",
      "Epoch [588/5000], Avg. Train Sample Loss: 0.0055, Avg. Validate Sample Loss: 0.0044,                             L2 Loss: 0.0077\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [589/5000] -------------------- \n",
      "Epoch [589/5000], Step [1/47], Loss: 0.0052\n",
      "Epoch [589/5000], Step [9/47], Loss: 0.0020\n",
      "Epoch [589/5000], Step [18/47], Loss: 0.0011\n",
      "Epoch [589/5000], Step [27/47], Loss: 0.0050\n",
      "Epoch [589/5000], Step [36/47], Loss: 0.0105\n",
      "Epoch [589/5000], Step [45/47], Loss: 0.0035\n",
      "Epoch [589/5000], Avg. Train Sample Loss: 0.0048, Avg. Validate Sample Loss: 0.0018,                             L2 Loss: 0.0025\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [590/5000] -------------------- \n",
      "Epoch [590/5000], Step [1/47], Loss: 0.0019\n",
      "Epoch [590/5000], Step [9/47], Loss: 0.0010\n",
      "Epoch [590/5000], Step [18/47], Loss: 0.0031\n",
      "Epoch [590/5000], Step [27/47], Loss: 0.0023\n",
      "Epoch [590/5000], Step [36/47], Loss: 0.0019\n",
      "Epoch [590/5000], Step [45/47], Loss: 0.0248\n",
      "Epoch [590/5000], Avg. Train Sample Loss: 0.0054, Avg. Validate Sample Loss: 0.0051,                             L2 Loss: 0.0089\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [591/5000] -------------------- \n",
      "Epoch [591/5000], Step [1/47], Loss: 0.0054\n",
      "Epoch [591/5000], Step [9/47], Loss: 0.0069\n",
      "Epoch [591/5000], Step [18/47], Loss: 0.0019\n",
      "Epoch [591/5000], Step [27/47], Loss: 0.0018\n",
      "Epoch [591/5000], Step [36/47], Loss: 0.0064\n",
      "Epoch [591/5000], Step [45/47], Loss: 0.0016\n",
      "Epoch [591/5000], Avg. Train Sample Loss: 0.0035, Avg. Validate Sample Loss: 0.0018,                             L2 Loss: 0.0027\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [592/5000] -------------------- \n",
      "Epoch [592/5000], Step [1/47], Loss: 0.0022\n",
      "Epoch [592/5000], Step [9/47], Loss: 0.0013\n",
      "Epoch [592/5000], Step [18/47], Loss: 0.0019\n",
      "Epoch [592/5000], Step [27/47], Loss: 0.0036\n",
      "Epoch [592/5000], Step [36/47], Loss: 0.0021\n",
      "Epoch [592/5000], Step [45/47], Loss: 0.0014\n",
      "Epoch [592/5000], Avg. Train Sample Loss: 0.0022, Avg. Validate Sample Loss: 0.0019,                             L2 Loss: 0.0019\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [593/5000] -------------------- \n",
      "Epoch [593/5000], Step [1/47], Loss: 0.0016\n",
      "Epoch [593/5000], Step [9/47], Loss: 0.0011\n",
      "Epoch [593/5000], Step [18/47], Loss: 0.0038\n",
      "Epoch [593/5000], Step [27/47], Loss: 0.0109\n",
      "Epoch [593/5000], Step [36/47], Loss: 0.0082\n",
      "Epoch [593/5000], Step [45/47], Loss: 0.0108\n",
      "Epoch [593/5000], Avg. Train Sample Loss: 0.0046, Avg. Validate Sample Loss: 0.0032,                             L2 Loss: 0.0057\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [594/5000] -------------------- \n",
      "Epoch [594/5000], Step [1/47], Loss: 0.0027\n",
      "Epoch [594/5000], Step [9/47], Loss: 0.0021\n",
      "Epoch [594/5000], Step [18/47], Loss: 0.0039\n",
      "Epoch [594/5000], Step [27/47], Loss: 0.0012\n",
      "Epoch [594/5000], Step [36/47], Loss: 0.0017\n",
      "Epoch [594/5000], Step [45/47], Loss: 0.0012\n",
      "Epoch [594/5000], Avg. Train Sample Loss: 0.0023, Avg. Validate Sample Loss: 0.0020,                             L2 Loss: 0.0013\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [595/5000] -------------------- \n",
      "Epoch [595/5000], Step [1/47], Loss: 0.0015\n",
      "Epoch [595/5000], Step [9/47], Loss: 0.0036\n",
      "Epoch [595/5000], Step [18/47], Loss: 0.0022\n",
      "Epoch [595/5000], Step [27/47], Loss: 0.0028\n",
      "Epoch [595/5000], Step [36/47], Loss: 0.0039\n",
      "Epoch [595/5000], Step [45/47], Loss: 0.0024\n",
      "Epoch [595/5000], Avg. Train Sample Loss: 0.0022, Avg. Validate Sample Loss: 0.0036,                             L2 Loss: 0.0070\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [596/5000] -------------------- \n",
      "Epoch [596/5000], Step [1/47], Loss: 0.0042\n",
      "Epoch [596/5000], Step [9/47], Loss: 0.0104\n",
      "Epoch [596/5000], Step [18/47], Loss: 0.0178\n",
      "Epoch [596/5000], Step [27/47], Loss: 0.2367\n",
      "Epoch [596/5000], Step [36/47], Loss: 0.1465\n",
      "Epoch [596/5000], Step [45/47], Loss: 0.0177\n",
      "Epoch [596/5000], Avg. Train Sample Loss: 0.0603, Avg. Validate Sample Loss: 0.0325,                             L2 Loss: 0.0266\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [597/5000] -------------------- \n",
      "Epoch [597/5000], Step [1/47], Loss: 0.0378\n",
      "Epoch [597/5000], Step [9/47], Loss: 0.0172\n",
      "Epoch [597/5000], Step [18/47], Loss: 0.0039\n",
      "Epoch [597/5000], Step [27/47], Loss: 0.0025\n",
      "Epoch [597/5000], Step [36/47], Loss: 0.0023\n",
      "Epoch [597/5000], Step [45/47], Loss: 0.0030\n",
      "Epoch [597/5000], Avg. Train Sample Loss: 0.0074, Avg. Validate Sample Loss: 0.0016,                             L2 Loss: 0.0015\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [598/5000] -------------------- \n",
      "Epoch [598/5000], Step [1/47], Loss: 0.0024\n",
      "Epoch [598/5000], Step [9/47], Loss: 0.0043\n",
      "Epoch [598/5000], Step [18/47], Loss: 0.0015\n",
      "Epoch [598/5000], Step [27/47], Loss: 0.0013\n",
      "Epoch [598/5000], Step [36/47], Loss: 0.0032\n",
      "Epoch [598/5000], Step [45/47], Loss: 0.0013\n",
      "Epoch [598/5000], Avg. Train Sample Loss: 0.0026, Avg. Validate Sample Loss: 0.0047,                             L2 Loss: 0.0087\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [599/5000] -------------------- \n",
      "Epoch [599/5000], Step [1/47], Loss: 0.0046\n",
      "Epoch [599/5000], Step [9/47], Loss: 0.0073\n",
      "Epoch [599/5000], Step [18/47], Loss: 0.0020\n",
      "Epoch [599/5000], Step [27/47], Loss: 0.0026\n",
      "Epoch [599/5000], Step [36/47], Loss: 0.0012\n",
      "Epoch [599/5000], Step [45/47], Loss: 0.0026\n",
      "Epoch [599/5000], Avg. Train Sample Loss: 0.0026, Avg. Validate Sample Loss: 0.0020,                             L2 Loss: 0.0017\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [600/5000] -------------------- \n",
      "Epoch [600/5000], Step [1/47], Loss: 0.0018\n",
      "Epoch [600/5000], Step [9/47], Loss: 0.0016\n",
      "Epoch [600/5000], Step [18/47], Loss: 0.0123\n",
      "Epoch [600/5000], Step [27/47], Loss: 0.0078\n",
      "Epoch [600/5000], Step [36/47], Loss: 0.0016\n",
      "Epoch [600/5000], Step [45/47], Loss: 0.0021\n",
      "Epoch [600/5000], Avg. Train Sample Loss: 0.0038, Avg. Validate Sample Loss: 0.0069,                             L2 Loss: 0.0111\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [601/5000] -------------------- \n",
      "Epoch [601/5000], Step [1/47], Loss: 0.0060\n",
      "Epoch [601/5000], Step [9/47], Loss: 0.0030\n",
      "Epoch [601/5000], Step [18/47], Loss: 0.0035\n",
      "Epoch [601/5000], Step [27/47], Loss: 0.0020\n",
      "Epoch [601/5000], Step [36/47], Loss: 0.0019\n",
      "Epoch [601/5000], Step [45/47], Loss: 0.0017\n",
      "Epoch [601/5000], Avg. Train Sample Loss: 0.0025, Avg. Validate Sample Loss: 0.0018,                             L2 Loss: 0.0020\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [602/5000] -------------------- \n",
      "Epoch [602/5000], Step [1/47], Loss: 0.0016\n",
      "Epoch [602/5000], Step [9/47], Loss: 0.0017\n",
      "Epoch [602/5000], Step [18/47], Loss: 0.0032\n",
      "Epoch [602/5000], Step [27/47], Loss: 0.0031\n",
      "Epoch [602/5000], Step [36/47], Loss: 0.0028\n",
      "Epoch [602/5000], Step [45/47], Loss: 0.0072\n",
      "Epoch [602/5000], Avg. Train Sample Loss: 0.0032, Avg. Validate Sample Loss: 0.0059,                             L2 Loss: 0.0103\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [603/5000] -------------------- \n",
      "Epoch [603/5000], Step [1/47], Loss: 0.0064\n",
      "Epoch [603/5000], Step [9/47], Loss: 0.0028\n",
      "Epoch [603/5000], Step [18/47], Loss: 0.0106\n",
      "Epoch [603/5000], Step [27/47], Loss: 0.0016\n",
      "Epoch [603/5000], Step [36/47], Loss: 0.0010\n",
      "Epoch [603/5000], Step [45/47], Loss: 0.0016\n",
      "Epoch [603/5000], Avg. Train Sample Loss: 0.0036, Avg. Validate Sample Loss: 0.0019,                             L2 Loss: 0.0029\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [604/5000] -------------------- \n",
      "Epoch [604/5000], Step [1/47], Loss: 0.0018\n",
      "Epoch [604/5000], Step [9/47], Loss: 0.0023\n",
      "Epoch [604/5000], Step [18/47], Loss: 0.0085\n",
      "Epoch [604/5000], Step [27/47], Loss: 0.0102\n",
      "Epoch [604/5000], Step [36/47], Loss: 0.0153\n",
      "Epoch [604/5000], Step [45/47], Loss: 0.0030\n",
      "Epoch [604/5000], Avg. Train Sample Loss: 0.0061, Avg. Validate Sample Loss: 0.0026,                             L2 Loss: 0.0066\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [605/5000] -------------------- \n",
      "Epoch [605/5000], Step [1/47], Loss: 0.0037\n",
      "Epoch [605/5000], Step [9/47], Loss: 0.0014\n",
      "Epoch [605/5000], Step [18/47], Loss: 0.0016\n",
      "Epoch [605/5000], Step [27/47], Loss: 0.0094\n",
      "Epoch [605/5000], Step [36/47], Loss: 0.0020\n",
      "Epoch [605/5000], Step [45/47], Loss: 0.0016\n",
      "Epoch [605/5000], Avg. Train Sample Loss: 0.0031, Avg. Validate Sample Loss: 0.0018,                             L2 Loss: 0.0016\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [606/5000] -------------------- \n",
      "Epoch [606/5000], Step [1/47], Loss: 0.0016\n",
      "Epoch [606/5000], Step [9/47], Loss: 0.0018\n",
      "Epoch [606/5000], Step [18/47], Loss: 0.0024\n",
      "Epoch [606/5000], Step [27/47], Loss: 0.0009\n",
      "Epoch [606/5000], Step [36/47], Loss: 0.0022\n",
      "Epoch [606/5000], Step [45/47], Loss: 0.0049\n",
      "Epoch [606/5000], Avg. Train Sample Loss: 0.0030, Avg. Validate Sample Loss: 0.0053,                             L2 Loss: 0.0095\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [607/5000] -------------------- \n",
      "Epoch [607/5000], Step [1/47], Loss: 0.0051\n",
      "Epoch [607/5000], Step [9/47], Loss: 0.0015\n",
      "Epoch [607/5000], Step [18/47], Loss: 0.0018\n",
      "Epoch [607/5000], Step [27/47], Loss: 0.0020\n",
      "Epoch [607/5000], Step [36/47], Loss: 0.0020\n",
      "Epoch [607/5000], Step [45/47], Loss: 0.0039\n",
      "Epoch [607/5000], Avg. Train Sample Loss: 0.0035, Avg. Validate Sample Loss: 0.0031,                             L2 Loss: 0.0060\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [608/5000] -------------------- \n",
      "Epoch [608/5000], Step [1/47], Loss: 0.0031\n",
      "Epoch [608/5000], Step [9/47], Loss: 0.0023\n",
      "Epoch [608/5000], Step [18/47], Loss: 0.0035\n",
      "Epoch [608/5000], Step [27/47], Loss: 0.0056\n",
      "Epoch [608/5000], Step [36/47], Loss: 0.0228\n",
      "Epoch [608/5000], Step [45/47], Loss: 0.0154\n",
      "Epoch [608/5000], Avg. Train Sample Loss: 0.0077, Avg. Validate Sample Loss: 0.0095,                             L2 Loss: 0.0141\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [609/5000] -------------------- \n",
      "Epoch [609/5000], Step [1/47], Loss: 0.0113\n",
      "Epoch [609/5000], Step [9/47], Loss: 0.0015\n",
      "Epoch [609/5000], Step [18/47], Loss: 0.0028\n",
      "Epoch [609/5000], Step [27/47], Loss: 0.0035\n",
      "Epoch [609/5000], Step [36/47], Loss: 0.0023\n",
      "Epoch [609/5000], Step [45/47], Loss: 0.0017\n",
      "Epoch [609/5000], Avg. Train Sample Loss: 0.0034, Avg. Validate Sample Loss: 0.0017,                             L2 Loss: 0.0040\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [610/5000] -------------------- \n",
      "Epoch [610/5000], Step [1/47], Loss: 0.0019\n",
      "Epoch [610/5000], Step [9/47], Loss: 0.0032\n",
      "Epoch [610/5000], Step [18/47], Loss: 0.0023\n",
      "Epoch [610/5000], Step [27/47], Loss: 0.0020\n",
      "Epoch [610/5000], Step [36/47], Loss: 0.0032\n",
      "Epoch [610/5000], Step [45/47], Loss: 0.0148\n",
      "Epoch [610/5000], Avg. Train Sample Loss: 0.0068, Avg. Validate Sample Loss: 0.0047,                             L2 Loss: 0.0083\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [611/5000] -------------------- \n",
      "Epoch [611/5000], Step [1/47], Loss: 0.0054\n",
      "Epoch [611/5000], Step [9/47], Loss: 0.0062\n",
      "Epoch [611/5000], Step [18/47], Loss: 0.0035\n",
      "Epoch [611/5000], Step [27/47], Loss: 0.0023\n",
      "Epoch [611/5000], Step [36/47], Loss: 0.0025\n",
      "Epoch [611/5000], Step [45/47], Loss: 0.0038\n",
      "Epoch [611/5000], Avg. Train Sample Loss: 0.0042, Avg. Validate Sample Loss: 0.0032,                             L2 Loss: 0.0061\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [612/5000] -------------------- \n",
      "Epoch [612/5000], Step [1/47], Loss: 0.0035\n",
      "Epoch [612/5000], Step [9/47], Loss: 0.0030\n",
      "Epoch [612/5000], Step [18/47], Loss: 0.0021\n",
      "Epoch [612/5000], Step [27/47], Loss: 0.0029\n",
      "Epoch [612/5000], Step [36/47], Loss: 0.0022\n",
      "Epoch [612/5000], Step [45/47], Loss: 0.0019\n",
      "Epoch [612/5000], Avg. Train Sample Loss: 0.0040, Avg. Validate Sample Loss: 0.0027,                             L2 Loss: 0.0045\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [613/5000] -------------------- \n",
      "Epoch [613/5000], Step [1/47], Loss: 0.0032\n",
      "Epoch [613/5000], Step [9/47], Loss: 0.0077\n",
      "Epoch [613/5000], Step [18/47], Loss: 0.0011\n",
      "Epoch [613/5000], Step [27/47], Loss: 0.0016\n",
      "Epoch [613/5000], Step [36/47], Loss: 0.0015\n",
      "Epoch [613/5000], Step [45/47], Loss: 0.0041\n",
      "Epoch [613/5000], Avg. Train Sample Loss: 0.0047, Avg. Validate Sample Loss: 0.0021,                             L2 Loss: 0.0047\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [614/5000] -------------------- \n",
      "Epoch [614/5000], Step [1/47], Loss: 0.0025\n",
      "Epoch [614/5000], Step [9/47], Loss: 0.0014\n",
      "Epoch [614/5000], Step [18/47], Loss: 0.0034\n",
      "Epoch [614/5000], Step [27/47], Loss: 0.0016\n",
      "Epoch [614/5000], Step [36/47], Loss: 0.0052\n",
      "Epoch [614/5000], Step [45/47], Loss: 0.0103\n",
      "Epoch [614/5000], Avg. Train Sample Loss: 0.0038, Avg. Validate Sample Loss: 0.0041,                             L2 Loss: 0.0069\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [615/5000] -------------------- \n",
      "Epoch [615/5000], Step [1/47], Loss: 0.0046\n",
      "Epoch [615/5000], Step [9/47], Loss: 0.0015\n",
      "Epoch [615/5000], Step [18/47], Loss: 0.0032\n",
      "Epoch [615/5000], Step [27/47], Loss: 0.0024\n",
      "Epoch [615/5000], Step [36/47], Loss: 0.0046\n",
      "Epoch [615/5000], Step [45/47], Loss: 0.0031\n",
      "Epoch [615/5000], Avg. Train Sample Loss: 0.0041, Avg. Validate Sample Loss: 0.0023,                             L2 Loss: 0.0042\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [616/5000] -------------------- \n",
      "Epoch [616/5000], Step [1/47], Loss: 0.0023\n",
      "Epoch [616/5000], Step [9/47], Loss: 0.0017\n",
      "Epoch [616/5000], Step [18/47], Loss: 0.0201\n",
      "Epoch [616/5000], Step [27/47], Loss: 0.0047\n",
      "Epoch [616/5000], Step [36/47], Loss: 0.0089\n",
      "Epoch [616/5000], Step [45/47], Loss: 0.0017\n",
      "Epoch [616/5000], Avg. Train Sample Loss: 0.0046, Avg. Validate Sample Loss: 0.0031,                             L2 Loss: 0.0064\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [617/5000] -------------------- \n",
      "Epoch [617/5000], Step [1/47], Loss: 0.0038\n",
      "Epoch [617/5000], Step [9/47], Loss: 0.0055\n",
      "Epoch [617/5000], Step [18/47], Loss: 0.0009\n",
      "Epoch [617/5000], Step [27/47], Loss: 0.0026\n",
      "Epoch [617/5000], Step [36/47], Loss: 0.0018\n",
      "Epoch [617/5000], Step [45/47], Loss: 0.0063\n",
      "Epoch [617/5000], Avg. Train Sample Loss: 0.0039, Avg. Validate Sample Loss: 0.0026,                             L2 Loss: 0.0053\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [618/5000] -------------------- \n",
      "Epoch [618/5000], Step [1/47], Loss: 0.0021\n",
      "Epoch [618/5000], Step [9/47], Loss: 0.0136\n",
      "Epoch [618/5000], Step [18/47], Loss: 0.0094\n",
      "Epoch [618/5000], Step [27/47], Loss: 0.0030\n",
      "Epoch [618/5000], Step [36/47], Loss: 0.0023\n",
      "Epoch [618/5000], Step [45/47], Loss: 0.0019\n",
      "Epoch [618/5000], Avg. Train Sample Loss: 0.0039, Avg. Validate Sample Loss: 0.0039,                             L2 Loss: 0.0079\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [619/5000] -------------------- \n",
      "Epoch [619/5000], Step [1/47], Loss: 0.0032\n",
      "Epoch [619/5000], Step [9/47], Loss: 0.0036\n",
      "Epoch [619/5000], Step [18/47], Loss: 0.0029\n",
      "Epoch [619/5000], Step [27/47], Loss: 0.0054\n",
      "Epoch [619/5000], Step [36/47], Loss: 0.0021\n",
      "Epoch [619/5000], Step [45/47], Loss: 0.0056\n",
      "Epoch [619/5000], Avg. Train Sample Loss: 0.0041, Avg. Validate Sample Loss: 0.0037,                             L2 Loss: 0.0065\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [620/5000] -------------------- \n",
      "Epoch [620/5000], Step [1/47], Loss: 0.0035\n",
      "Epoch [620/5000], Step [9/47], Loss: 0.0288\n",
      "Epoch [620/5000], Step [18/47], Loss: 0.0016\n",
      "Epoch [620/5000], Step [27/47], Loss: 0.0500\n",
      "Epoch [620/5000], Step [36/47], Loss: 0.0481\n",
      "Epoch [620/5000], Step [45/47], Loss: 0.0643\n",
      "Epoch [620/5000], Avg. Train Sample Loss: 0.0289, Avg. Validate Sample Loss: 0.0203,                             L2 Loss: 0.0203\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [621/5000] -------------------- \n",
      "Epoch [621/5000], Step [1/47], Loss: 0.0202\n",
      "Epoch [621/5000], Step [9/47], Loss: 0.0039\n",
      "Epoch [621/5000], Step [18/47], Loss: 0.0018\n",
      "Epoch [621/5000], Step [27/47], Loss: 0.0016\n",
      "Epoch [621/5000], Step [36/47], Loss: 0.0020\n",
      "Epoch [621/5000], Step [45/47], Loss: 0.0022\n",
      "Epoch [621/5000], Avg. Train Sample Loss: 0.0042, Avg. Validate Sample Loss: 0.0036,                             L2 Loss: 0.0068\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [622/5000] -------------------- \n",
      "Epoch [622/5000], Step [1/47], Loss: 0.0030\n",
      "Epoch [622/5000], Step [9/47], Loss: 0.0012\n",
      "Epoch [622/5000], Step [18/47], Loss: 0.0015\n",
      "Epoch [622/5000], Step [27/47], Loss: 0.0021\n",
      "Epoch [622/5000], Step [36/47], Loss: 0.0024\n",
      "Epoch [622/5000], Step [45/47], Loss: 0.0018\n",
      "Epoch [622/5000], Avg. Train Sample Loss: 0.0042, Avg. Validate Sample Loss: 0.0022,                             L2 Loss: 0.0029\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [623/5000] -------------------- \n",
      "Epoch [623/5000], Step [1/47], Loss: 0.0017\n",
      "Epoch [623/5000], Step [9/47], Loss: 0.0028\n",
      "Epoch [623/5000], Step [18/47], Loss: 0.0020\n",
      "Epoch [623/5000], Step [27/47], Loss: 0.0057\n",
      "Epoch [623/5000], Step [36/47], Loss: 0.0177\n",
      "Epoch [623/5000], Step [45/47], Loss: 0.0845\n",
      "Epoch [623/5000], Avg. Train Sample Loss: 0.0089, Avg. Validate Sample Loss: 0.0777,                             L2 Loss: 0.0428\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [624/5000] -------------------- \n",
      "Epoch [624/5000], Step [1/47], Loss: 0.0838\n",
      "Epoch [624/5000], Step [9/47], Loss: 0.0080\n",
      "Epoch [624/5000], Step [18/47], Loss: 0.0256\n",
      "Epoch [624/5000], Step [27/47], Loss: 0.0017\n",
      "Epoch [624/5000], Step [36/47], Loss: 0.0017\n",
      "Epoch [624/5000], Step [45/47], Loss: 0.0015\n",
      "Epoch [624/5000], Avg. Train Sample Loss: 0.0113, Avg. Validate Sample Loss: 0.0020,                             L2 Loss: 0.0039\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [625/5000] -------------------- \n",
      "Epoch [625/5000], Step [1/47], Loss: 0.0012\n",
      "Epoch [625/5000], Step [9/47], Loss: 0.0011\n",
      "Epoch [625/5000], Step [18/47], Loss: 0.0016\n",
      "Epoch [625/5000], Step [27/47], Loss: 0.0023\n",
      "Epoch [625/5000], Step [36/47], Loss: 0.0029\n",
      "Epoch [625/5000], Step [45/47], Loss: 0.0026\n",
      "Epoch [625/5000], Avg. Train Sample Loss: 0.0024, Avg. Validate Sample Loss: 0.0032,                             L2 Loss: 0.0065\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [626/5000] -------------------- \n",
      "Epoch [626/5000], Step [1/47], Loss: 0.0023\n",
      "Epoch [626/5000], Step [9/47], Loss: 0.0140\n",
      "Epoch [626/5000], Step [18/47], Loss: 0.0014\n",
      "Epoch [626/5000], Step [27/47], Loss: 0.0075\n",
      "Epoch [626/5000], Step [36/47], Loss: 0.0011\n",
      "Epoch [626/5000], Step [45/47], Loss: 0.0044\n",
      "Epoch [626/5000], Avg. Train Sample Loss: 0.0033, Avg. Validate Sample Loss: 0.0069,                             L2 Loss: 0.0117\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [627/5000] -------------------- \n",
      "Epoch [627/5000], Step [1/47], Loss: 0.0070\n",
      "Epoch [627/5000], Step [9/47], Loss: 0.0035\n",
      "Epoch [627/5000], Step [18/47], Loss: 0.0014\n",
      "Epoch [627/5000], Step [27/47], Loss: 0.0016\n",
      "Epoch [627/5000], Step [36/47], Loss: 0.0012\n",
      "Epoch [627/5000], Step [45/47], Loss: 0.0040\n",
      "Epoch [627/5000], Avg. Train Sample Loss: 0.0033, Avg. Validate Sample Loss: 0.0025,                             L2 Loss: 0.0044\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [628/5000] -------------------- \n",
      "Epoch [628/5000], Step [1/47], Loss: 0.0024\n",
      "Epoch [628/5000], Step [9/47], Loss: 0.0020\n",
      "Epoch [628/5000], Step [18/47], Loss: 0.0016\n",
      "Epoch [628/5000], Step [27/47], Loss: 0.0021\n",
      "Epoch [628/5000], Step [36/47], Loss: 0.0053\n",
      "Epoch [628/5000], Step [45/47], Loss: 0.0091\n",
      "Epoch [628/5000], Avg. Train Sample Loss: 0.0034, Avg. Validate Sample Loss: 0.0108,                             L2 Loss: 0.0146\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [629/5000] -------------------- \n",
      "Epoch [629/5000], Step [1/47], Loss: 0.0097\n",
      "Epoch [629/5000], Step [9/47], Loss: 0.0052\n",
      "Epoch [629/5000], Step [18/47], Loss: 0.0142\n",
      "Epoch [629/5000], Step [27/47], Loss: 0.0159\n",
      "Epoch [629/5000], Step [36/47], Loss: 0.0019\n",
      "Epoch [629/5000], Step [45/47], Loss: 0.0012\n",
      "Epoch [629/5000], Avg. Train Sample Loss: 0.0071, Avg. Validate Sample Loss: 0.0034,                             L2 Loss: 0.0069\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [630/5000] -------------------- \n",
      "Epoch [630/5000], Step [1/47], Loss: 0.0042\n",
      "Epoch [630/5000], Step [9/47], Loss: 0.0090\n",
      "Epoch [630/5000], Step [18/47], Loss: 0.0047\n",
      "Epoch [630/5000], Step [27/47], Loss: 0.0014\n",
      "Epoch [630/5000], Step [36/47], Loss: 0.0091\n",
      "Epoch [630/5000], Step [45/47], Loss: 0.0061\n",
      "Epoch [630/5000], Avg. Train Sample Loss: 0.0057, Avg. Validate Sample Loss: 0.0071,                             L2 Loss: 0.0114\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [631/5000] -------------------- \n",
      "Epoch [631/5000], Step [1/47], Loss: 0.0073\n",
      "Epoch [631/5000], Step [9/47], Loss: 0.0143\n",
      "Epoch [631/5000], Step [18/47], Loss: 0.0026\n",
      "Epoch [631/5000], Step [27/47], Loss: 0.0022\n",
      "Epoch [631/5000], Step [36/47], Loss: 0.0047\n",
      "Epoch [631/5000], Step [45/47], Loss: 0.0131\n",
      "Epoch [631/5000], Avg. Train Sample Loss: 0.0072, Avg. Validate Sample Loss: 0.0165,                             L2 Loss: 0.0184\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [632/5000] -------------------- \n",
      "Epoch [632/5000], Step [1/47], Loss: 0.0156\n",
      "Epoch [632/5000], Step [9/47], Loss: 0.0012\n",
      "Epoch [632/5000], Step [18/47], Loss: 0.0024\n",
      "Epoch [632/5000], Step [27/47], Loss: 0.0019\n",
      "Epoch [632/5000], Step [36/47], Loss: 0.0041\n",
      "Epoch [632/5000], Step [45/47], Loss: 0.0112\n",
      "Epoch [632/5000], Avg. Train Sample Loss: 0.0062, Avg. Validate Sample Loss: 0.0022,                             L2 Loss: 0.0042\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [633/5000] -------------------- \n",
      "Epoch [633/5000], Step [1/47], Loss: 0.0032\n",
      "Epoch [633/5000], Step [9/47], Loss: 0.0090\n",
      "Epoch [633/5000], Step [18/47], Loss: 0.0080\n",
      "Epoch [633/5000], Step [27/47], Loss: 0.0036\n",
      "Epoch [633/5000], Step [36/47], Loss: 0.0048\n",
      "Epoch [633/5000], Step [45/47], Loss: 0.0024\n",
      "Epoch [633/5000], Avg. Train Sample Loss: 0.0064, Avg. Validate Sample Loss: 0.0030,                             L2 Loss: 0.0056\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [634/5000] -------------------- \n",
      "Epoch [634/5000], Step [1/47], Loss: 0.0025\n",
      "Epoch [634/5000], Step [9/47], Loss: 0.0167\n",
      "Epoch [634/5000], Step [18/47], Loss: 0.0076\n",
      "Epoch [634/5000], Step [27/47], Loss: 0.0127\n",
      "Epoch [634/5000], Step [36/47], Loss: 0.0232\n",
      "Epoch [634/5000], Step [45/47], Loss: 0.0199\n",
      "Epoch [634/5000], Avg. Train Sample Loss: 0.0102, Avg. Validate Sample Loss: 0.0098,                             L2 Loss: 0.0141\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [635/5000] -------------------- \n",
      "Epoch [635/5000], Step [1/47], Loss: 0.0086\n",
      "Epoch [635/5000], Step [9/47], Loss: 0.0025\n",
      "Epoch [635/5000], Step [18/47], Loss: 0.0040\n",
      "Epoch [635/5000], Step [27/47], Loss: 0.0068\n",
      "Epoch [635/5000], Step [36/47], Loss: 0.0106\n",
      "Epoch [635/5000], Step [45/47], Loss: 0.0015\n",
      "Epoch [635/5000], Avg. Train Sample Loss: 0.0047, Avg. Validate Sample Loss: 0.0018,                             L2 Loss: 0.0038\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [636/5000] -------------------- \n",
      "Epoch [636/5000], Step [1/47], Loss: 0.0025\n",
      "Epoch [636/5000], Step [9/47], Loss: 0.0027\n",
      "Epoch [636/5000], Step [18/47], Loss: 0.0060\n",
      "Epoch [636/5000], Step [27/47], Loss: 0.0057\n",
      "Epoch [636/5000], Step [36/47], Loss: 0.0473\n",
      "Epoch [636/5000], Step [45/47], Loss: 0.0285\n",
      "Epoch [636/5000], Avg. Train Sample Loss: 0.0109, Avg. Validate Sample Loss: 0.0267,                             L2 Loss: 0.0245\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [637/5000] -------------------- \n",
      "Epoch [637/5000], Step [1/47], Loss: 0.0245\n",
      "Epoch [637/5000], Step [9/47], Loss: 0.0254\n",
      "Epoch [637/5000], Step [18/47], Loss: 0.0098\n",
      "Epoch [637/5000], Step [27/47], Loss: 0.0013\n",
      "Epoch [637/5000], Step [36/47], Loss: 0.0010\n",
      "Epoch [637/5000], Step [45/47], Loss: 0.0022\n",
      "Epoch [637/5000], Avg. Train Sample Loss: 0.0076, Avg. Validate Sample Loss: 0.0031,                             L2 Loss: 0.0068\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [638/5000] -------------------- \n",
      "Epoch [638/5000], Step [1/47], Loss: 0.0022\n",
      "Epoch [638/5000], Step [9/47], Loss: 0.0026\n",
      "Epoch [638/5000], Step [18/47], Loss: 0.0027\n",
      "Epoch [638/5000], Step [27/47], Loss: 0.0088\n",
      "Epoch [638/5000], Step [36/47], Loss: 0.0042\n",
      "Epoch [638/5000], Step [45/47], Loss: 0.0049\n",
      "Epoch [638/5000], Avg. Train Sample Loss: 0.0042, Avg. Validate Sample Loss: 0.0067,                             L2 Loss: 0.0113\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [639/5000] -------------------- \n",
      "Epoch [639/5000], Step [1/47], Loss: 0.0067\n",
      "Epoch [639/5000], Step [9/47], Loss: 0.0025\n",
      "Epoch [639/5000], Step [18/47], Loss: 0.0032\n",
      "Epoch [639/5000], Step [27/47], Loss: 0.0018\n",
      "Epoch [639/5000], Step [36/47], Loss: 0.0026\n",
      "Epoch [639/5000], Step [45/47], Loss: 0.0019\n",
      "Epoch [639/5000], Avg. Train Sample Loss: 0.0030, Avg. Validate Sample Loss: 0.0016,                             L2 Loss: 0.0033\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [640/5000] -------------------- \n",
      "Epoch [640/5000], Step [1/47], Loss: 0.0010\n",
      "Epoch [640/5000], Step [9/47], Loss: 0.0018\n",
      "Epoch [640/5000], Step [18/47], Loss: 0.0011\n",
      "Epoch [640/5000], Step [27/47], Loss: 0.0013\n",
      "Epoch [640/5000], Step [36/47], Loss: 0.0029\n",
      "Epoch [640/5000], Step [45/47], Loss: 0.0020\n",
      "Epoch [640/5000], Avg. Train Sample Loss: 0.0036, Avg. Validate Sample Loss: 0.0021,                             L2 Loss: 0.0034\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [641/5000] -------------------- \n",
      "Epoch [641/5000], Step [1/47], Loss: 0.0019\n",
      "Epoch [641/5000], Step [9/47], Loss: 0.0057\n",
      "Epoch [641/5000], Step [18/47], Loss: 0.0011\n",
      "Epoch [641/5000], Step [27/47], Loss: 0.0011\n",
      "Epoch [641/5000], Step [36/47], Loss: 0.0017\n",
      "Epoch [641/5000], Step [45/47], Loss: 0.0016\n",
      "Epoch [641/5000], Avg. Train Sample Loss: 0.0027, Avg. Validate Sample Loss: 0.0025,                             L2 Loss: 0.0050\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [642/5000] -------------------- \n",
      "Epoch [642/5000], Step [1/47], Loss: 0.0022\n",
      "Epoch [642/5000], Step [9/47], Loss: 0.0017\n",
      "Epoch [642/5000], Step [18/47], Loss: 0.0026\n",
      "Epoch [642/5000], Step [27/47], Loss: 0.0016\n",
      "Epoch [642/5000], Step [36/47], Loss: 0.0010\n",
      "Epoch [642/5000], Step [45/47], Loss: 0.0046\n",
      "Epoch [642/5000], Avg. Train Sample Loss: 0.0029, Avg. Validate Sample Loss: 0.0015,                             L2 Loss: 0.0017\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [643/5000] -------------------- \n",
      "Epoch [643/5000], Step [1/47], Loss: 0.0021\n",
      "Epoch [643/5000], Step [9/47], Loss: 0.0027\n",
      "Epoch [643/5000], Step [18/47], Loss: 0.0043\n",
      "Epoch [643/5000], Step [27/47], Loss: 0.0057\n",
      "Epoch [643/5000], Step [36/47], Loss: 0.0012\n",
      "Epoch [643/5000], Step [45/47], Loss: 0.0021\n",
      "Epoch [643/5000], Avg. Train Sample Loss: 0.0040, Avg. Validate Sample Loss: 0.0014,                             L2 Loss: 0.0018\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [644/5000] -------------------- \n",
      "Epoch [644/5000], Step [1/47], Loss: 0.0012\n",
      "Epoch [644/5000], Step [9/47], Loss: 0.0052\n",
      "Epoch [644/5000], Step [18/47], Loss: 0.0181\n",
      "Epoch [644/5000], Step [27/47], Loss: 0.0084\n",
      "Epoch [644/5000], Step [36/47], Loss: 0.0074\n",
      "Epoch [644/5000], Step [45/47], Loss: 0.0110\n",
      "Epoch [644/5000], Avg. Train Sample Loss: 0.0083, Avg. Validate Sample Loss: 0.0027,                             L2 Loss: 0.0044\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [645/5000] -------------------- \n",
      "Epoch [645/5000], Step [1/47], Loss: 0.0027\n",
      "Epoch [645/5000], Step [9/47], Loss: 0.0021\n",
      "Epoch [645/5000], Step [18/47], Loss: 0.0014\n",
      "Epoch [645/5000], Step [27/47], Loss: 0.0021\n",
      "Epoch [645/5000], Step [36/47], Loss: 0.0016\n",
      "Epoch [645/5000], Step [45/47], Loss: 0.0018\n",
      "Epoch [645/5000], Avg. Train Sample Loss: 0.0025, Avg. Validate Sample Loss: 0.0023,                             L2 Loss: 0.0043\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [646/5000] -------------------- \n",
      "Epoch [646/5000], Step [1/47], Loss: 0.0018\n",
      "Epoch [646/5000], Step [9/47], Loss: 0.0058\n",
      "Epoch [646/5000], Step [18/47], Loss: 0.0100\n",
      "Epoch [646/5000], Step [27/47], Loss: 0.0143\n",
      "Epoch [646/5000], Step [36/47], Loss: 0.0026\n",
      "Epoch [646/5000], Step [45/47], Loss: 0.0016\n",
      "Epoch [646/5000], Avg. Train Sample Loss: 0.0149, Avg. Validate Sample Loss: 0.0068,                             L2 Loss: 0.0111\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [647/5000] -------------------- \n",
      "Epoch [647/5000], Step [1/47], Loss: 0.0051\n",
      "Epoch [647/5000], Step [9/47], Loss: 0.0025\n",
      "Epoch [647/5000], Step [18/47], Loss: 0.0020\n",
      "Epoch [647/5000], Step [27/47], Loss: 0.0045\n",
      "Epoch [647/5000], Step [36/47], Loss: 0.0028\n",
      "Epoch [647/5000], Step [45/47], Loss: 0.0019\n",
      "Epoch [647/5000], Avg. Train Sample Loss: 0.0035, Avg. Validate Sample Loss: 0.0020,                             L2 Loss: 0.0037\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [648/5000] -------------------- \n",
      "Epoch [648/5000], Step [1/47], Loss: 0.0020\n",
      "Epoch [648/5000], Step [9/47], Loss: 0.0012\n",
      "Epoch [648/5000], Step [18/47], Loss: 0.0033\n",
      "Epoch [648/5000], Step [27/47], Loss: 0.0014\n",
      "Epoch [648/5000], Step [36/47], Loss: 0.0011\n",
      "Epoch [648/5000], Step [45/47], Loss: 0.0162\n",
      "Epoch [648/5000], Avg. Train Sample Loss: 0.0041, Avg. Validate Sample Loss: 0.0094,                             L2 Loss: 0.0133\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [649/5000] -------------------- \n",
      "Epoch [649/5000], Step [1/47], Loss: 0.0096\n",
      "Epoch [649/5000], Step [9/47], Loss: 0.0017\n",
      "Epoch [649/5000], Step [18/47], Loss: 0.0447\n",
      "Epoch [649/5000], Step [27/47], Loss: 0.0040\n",
      "Epoch [649/5000], Step [36/47], Loss: 0.0032\n",
      "Epoch [649/5000], Step [45/47], Loss: 0.0036\n",
      "Epoch [649/5000], Avg. Train Sample Loss: 0.0105, Avg. Validate Sample Loss: 0.0237,                             L2 Loss: 0.0233\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [650/5000] -------------------- \n",
      "Epoch [650/5000], Step [1/47], Loss: 0.0249\n",
      "Epoch [650/5000], Step [9/47], Loss: 0.0095\n",
      "Epoch [650/5000], Step [18/47], Loss: 0.0026\n",
      "Epoch [650/5000], Step [27/47], Loss: 0.0131\n",
      "Epoch [650/5000], Step [36/47], Loss: 0.0021\n",
      "Epoch [650/5000], Step [45/47], Loss: 0.0013\n",
      "Epoch [650/5000], Avg. Train Sample Loss: 0.0078, Avg. Validate Sample Loss: 0.0018,                             L2 Loss: 0.0047\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [651/5000] -------------------- \n",
      "Epoch [651/5000], Step [1/47], Loss: 0.0012\n",
      "Epoch [651/5000], Step [9/47], Loss: 0.0015\n",
      "Epoch [651/5000], Step [18/47], Loss: 0.0016\n",
      "Epoch [651/5000], Step [27/47], Loss: 0.0015\n",
      "Epoch [651/5000], Step [36/47], Loss: 0.0036\n",
      "Epoch [651/5000], Step [45/47], Loss: 0.0062\n",
      "Epoch [651/5000], Avg. Train Sample Loss: 0.0028, Avg. Validate Sample Loss: 0.0015,                             L2 Loss: 0.0021\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [652/5000] -------------------- \n",
      "Epoch [652/5000], Step [1/47], Loss: 0.0015\n",
      "Epoch [652/5000], Step [9/47], Loss: 0.0014\n",
      "Epoch [652/5000], Step [18/47], Loss: 0.0032\n",
      "Epoch [652/5000], Step [27/47], Loss: 0.0016\n",
      "Epoch [652/5000], Step [36/47], Loss: 0.0020\n",
      "Epoch [652/5000], Step [45/47], Loss: 0.0057\n",
      "Epoch [652/5000], Avg. Train Sample Loss: 0.0021, Avg. Validate Sample Loss: 0.0030,                             L2 Loss: 0.0060\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [653/5000] -------------------- \n",
      "Epoch [653/5000], Step [1/47], Loss: 0.0030\n",
      "Epoch [653/5000], Step [9/47], Loss: 0.0017\n",
      "Epoch [653/5000], Step [18/47], Loss: 0.0020\n",
      "Epoch [653/5000], Step [27/47], Loss: 0.0021\n",
      "Epoch [653/5000], Step [36/47], Loss: 0.0021\n",
      "Epoch [653/5000], Step [45/47], Loss: 0.0023\n",
      "Epoch [653/5000], Avg. Train Sample Loss: 0.0028, Avg. Validate Sample Loss: 0.0017,                             L2 Loss: 0.0028\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [654/5000] -------------------- \n",
      "Epoch [654/5000], Step [1/47], Loss: 0.0023\n",
      "Epoch [654/5000], Step [9/47], Loss: 0.0059\n",
      "Epoch [654/5000], Step [18/47], Loss: 0.0156\n",
      "Epoch [654/5000], Step [27/47], Loss: 0.0017\n",
      "Epoch [654/5000], Step [36/47], Loss: 0.0017\n",
      "Epoch [654/5000], Step [45/47], Loss: 0.0039\n",
      "Epoch [654/5000], Avg. Train Sample Loss: 0.0051, Avg. Validate Sample Loss: 0.0070,                             L2 Loss: 0.0117\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [655/5000] -------------------- \n",
      "Epoch [655/5000], Step [1/47], Loss: 0.0061\n",
      "Epoch [655/5000], Step [9/47], Loss: 0.0024\n",
      "Epoch [655/5000], Step [18/47], Loss: 0.0013\n",
      "Epoch [655/5000], Step [27/47], Loss: 0.0019\n",
      "Epoch [655/5000], Step [36/47], Loss: 0.0090\n",
      "Epoch [655/5000], Step [45/47], Loss: 0.0021\n",
      "Epoch [655/5000], Avg. Train Sample Loss: 0.0058, Avg. Validate Sample Loss: 0.0080,                             L2 Loss: 0.0119\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [656/5000] -------------------- \n",
      "Epoch [656/5000], Step [1/47], Loss: 0.0078\n",
      "Epoch [656/5000], Step [9/47], Loss: 0.0021\n",
      "Epoch [656/5000], Step [18/47], Loss: 0.0112\n",
      "Epoch [656/5000], Step [27/47], Loss: 0.0117\n",
      "Epoch [656/5000], Step [36/47], Loss: 0.0160\n",
      "Epoch [656/5000], Step [45/47], Loss: 0.0023\n",
      "Epoch [656/5000], Avg. Train Sample Loss: 0.0115, Avg. Validate Sample Loss: 0.0030,                             L2 Loss: 0.0058\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [657/5000] -------------------- \n",
      "Epoch [657/5000], Step [1/47], Loss: 0.0033\n",
      "Epoch [657/5000], Step [9/47], Loss: 0.0124\n",
      "Epoch [657/5000], Step [18/47], Loss: 0.0041\n",
      "Epoch [657/5000], Step [27/47], Loss: 0.0018\n",
      "Epoch [657/5000], Step [36/47], Loss: 0.0071\n",
      "Epoch [657/5000], Step [45/47], Loss: 0.0081\n",
      "Epoch [657/5000], Avg. Train Sample Loss: 0.0074, Avg. Validate Sample Loss: 0.0014,                             L2 Loss: 0.0021\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [658/5000] -------------------- \n",
      "Epoch [658/5000], Step [1/47], Loss: 0.0024\n",
      "Epoch [658/5000], Step [9/47], Loss: 0.0015\n",
      "Epoch [658/5000], Step [18/47], Loss: 0.0021\n",
      "Epoch [658/5000], Step [27/47], Loss: 0.0047\n",
      "Epoch [658/5000], Step [36/47], Loss: 0.0021\n",
      "Epoch [658/5000], Step [45/47], Loss: 0.0012\n",
      "Epoch [658/5000], Avg. Train Sample Loss: 0.0026, Avg. Validate Sample Loss: 0.0016,                             L2 Loss: 0.0026\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [659/5000] -------------------- \n",
      "Epoch [659/5000], Step [1/47], Loss: 0.0010\n",
      "Epoch [659/5000], Step [9/47], Loss: 0.0024\n",
      "Epoch [659/5000], Step [18/47], Loss: 0.0021\n",
      "Epoch [659/5000], Step [27/47], Loss: 0.0016\n",
      "Epoch [659/5000], Step [36/47], Loss: 0.0015\n",
      "Epoch [659/5000], Step [45/47], Loss: 0.0084\n",
      "Epoch [659/5000], Avg. Train Sample Loss: 0.0032, Avg. Validate Sample Loss: 0.0085,                             L2 Loss: 0.0136\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [660/5000] -------------------- \n",
      "Epoch [660/5000], Step [1/47], Loss: 0.0089\n",
      "Epoch [660/5000], Step [9/47], Loss: 0.0014\n",
      "Epoch [660/5000], Step [18/47], Loss: 0.0049\n",
      "Epoch [660/5000], Step [27/47], Loss: 0.0081\n",
      "Epoch [660/5000], Step [36/47], Loss: 0.0028\n",
      "Epoch [660/5000], Step [45/47], Loss: 0.0065\n",
      "Epoch [660/5000], Avg. Train Sample Loss: 0.0049, Avg. Validate Sample Loss: 0.0065,                             L2 Loss: 0.0112\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [661/5000] -------------------- \n",
      "Epoch [661/5000], Step [1/47], Loss: 0.0070\n",
      "Epoch [661/5000], Step [9/47], Loss: 0.0046\n",
      "Epoch [661/5000], Step [18/47], Loss: 0.0081\n",
      "Epoch [661/5000], Step [27/47], Loss: 0.0142\n",
      "Epoch [661/5000], Step [36/47], Loss: 0.0477\n",
      "Epoch [661/5000], Step [45/47], Loss: 0.0312\n",
      "Epoch [661/5000], Avg. Train Sample Loss: 0.0148, Avg. Validate Sample Loss: 0.0221,                             L2 Loss: 0.0213\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [662/5000] -------------------- \n",
      "Epoch [662/5000], Step [1/47], Loss: 0.0212\n",
      "Epoch [662/5000], Step [9/47], Loss: 0.0032\n",
      "Epoch [662/5000], Step [18/47], Loss: 0.0044\n",
      "Epoch [662/5000], Step [27/47], Loss: 0.0014\n",
      "Epoch [662/5000], Step [36/47], Loss: 0.0007\n",
      "Epoch [662/5000], Step [45/47], Loss: 0.0016\n",
      "Epoch [662/5000], Avg. Train Sample Loss: 0.0048, Avg. Validate Sample Loss: 0.0024,                             L2 Loss: 0.0051\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [663/5000] -------------------- \n",
      "Epoch [663/5000], Step [1/47], Loss: 0.0019\n",
      "Epoch [663/5000], Step [9/47], Loss: 0.0020\n",
      "Epoch [663/5000], Step [18/47], Loss: 0.0014\n",
      "Epoch [663/5000], Step [27/47], Loss: 0.0031\n",
      "Epoch [663/5000], Step [36/47], Loss: 0.0084\n",
      "Epoch [663/5000], Step [45/47], Loss: 0.0036\n",
      "Epoch [663/5000], Avg. Train Sample Loss: 0.0036, Avg. Validate Sample Loss: 0.0025,                             L2 Loss: 0.0051\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [664/5000] -------------------- \n",
      "Epoch [664/5000], Step [1/47], Loss: 0.0025\n",
      "Epoch [664/5000], Step [9/47], Loss: 0.0021\n",
      "Epoch [664/5000], Step [18/47], Loss: 0.0020\n",
      "Epoch [664/5000], Step [27/47], Loss: 0.0021\n",
      "Epoch [664/5000], Step [36/47], Loss: 0.0013\n",
      "Epoch [664/5000], Step [45/47], Loss: 0.0013\n",
      "Epoch [664/5000], Avg. Train Sample Loss: 0.0018, Avg. Validate Sample Loss: 0.0017,                             L2 Loss: 0.0038\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [665/5000] -------------------- \n",
      "Epoch [665/5000], Step [1/47], Loss: 0.0013\n",
      "Epoch [665/5000], Step [9/47], Loss: 0.0015\n",
      "Epoch [665/5000], Step [18/47], Loss: 0.0020\n",
      "Epoch [665/5000], Step [27/47], Loss: 0.0012\n",
      "Epoch [665/5000], Step [36/47], Loss: 0.0032\n",
      "Epoch [665/5000], Step [45/47], Loss: 0.0026\n",
      "Epoch [665/5000], Avg. Train Sample Loss: 0.0027, Avg. Validate Sample Loss: 0.0014,                             L2 Loss: 0.0029\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [666/5000] -------------------- \n",
      "Epoch [666/5000], Step [1/47], Loss: 0.0014\n",
      "Epoch [666/5000], Step [9/47], Loss: 0.0012\n",
      "Epoch [666/5000], Step [18/47], Loss: 0.0059\n",
      "Epoch [666/5000], Step [27/47], Loss: 0.0224\n",
      "Epoch [666/5000], Step [36/47], Loss: 0.0040\n",
      "Epoch [666/5000], Step [45/47], Loss: 0.0033\n",
      "Epoch [666/5000], Avg. Train Sample Loss: 0.0067, Avg. Validate Sample Loss: 0.0017,                             L2 Loss: 0.0020\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [667/5000] -------------------- \n",
      "Epoch [667/5000], Step [1/47], Loss: 0.0021\n",
      "Epoch [667/5000], Step [9/47], Loss: 0.0016\n",
      "Epoch [667/5000], Step [18/47], Loss: 0.0124\n",
      "Epoch [667/5000], Step [27/47], Loss: 0.0186\n",
      "Epoch [667/5000], Step [36/47], Loss: 0.0014\n",
      "Epoch [667/5000], Step [45/47], Loss: 0.0020\n",
      "Epoch [667/5000], Avg. Train Sample Loss: 0.0068, Avg. Validate Sample Loss: 0.0014,                             L2 Loss: 0.0008\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [668/5000] -------------------- \n",
      "Epoch [668/5000], Step [1/47], Loss: 0.0010\n",
      "Epoch [668/5000], Step [9/47], Loss: 0.0014\n",
      "Epoch [668/5000], Step [18/47], Loss: 0.0059\n",
      "Epoch [668/5000], Step [27/47], Loss: 0.0032\n",
      "Epoch [668/5000], Step [36/47], Loss: 0.0016\n",
      "Epoch [668/5000], Step [45/47], Loss: 0.0023\n",
      "Epoch [668/5000], Avg. Train Sample Loss: 0.0034, Avg. Validate Sample Loss: 0.0019,                             L2 Loss: 0.0031\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [669/5000] -------------------- \n",
      "Epoch [669/5000], Step [1/47], Loss: 0.0017\n",
      "Epoch [669/5000], Step [9/47], Loss: 0.0069\n",
      "Epoch [669/5000], Step [18/47], Loss: 0.0026\n",
      "Epoch [669/5000], Step [27/47], Loss: 0.0020\n",
      "Epoch [669/5000], Step [36/47], Loss: 0.0053\n",
      "Epoch [669/5000], Step [45/47], Loss: 0.0016\n",
      "Epoch [669/5000], Avg. Train Sample Loss: 0.0039, Avg. Validate Sample Loss: 0.0020,                             L2 Loss: 0.0039\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [670/5000] -------------------- \n",
      "Epoch [670/5000], Step [1/47], Loss: 0.0022\n",
      "Epoch [670/5000], Step [9/47], Loss: 0.0021\n",
      "Epoch [670/5000], Step [18/47], Loss: 0.0018\n",
      "Epoch [670/5000], Step [27/47], Loss: 0.0070\n",
      "Epoch [670/5000], Step [36/47], Loss: 0.0011\n",
      "Epoch [670/5000], Step [45/47], Loss: 0.0024\n",
      "Epoch [670/5000], Avg. Train Sample Loss: 0.0041, Avg. Validate Sample Loss: 0.0032,                             L2 Loss: 0.0063\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [671/5000] -------------------- \n",
      "Epoch [671/5000], Step [1/47], Loss: 0.0026\n",
      "Epoch [671/5000], Step [9/47], Loss: 0.0257\n",
      "Epoch [671/5000], Step [18/47], Loss: 0.0078\n",
      "Epoch [671/5000], Step [27/47], Loss: 0.0386\n",
      "Epoch [671/5000], Step [36/47], Loss: 0.0262\n",
      "Epoch [671/5000], Step [45/47], Loss: 0.0011\n",
      "Epoch [671/5000], Avg. Train Sample Loss: 0.0125, Avg. Validate Sample Loss: 0.0021,                             L2 Loss: 0.0036\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [672/5000] -------------------- \n",
      "Epoch [672/5000], Step [1/47], Loss: 0.0020\n",
      "Epoch [672/5000], Step [9/47], Loss: 0.0009\n",
      "Epoch [672/5000], Step [18/47], Loss: 0.0012\n",
      "Epoch [672/5000], Step [27/47], Loss: 0.0017\n",
      "Epoch [672/5000], Step [36/47], Loss: 0.0095\n",
      "Epoch [672/5000], Step [45/47], Loss: 0.0023\n",
      "Epoch [672/5000], Avg. Train Sample Loss: 0.0043, Avg. Validate Sample Loss: 0.0016,                             L2 Loss: 0.0015\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [673/5000] -------------------- \n",
      "Epoch [673/5000], Step [1/47], Loss: 0.0012\n",
      "Epoch [673/5000], Step [9/47], Loss: 0.0081\n",
      "Epoch [673/5000], Step [18/47], Loss: 0.0039\n",
      "Epoch [673/5000], Step [27/47], Loss: 0.0030\n",
      "Epoch [673/5000], Step [36/47], Loss: 0.0027\n",
      "Epoch [673/5000], Step [45/47], Loss: 0.0037\n",
      "Epoch [673/5000], Avg. Train Sample Loss: 0.0056, Avg. Validate Sample Loss: 0.0020,                             L2 Loss: 0.0031\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [674/5000] -------------------- \n",
      "Epoch [674/5000], Step [1/47], Loss: 0.0019\n",
      "Epoch [674/5000], Step [9/47], Loss: 0.0148\n",
      "Epoch [674/5000], Step [18/47], Loss: 0.0084\n",
      "Epoch [674/5000], Step [27/47], Loss: 0.0012\n",
      "Epoch [674/5000], Step [36/47], Loss: 0.0008\n",
      "Epoch [674/5000], Step [45/47], Loss: 0.0028\n",
      "Epoch [674/5000], Avg. Train Sample Loss: 0.0048, Avg. Validate Sample Loss: 0.0019,                             L2 Loss: 0.0037\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [675/5000] -------------------- \n",
      "Epoch [675/5000], Step [1/47], Loss: 0.0019\n",
      "Epoch [675/5000], Step [9/47], Loss: 0.0010\n",
      "Epoch [675/5000], Step [18/47], Loss: 0.0018\n",
      "Epoch [675/5000], Step [27/47], Loss: 0.0033\n",
      "Epoch [675/5000], Step [36/47], Loss: 0.0013\n",
      "Epoch [675/5000], Step [45/47], Loss: 0.0055\n",
      "Epoch [675/5000], Avg. Train Sample Loss: 0.0028, Avg. Validate Sample Loss: 0.0015,                             L2 Loss: 0.0022\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [676/5000] -------------------- \n",
      "Epoch [676/5000], Step [1/47], Loss: 0.0014\n",
      "Epoch [676/5000], Step [9/47], Loss: 0.0061\n",
      "Epoch [676/5000], Step [18/47], Loss: 0.0036\n",
      "Epoch [676/5000], Step [27/47], Loss: 0.0015\n",
      "Epoch [676/5000], Step [36/47], Loss: 0.0058\n",
      "Epoch [676/5000], Step [45/47], Loss: 0.0023\n",
      "Epoch [676/5000], Avg. Train Sample Loss: 0.0037, Avg. Validate Sample Loss: 0.0018,                             L2 Loss: 0.0035\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [677/5000] -------------------- \n",
      "Epoch [677/5000], Step [1/47], Loss: 0.0009\n",
      "Epoch [677/5000], Step [9/47], Loss: 0.0032\n",
      "Epoch [677/5000], Step [18/47], Loss: 0.0018\n",
      "Epoch [677/5000], Step [27/47], Loss: 0.0018\n",
      "Epoch [677/5000], Step [36/47], Loss: 0.0016\n",
      "Epoch [677/5000], Step [45/47], Loss: 0.0015\n",
      "Epoch [677/5000], Avg. Train Sample Loss: 0.0025, Avg. Validate Sample Loss: 0.0044,                             L2 Loss: 0.0082\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [678/5000] -------------------- \n",
      "Epoch [678/5000], Step [1/47], Loss: 0.0040\n",
      "Epoch [678/5000], Step [9/47], Loss: 0.0016\n",
      "Epoch [678/5000], Step [18/47], Loss: 0.0007\n",
      "Epoch [678/5000], Step [27/47], Loss: 0.0015\n",
      "Epoch [678/5000], Step [36/47], Loss: 0.0031\n",
      "Epoch [678/5000], Step [45/47], Loss: 0.0014\n",
      "Epoch [678/5000], Avg. Train Sample Loss: 0.0024, Avg. Validate Sample Loss: 0.0016,                             L2 Loss: 0.0018\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [679/5000] -------------------- \n",
      "Epoch [679/5000], Step [1/47], Loss: 0.0014\n",
      "Epoch [679/5000], Step [9/47], Loss: 0.0017\n",
      "Epoch [679/5000], Step [18/47], Loss: 0.0022\n",
      "Epoch [679/5000], Step [27/47], Loss: 0.0012\n",
      "Epoch [679/5000], Step [36/47], Loss: 0.0047\n",
      "Epoch [679/5000], Step [45/47], Loss: 0.0082\n",
      "Epoch [679/5000], Avg. Train Sample Loss: 0.0024, Avg. Validate Sample Loss: 0.0033,                             L2 Loss: 0.0070\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [680/5000] -------------------- \n",
      "Epoch [680/5000], Step [1/47], Loss: 0.0035\n",
      "Epoch [680/5000], Step [9/47], Loss: 0.0017\n",
      "Epoch [680/5000], Step [18/47], Loss: 0.0019\n",
      "Epoch [680/5000], Step [27/47], Loss: 0.0016\n",
      "Epoch [680/5000], Step [36/47], Loss: 0.0087\n",
      "Epoch [680/5000], Step [45/47], Loss: 0.0044\n",
      "Epoch [680/5000], Avg. Train Sample Loss: 0.0048, Avg. Validate Sample Loss: 0.0028,                             L2 Loss: 0.0067\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [681/5000] -------------------- \n",
      "Epoch [681/5000], Step [1/47], Loss: 0.0027\n",
      "Epoch [681/5000], Step [9/47], Loss: 0.0069\n",
      "Epoch [681/5000], Step [18/47], Loss: 0.0049\n",
      "Epoch [681/5000], Step [27/47], Loss: 0.0038\n",
      "Epoch [681/5000], Step [36/47], Loss: 0.0068\n",
      "Epoch [681/5000], Step [45/47], Loss: 0.0229\n",
      "Epoch [681/5000], Avg. Train Sample Loss: 0.0053, Avg. Validate Sample Loss: 0.0371,                             L2 Loss: 0.0292\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [682/5000] -------------------- \n",
      "Epoch [682/5000], Step [1/47], Loss: 0.0344\n",
      "Epoch [682/5000], Step [9/47], Loss: 0.0125\n",
      "Epoch [682/5000], Step [18/47], Loss: 0.0019\n",
      "Epoch [682/5000], Step [27/47], Loss: 0.0071\n",
      "Epoch [682/5000], Step [36/47], Loss: 0.0266\n",
      "Epoch [682/5000], Step [45/47], Loss: 0.0017\n",
      "Epoch [682/5000], Avg. Train Sample Loss: 0.0136, Avg. Validate Sample Loss: 0.0034,                             L2 Loss: 0.0075\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [683/5000] -------------------- \n",
      "Epoch [683/5000], Step [1/47], Loss: 0.0036\n",
      "Epoch [683/5000], Step [9/47], Loss: 0.0015\n",
      "Epoch [683/5000], Step [18/47], Loss: 0.0095\n",
      "Epoch [683/5000], Step [27/47], Loss: 0.0030\n",
      "Epoch [683/5000], Step [36/47], Loss: 0.0021\n",
      "Epoch [683/5000], Step [45/47], Loss: 0.0027\n",
      "Epoch [683/5000], Avg. Train Sample Loss: 0.0045, Avg. Validate Sample Loss: 0.0017,                             L2 Loss: 0.0018\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [684/5000] -------------------- \n",
      "Epoch [684/5000], Step [1/47], Loss: 0.0019\n",
      "Epoch [684/5000], Step [9/47], Loss: 0.0015\n",
      "Epoch [684/5000], Step [18/47], Loss: 0.0020\n",
      "Epoch [684/5000], Step [27/47], Loss: 0.0129\n",
      "Epoch [684/5000], Step [36/47], Loss: 0.0166\n",
      "Epoch [684/5000], Step [45/47], Loss: 0.0012\n",
      "Epoch [684/5000], Avg. Train Sample Loss: 0.0063, Avg. Validate Sample Loss: 0.0040,                             L2 Loss: 0.0082\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [685/5000] -------------------- \n",
      "Epoch [685/5000], Step [1/47], Loss: 0.0047\n",
      "Epoch [685/5000], Step [9/47], Loss: 0.0055\n",
      "Epoch [685/5000], Step [18/47], Loss: 0.0029\n",
      "Epoch [685/5000], Step [27/47], Loss: 0.0009\n",
      "Epoch [685/5000], Step [36/47], Loss: 0.0019\n",
      "Epoch [685/5000], Step [45/47], Loss: 0.0049\n",
      "Epoch [685/5000], Avg. Train Sample Loss: 0.0030, Avg. Validate Sample Loss: 0.0052,                             L2 Loss: 0.0107\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [686/5000] -------------------- \n",
      "Epoch [686/5000], Step [1/47], Loss: 0.0053\n",
      "Epoch [686/5000], Step [9/47], Loss: 0.0017\n",
      "Epoch [686/5000], Step [18/47], Loss: 0.0022\n",
      "Epoch [686/5000], Step [27/47], Loss: 0.0028\n",
      "Epoch [686/5000], Step [36/47], Loss: 0.0027\n",
      "Epoch [686/5000], Step [45/47], Loss: 0.0156\n",
      "Epoch [686/5000], Avg. Train Sample Loss: 0.0040, Avg. Validate Sample Loss: 0.0012,                             L2 Loss: 0.0020\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [687/5000] -------------------- \n",
      "Epoch [687/5000], Step [1/47], Loss: 0.0011\n",
      "Epoch [687/5000], Step [9/47], Loss: 0.0022\n",
      "Epoch [687/5000], Step [18/47], Loss: 0.0056\n",
      "Epoch [687/5000], Step [27/47], Loss: 0.0019\n",
      "Epoch [687/5000], Step [36/47], Loss: 0.0019\n",
      "Epoch [687/5000], Step [45/47], Loss: 0.0095\n",
      "Epoch [687/5000], Avg. Train Sample Loss: 0.0057, Avg. Validate Sample Loss: 0.0027,                             L2 Loss: 0.0058\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [688/5000] -------------------- \n",
      "Epoch [688/5000], Step [1/47], Loss: 0.0039\n",
      "Epoch [688/5000], Step [9/47], Loss: 0.0052\n",
      "Epoch [688/5000], Step [18/47], Loss: 0.0415\n",
      "Epoch [688/5000], Step [27/47], Loss: 0.0012\n",
      "Epoch [688/5000], Step [36/47], Loss: 0.0014\n",
      "Epoch [688/5000], Step [45/47], Loss: 0.0031\n",
      "Epoch [688/5000], Avg. Train Sample Loss: 0.0082, Avg. Validate Sample Loss: 0.0030,                             L2 Loss: 0.0064\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [689/5000] -------------------- \n",
      "Epoch [689/5000], Step [1/47], Loss: 0.0027\n",
      "Epoch [689/5000], Step [9/47], Loss: 0.0080\n",
      "Epoch [689/5000], Step [18/47], Loss: 0.0019\n",
      "Epoch [689/5000], Step [27/47], Loss: 0.0297\n",
      "Epoch [689/5000], Step [36/47], Loss: 0.0206\n",
      "Epoch [689/5000], Step [45/47], Loss: 0.0385\n",
      "Epoch [689/5000], Avg. Train Sample Loss: 0.0135, Avg. Validate Sample Loss: 0.0323,                             L2 Loss: 0.0272\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [690/5000] -------------------- \n",
      "Epoch [690/5000], Step [1/47], Loss: 0.0368\n",
      "Epoch [690/5000], Step [9/47], Loss: 0.0065\n",
      "Epoch [690/5000], Step [18/47], Loss: 0.0020\n",
      "Epoch [690/5000], Step [27/47], Loss: 0.0019\n",
      "Epoch [690/5000], Step [36/47], Loss: 0.0093\n",
      "Epoch [690/5000], Step [45/47], Loss: 0.0106\n",
      "Epoch [690/5000], Avg. Train Sample Loss: 0.0134, Avg. Validate Sample Loss: 0.0064,                             L2 Loss: 0.0113\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [691/5000] -------------------- \n",
      "Epoch [691/5000], Step [1/47], Loss: 0.0060\n",
      "Epoch [691/5000], Step [9/47], Loss: 0.0081\n",
      "Epoch [691/5000], Step [18/47], Loss: 0.0161\n",
      "Epoch [691/5000], Step [27/47], Loss: 0.0149\n",
      "Epoch [691/5000], Step [36/47], Loss: 0.0092\n",
      "Epoch [691/5000], Step [45/47], Loss: 0.0017\n",
      "Epoch [691/5000], Avg. Train Sample Loss: 0.0070, Avg. Validate Sample Loss: 0.0017,                             L2 Loss: 0.0014\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [692/5000] -------------------- \n",
      "Epoch [692/5000], Step [1/47], Loss: 0.0025\n",
      "Epoch [692/5000], Step [9/47], Loss: 0.0015\n",
      "Epoch [692/5000], Step [18/47], Loss: 0.0008\n",
      "Epoch [692/5000], Step [27/47], Loss: 0.0046\n",
      "Epoch [692/5000], Step [36/47], Loss: 0.0036\n",
      "Epoch [692/5000], Step [45/47], Loss: 0.0019\n",
      "Epoch [692/5000], Avg. Train Sample Loss: 0.0072, Avg. Validate Sample Loss: 0.0062,                             L2 Loss: 0.0109\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [693/5000] -------------------- \n",
      "Epoch [693/5000], Step [1/47], Loss: 0.0052\n",
      "Epoch [693/5000], Step [9/47], Loss: 0.0024\n",
      "Epoch [693/5000], Step [18/47], Loss: 0.0071\n",
      "Epoch [693/5000], Step [27/47], Loss: 0.0063\n",
      "Epoch [693/5000], Step [36/47], Loss: 0.0106\n",
      "Epoch [693/5000], Step [45/47], Loss: 0.0036\n",
      "Epoch [693/5000], Avg. Train Sample Loss: 0.0045, Avg. Validate Sample Loss: 0.0082,                             L2 Loss: 0.0125\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [694/5000] -------------------- \n",
      "Epoch [694/5000], Step [1/47], Loss: 0.0073\n",
      "Epoch [694/5000], Step [9/47], Loss: 0.0064\n",
      "Epoch [694/5000], Step [18/47], Loss: 0.0023\n",
      "Epoch [694/5000], Step [27/47], Loss: 0.0013\n",
      "Epoch [694/5000], Step [36/47], Loss: 0.0025\n",
      "Epoch [694/5000], Step [45/47], Loss: 0.0007\n",
      "Epoch [694/5000], Avg. Train Sample Loss: 0.0027, Avg. Validate Sample Loss: 0.0014,                             L2 Loss: 0.0020\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [695/5000] -------------------- \n",
      "Epoch [695/5000], Step [1/47], Loss: 0.0008\n",
      "Epoch [695/5000], Step [9/47], Loss: 0.0012\n",
      "Epoch [695/5000], Step [18/47], Loss: 0.0032\n",
      "Epoch [695/5000], Step [27/47], Loss: 0.0019\n",
      "Epoch [695/5000], Step [36/47], Loss: 0.0034\n",
      "Epoch [695/5000], Step [45/47], Loss: 0.0019\n",
      "Epoch [695/5000], Avg. Train Sample Loss: 0.0027, Avg. Validate Sample Loss: 0.0014,                             L2 Loss: 0.0022\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [696/5000] -------------------- \n",
      "Epoch [696/5000], Step [1/47], Loss: 0.0013\n",
      "Epoch [696/5000], Step [9/47], Loss: 0.0014\n",
      "Epoch [696/5000], Step [18/47], Loss: 0.0049\n",
      "Epoch [696/5000], Step [27/47], Loss: 0.0017\n",
      "Epoch [696/5000], Step [36/47], Loss: 0.0019\n",
      "Epoch [696/5000], Step [45/47], Loss: 0.0019\n",
      "Epoch [696/5000], Avg. Train Sample Loss: 0.0037, Avg. Validate Sample Loss: 0.0059,                             L2 Loss: 0.0100\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [697/5000] -------------------- \n",
      "Epoch [697/5000], Step [1/47], Loss: 0.0042\n",
      "Epoch [697/5000], Step [9/47], Loss: 0.0053\n",
      "Epoch [697/5000], Step [18/47], Loss: 0.0032\n",
      "Epoch [697/5000], Step [27/47], Loss: 0.0018\n",
      "Epoch [697/5000], Step [36/47], Loss: 0.0028\n",
      "Epoch [697/5000], Step [45/47], Loss: 0.0015\n",
      "Epoch [697/5000], Avg. Train Sample Loss: 0.0041, Avg. Validate Sample Loss: 0.0014,                             L2 Loss: 0.0021\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [698/5000] -------------------- \n",
      "Epoch [698/5000], Step [1/47], Loss: 0.0015\n",
      "Epoch [698/5000], Step [9/47], Loss: 0.0085\n",
      "Epoch [698/5000], Step [18/47], Loss: 0.0096\n",
      "Epoch [698/5000], Step [27/47], Loss: 0.0028\n",
      "Epoch [698/5000], Step [36/47], Loss: 0.0036\n",
      "Epoch [698/5000], Step [45/47], Loss: 0.0015\n",
      "Epoch [698/5000], Avg. Train Sample Loss: 0.0036, Avg. Validate Sample Loss: 0.0024,                             L2 Loss: 0.0053\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [699/5000] -------------------- \n",
      "Epoch [699/5000], Step [1/47], Loss: 0.0020\n",
      "Epoch [699/5000], Step [9/47], Loss: 0.0011\n",
      "Epoch [699/5000], Step [18/47], Loss: 0.0015\n",
      "Epoch [699/5000], Step [27/47], Loss: 0.0023\n",
      "Epoch [699/5000], Step [36/47], Loss: 0.0043\n",
      "Epoch [699/5000], Step [45/47], Loss: 0.0055\n",
      "Epoch [699/5000], Avg. Train Sample Loss: 0.0027, Avg. Validate Sample Loss: 0.0043,                             L2 Loss: 0.0088\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [700/5000] -------------------- \n",
      "Epoch [700/5000], Step [1/47], Loss: 0.0037\n",
      "Epoch [700/5000], Step [9/47], Loss: 0.0051\n",
      "Epoch [700/5000], Step [18/47], Loss: 0.0022\n",
      "Epoch [700/5000], Step [27/47], Loss: 0.0227\n",
      "Epoch [700/5000], Step [36/47], Loss: 0.0020\n",
      "Epoch [700/5000], Step [45/47], Loss: 0.0032\n",
      "Epoch [700/5000], Avg. Train Sample Loss: 0.0061, Avg. Validate Sample Loss: 0.0016,                             L2 Loss: 0.0023\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [701/5000] -------------------- \n",
      "Epoch [701/5000], Step [1/47], Loss: 0.0015\n",
      "Epoch [701/5000], Step [9/47], Loss: 0.0019\n",
      "Epoch [701/5000], Step [18/47], Loss: 0.0026\n",
      "Epoch [701/5000], Step [27/47], Loss: 0.0136\n",
      "Epoch [701/5000], Step [36/47], Loss: 0.0012\n",
      "Epoch [701/5000], Step [45/47], Loss: 0.0040\n",
      "Epoch [701/5000], Avg. Train Sample Loss: 0.0036, Avg. Validate Sample Loss: 0.0171,                             L2 Loss: 0.0190\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [702/5000] -------------------- \n",
      "Epoch [702/5000], Step [1/47], Loss: 0.0191\n",
      "Epoch [702/5000], Step [9/47], Loss: 0.0081\n",
      "Epoch [702/5000], Step [18/47], Loss: 0.0286\n",
      "Epoch [702/5000], Step [27/47], Loss: 0.0012\n",
      "Epoch [702/5000], Step [36/47], Loss: 0.0024\n",
      "Epoch [702/5000], Step [45/47], Loss: 0.0017\n",
      "Epoch [702/5000], Avg. Train Sample Loss: 0.0083, Avg. Validate Sample Loss: 0.0013,                             L2 Loss: 0.0029\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [703/5000] -------------------- \n",
      "Epoch [703/5000], Step [1/47], Loss: 0.0018\n",
      "Epoch [703/5000], Step [9/47], Loss: 0.0027\n",
      "Epoch [703/5000], Step [18/47], Loss: 0.0031\n",
      "Epoch [703/5000], Step [27/47], Loss: 0.0041\n",
      "Epoch [703/5000], Step [36/47], Loss: 0.0035\n",
      "Epoch [703/5000], Step [45/47], Loss: 0.0037\n",
      "Epoch [703/5000], Avg. Train Sample Loss: 0.0033, Avg. Validate Sample Loss: 0.0024,                             L2 Loss: 0.0054\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [704/5000] -------------------- \n",
      "Epoch [704/5000], Step [1/47], Loss: 0.0027\n",
      "Epoch [704/5000], Step [9/47], Loss: 0.0011\n",
      "Epoch [704/5000], Step [18/47], Loss: 0.0017\n",
      "Epoch [704/5000], Step [27/47], Loss: 0.0017\n",
      "Epoch [704/5000], Step [36/47], Loss: 0.0013\n",
      "Epoch [704/5000], Step [45/47], Loss: 0.0047\n",
      "Epoch [704/5000], Avg. Train Sample Loss: 0.0020, Avg. Validate Sample Loss: 0.0015,                             L2 Loss: 0.0022\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [705/5000] -------------------- \n",
      "Epoch [705/5000], Step [1/47], Loss: 0.0017\n",
      "Epoch [705/5000], Step [9/47], Loss: 0.0016\n",
      "Epoch [705/5000], Step [18/47], Loss: 0.0130\n",
      "Epoch [705/5000], Step [27/47], Loss: 0.0050\n",
      "Epoch [705/5000], Step [36/47], Loss: 0.0014\n",
      "Epoch [705/5000], Step [45/47], Loss: 0.0032\n",
      "Epoch [705/5000], Avg. Train Sample Loss: 0.0076, Avg. Validate Sample Loss: 0.0166,                             L2 Loss: 0.0187\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [706/5000] -------------------- \n",
      "Epoch [706/5000], Step [1/47], Loss: 0.0148\n",
      "Epoch [706/5000], Step [9/47], Loss: 0.0053\n",
      "Epoch [706/5000], Step [18/47], Loss: 0.0028\n",
      "Epoch [706/5000], Step [27/47], Loss: 0.0103\n",
      "Epoch [706/5000], Step [36/47], Loss: 0.0098\n",
      "Epoch [706/5000], Step [45/47], Loss: 0.0038\n",
      "Epoch [706/5000], Avg. Train Sample Loss: 0.0103, Avg. Validate Sample Loss: 0.0169,                             L2 Loss: 0.0190\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [707/5000] -------------------- \n",
      "Epoch [707/5000], Step [1/47], Loss: 0.0176\n",
      "Epoch [707/5000], Step [9/47], Loss: 0.0089\n",
      "Epoch [707/5000], Step [18/47], Loss: 0.0018\n",
      "Epoch [707/5000], Step [27/47], Loss: 0.0036\n",
      "Epoch [707/5000], Step [36/47], Loss: 0.0163\n",
      "Epoch [707/5000], Step [45/47], Loss: 0.0124\n",
      "Epoch [707/5000], Avg. Train Sample Loss: 0.0061, Avg. Validate Sample Loss: 0.0092,                             L2 Loss: 0.0140\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [708/5000] -------------------- \n",
      "Epoch [708/5000], Step [1/47], Loss: 0.0085\n",
      "Epoch [708/5000], Step [9/47], Loss: 0.0086\n",
      "Epoch [708/5000], Step [18/47], Loss: 0.0024\n",
      "Epoch [708/5000], Step [27/47], Loss: 0.0065\n",
      "Epoch [708/5000], Step [36/47], Loss: 0.0021\n",
      "Epoch [708/5000], Step [45/47], Loss: 0.0110\n",
      "Epoch [708/5000], Avg. Train Sample Loss: 0.0052, Avg. Validate Sample Loss: 0.0157,                             L2 Loss: 0.0190\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [709/5000] -------------------- \n",
      "Epoch [709/5000], Step [1/47], Loss: 0.0130\n",
      "Epoch [709/5000], Step [9/47], Loss: 0.0008\n",
      "Epoch [709/5000], Step [18/47], Loss: 0.0011\n",
      "Epoch [709/5000], Step [27/47], Loss: 0.0028\n",
      "Epoch [709/5000], Step [36/47], Loss: 0.0022\n",
      "Epoch [709/5000], Step [45/47], Loss: 0.0030\n",
      "Epoch [709/5000], Avg. Train Sample Loss: 0.0042, Avg. Validate Sample Loss: 0.0157,                             L2 Loss: 0.0184\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [710/5000] -------------------- \n",
      "Epoch [710/5000], Step [1/47], Loss: 0.0160\n",
      "Epoch [710/5000], Step [9/47], Loss: 0.0033\n",
      "Epoch [710/5000], Step [18/47], Loss: 0.0060\n",
      "Epoch [710/5000], Step [27/47], Loss: 0.0011\n",
      "Epoch [710/5000], Step [36/47], Loss: 0.0017\n",
      "Epoch [710/5000], Step [45/47], Loss: 0.0010\n",
      "Epoch [710/5000], Avg. Train Sample Loss: 0.0037, Avg. Validate Sample Loss: 0.0013,                             L2 Loss: 0.0021\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [711/5000] -------------------- \n",
      "Epoch [711/5000], Step [1/47], Loss: 0.0008\n",
      "Epoch [711/5000], Step [9/47], Loss: 0.0016\n",
      "Epoch [711/5000], Step [18/47], Loss: 0.0027\n",
      "Epoch [711/5000], Step [27/47], Loss: 0.0149\n",
      "Epoch [711/5000], Step [36/47], Loss: 0.0220\n",
      "Epoch [711/5000], Step [45/47], Loss: 0.0086\n",
      "Epoch [711/5000], Avg. Train Sample Loss: 0.0116, Avg. Validate Sample Loss: 0.0112,                             L2 Loss: 0.0164\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [712/5000] -------------------- \n",
      "Epoch [712/5000], Step [1/47], Loss: 0.0130\n",
      "Epoch [712/5000], Step [9/47], Loss: 0.0505\n",
      "Epoch [712/5000], Step [18/47], Loss: 0.0557\n",
      "Epoch [712/5000], Step [27/47], Loss: 0.0207\n",
      "Epoch [712/5000], Step [36/47], Loss: 0.0117\n",
      "Epoch [712/5000], Step [45/47], Loss: 0.0046\n",
      "Epoch [712/5000], Avg. Train Sample Loss: 0.0185, Avg. Validate Sample Loss: 0.0043,                             L2 Loss: 0.0110\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [713/5000] -------------------- \n",
      "Epoch [713/5000], Step [1/47], Loss: 0.0049\n",
      "Epoch [713/5000], Step [9/47], Loss: 0.0019\n",
      "Epoch [713/5000], Step [18/47], Loss: 0.0071\n",
      "Epoch [713/5000], Step [27/47], Loss: 0.0016\n",
      "Epoch [713/5000], Step [36/47], Loss: 0.0013\n",
      "Epoch [713/5000], Step [45/47], Loss: 0.0028\n",
      "Epoch [713/5000], Avg. Train Sample Loss: 0.0032, Avg. Validate Sample Loss: 0.0053,                             L2 Loss: 0.0095\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [714/5000] -------------------- \n",
      "Epoch [714/5000], Step [1/47], Loss: 0.0048\n",
      "Epoch [714/5000], Step [9/47], Loss: 0.0187\n",
      "Epoch [714/5000], Step [18/47], Loss: 0.0039\n",
      "Epoch [714/5000], Step [27/47], Loss: 0.0045\n",
      "Epoch [714/5000], Step [36/47], Loss: 0.0009\n",
      "Epoch [714/5000], Step [45/47], Loss: 0.0015\n",
      "Epoch [714/5000], Avg. Train Sample Loss: 0.0039, Avg. Validate Sample Loss: 0.0030,                             L2 Loss: 0.0066\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [715/5000] -------------------- \n",
      "Epoch [715/5000], Step [1/47], Loss: 0.0036\n",
      "Epoch [715/5000], Step [9/47], Loss: 0.0030\n",
      "Epoch [715/5000], Step [18/47], Loss: 0.0026\n",
      "Epoch [715/5000], Step [27/47], Loss: 0.0012\n",
      "Epoch [715/5000], Step [36/47], Loss: 0.0041\n",
      "Epoch [715/5000], Step [45/47], Loss: 0.0108\n",
      "Epoch [715/5000], Avg. Train Sample Loss: 0.0051, Avg. Validate Sample Loss: 0.0071,                             L2 Loss: 0.0119\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [716/5000] -------------------- \n",
      "Epoch [716/5000], Step [1/47], Loss: 0.0080\n",
      "Epoch [716/5000], Step [9/47], Loss: 0.0024\n",
      "Epoch [716/5000], Step [18/47], Loss: 0.0125\n",
      "Epoch [716/5000], Step [27/47], Loss: 0.0162\n",
      "Epoch [716/5000], Step [36/47], Loss: 0.0339\n",
      "Epoch [716/5000], Step [45/47], Loss: 0.0118\n",
      "Epoch [716/5000], Avg. Train Sample Loss: 0.0096, Avg. Validate Sample Loss: 0.0120,                             L2 Loss: 0.0158\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [717/5000] -------------------- \n",
      "Epoch [717/5000], Step [1/47], Loss: 0.0112\n",
      "Epoch [717/5000], Step [9/47], Loss: 0.0087\n",
      "Epoch [717/5000], Step [18/47], Loss: 0.0018\n",
      "Epoch [717/5000], Step [27/47], Loss: 0.0015\n",
      "Epoch [717/5000], Step [36/47], Loss: 0.0012\n",
      "Epoch [717/5000], Step [45/47], Loss: 0.0017\n",
      "Epoch [717/5000], Avg. Train Sample Loss: 0.0028, Avg. Validate Sample Loss: 0.0012,                             L2 Loss: 0.0015\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [718/5000] -------------------- \n",
      "Epoch [718/5000], Step [1/47], Loss: 0.0010\n",
      "Epoch [718/5000], Step [9/47], Loss: 0.0012\n",
      "Epoch [718/5000], Step [18/47], Loss: 0.0018\n",
      "Epoch [718/5000], Step [27/47], Loss: 0.0013\n",
      "Epoch [718/5000], Step [36/47], Loss: 0.0018\n",
      "Epoch [718/5000], Step [45/47], Loss: 0.0015\n",
      "Epoch [718/5000], Avg. Train Sample Loss: 0.0014, Avg. Validate Sample Loss: 0.0012,                             L2 Loss: 0.0013\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [719/5000] -------------------- \n",
      "Epoch [719/5000], Step [1/47], Loss: 0.0015\n",
      "Epoch [719/5000], Step [9/47], Loss: 0.0029\n",
      "Epoch [719/5000], Step [18/47], Loss: 0.0035\n",
      "Epoch [719/5000], Step [27/47], Loss: 0.0040\n",
      "Epoch [719/5000], Step [36/47], Loss: 0.0014\n",
      "Epoch [719/5000], Step [45/47], Loss: 0.0012\n",
      "Epoch [719/5000], Avg. Train Sample Loss: 0.0029, Avg. Validate Sample Loss: 0.0015,                             L2 Loss: 0.0028\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [720/5000] -------------------- \n",
      "Epoch [720/5000], Step [1/47], Loss: 0.0012\n",
      "Epoch [720/5000], Step [9/47], Loss: 0.0017\n",
      "Epoch [720/5000], Step [18/47], Loss: 0.0018\n",
      "Epoch [720/5000], Step [27/47], Loss: 0.0084\n",
      "Epoch [720/5000], Step [36/47], Loss: 0.0055\n",
      "Epoch [720/5000], Step [45/47], Loss: 0.0110\n",
      "Epoch [720/5000], Avg. Train Sample Loss: 0.0054, Avg. Validate Sample Loss: 0.0263,                             L2 Loss: 0.0242\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [721/5000] -------------------- \n",
      "Epoch [721/5000], Step [1/47], Loss: 0.0266\n",
      "Epoch [721/5000], Step [9/47], Loss: 0.0257\n",
      "Epoch [721/5000], Step [18/47], Loss: 0.0020\n",
      "Epoch [721/5000], Step [27/47], Loss: 0.0016\n",
      "Epoch [721/5000], Step [36/47], Loss: 0.0061\n",
      "Epoch [721/5000], Step [45/47], Loss: 0.0051\n",
      "Epoch [721/5000], Avg. Train Sample Loss: 0.0065, Avg. Validate Sample Loss: 0.0053,                             L2 Loss: 0.0103\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [722/5000] -------------------- \n",
      "Epoch [722/5000], Step [1/47], Loss: 0.0034\n",
      "Epoch [722/5000], Step [9/47], Loss: 0.0011\n",
      "Epoch [722/5000], Step [18/47], Loss: 0.0012\n",
      "Epoch [722/5000], Step [27/47], Loss: 0.0139\n",
      "Epoch [722/5000], Step [36/47], Loss: 0.0068\n",
      "Epoch [722/5000], Step [45/47], Loss: 0.0070\n",
      "Epoch [722/5000], Avg. Train Sample Loss: 0.0056, Avg. Validate Sample Loss: 0.0086,                             L2 Loss: 0.0131\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [723/5000] -------------------- \n",
      "Epoch [723/5000], Step [1/47], Loss: 0.0079\n",
      "Epoch [723/5000], Step [9/47], Loss: 0.0024\n",
      "Epoch [723/5000], Step [18/47], Loss: 0.0033\n",
      "Epoch [723/5000], Step [27/47], Loss: 0.0023\n",
      "Epoch [723/5000], Step [36/47], Loss: 0.0021\n",
      "Epoch [723/5000], Step [45/47], Loss: 0.0012\n",
      "Epoch [723/5000], Avg. Train Sample Loss: 0.0035, Avg. Validate Sample Loss: 0.0017,                             L2 Loss: 0.0034\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [724/5000] -------------------- \n",
      "Epoch [724/5000], Step [1/47], Loss: 0.0009\n",
      "Epoch [724/5000], Step [9/47], Loss: 0.0057\n",
      "Epoch [724/5000], Step [18/47], Loss: 0.0019\n",
      "Epoch [724/5000], Step [27/47], Loss: 0.0038\n",
      "Epoch [724/5000], Step [36/47], Loss: 0.0010\n",
      "Epoch [724/5000], Step [45/47], Loss: 0.0259\n",
      "Epoch [724/5000], Avg. Train Sample Loss: 0.0057, Avg. Validate Sample Loss: 0.0164,                             L2 Loss: 0.0190\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [725/5000] -------------------- \n",
      "Epoch [725/5000], Step [1/47], Loss: 0.0152\n",
      "Epoch [725/5000], Step [9/47], Loss: 0.0240\n",
      "Epoch [725/5000], Step [18/47], Loss: 0.0264\n",
      "Epoch [725/5000], Step [27/47], Loss: 0.0091\n",
      "Epoch [725/5000], Step [36/47], Loss: 0.0050\n",
      "Epoch [725/5000], Step [45/47], Loss: 0.0184\n",
      "Epoch [725/5000], Avg. Train Sample Loss: 0.0105, Avg. Validate Sample Loss: 0.0315,                             L2 Loss: 0.0273\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [726/5000] -------------------- \n",
      "Epoch [726/5000], Step [1/47], Loss: 0.0298\n",
      "Epoch [726/5000], Step [9/47], Loss: 0.0030\n",
      "Epoch [726/5000], Step [18/47], Loss: 0.0011\n",
      "Epoch [726/5000], Step [27/47], Loss: 0.0019\n",
      "Epoch [726/5000], Step [36/47], Loss: 0.0022\n",
      "Epoch [726/5000], Step [45/47], Loss: 0.0009\n",
      "Epoch [726/5000], Avg. Train Sample Loss: 0.0029, Avg. Validate Sample Loss: 0.0014,                             L2 Loss: 0.0026\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [727/5000] -------------------- \n",
      "Epoch [727/5000], Step [1/47], Loss: 0.0014\n",
      "Epoch [727/5000], Step [9/47], Loss: 0.0015\n",
      "Epoch [727/5000], Step [18/47], Loss: 0.0090\n",
      "Epoch [727/5000], Step [27/47], Loss: 0.0026\n",
      "Epoch [727/5000], Step [36/47], Loss: 0.0016\n",
      "Epoch [727/5000], Step [45/47], Loss: 0.0034\n",
      "Epoch [727/5000], Avg. Train Sample Loss: 0.0040, Avg. Validate Sample Loss: 0.0039,                             L2 Loss: 0.0075\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [728/5000] -------------------- \n",
      "Epoch [728/5000], Step [1/47], Loss: 0.0040\n",
      "Epoch [728/5000], Step [9/47], Loss: 0.0027\n",
      "Epoch [728/5000], Step [18/47], Loss: 0.0026\n",
      "Epoch [728/5000], Step [27/47], Loss: 0.0012\n",
      "Epoch [728/5000], Step [36/47], Loss: 0.0020\n",
      "Epoch [728/5000], Step [45/47], Loss: 0.0012\n",
      "Epoch [728/5000], Avg. Train Sample Loss: 0.0028, Avg. Validate Sample Loss: 0.0020,                             L2 Loss: 0.0047\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [729/5000] -------------------- \n",
      "Epoch [729/5000], Step [1/47], Loss: 0.0016\n",
      "Epoch [729/5000], Step [9/47], Loss: 0.0031\n",
      "Epoch [729/5000], Step [18/47], Loss: 0.0010\n",
      "Epoch [729/5000], Step [27/47], Loss: 0.0023\n",
      "Epoch [729/5000], Step [36/47], Loss: 0.0114\n",
      "Epoch [729/5000], Step [45/47], Loss: 0.0173\n",
      "Epoch [729/5000], Avg. Train Sample Loss: 0.0050, Avg. Validate Sample Loss: 0.0101,                             L2 Loss: 0.0132\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [730/5000] -------------------- \n",
      "Epoch [730/5000], Step [1/47], Loss: 0.0092\n",
      "Epoch [730/5000], Step [9/47], Loss: 0.0025\n",
      "Epoch [730/5000], Step [18/47], Loss: 0.0055\n",
      "Epoch [730/5000], Step [27/47], Loss: 0.0014\n",
      "Epoch [730/5000], Step [36/47], Loss: 0.0012\n",
      "Epoch [730/5000], Step [45/47], Loss: 0.0024\n",
      "Epoch [730/5000], Avg. Train Sample Loss: 0.0028, Avg. Validate Sample Loss: 0.0028,                             L2 Loss: 0.0061\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [731/5000] -------------------- \n",
      "Epoch [731/5000], Step [1/47], Loss: 0.0030\n",
      "Epoch [731/5000], Step [9/47], Loss: 0.0045\n",
      "Epoch [731/5000], Step [18/47], Loss: 0.0039\n",
      "Epoch [731/5000], Step [27/47], Loss: 0.0042\n",
      "Epoch [731/5000], Step [36/47], Loss: 0.0033\n",
      "Epoch [731/5000], Step [45/47], Loss: 0.0010\n",
      "Epoch [731/5000], Avg. Train Sample Loss: 0.0033, Avg. Validate Sample Loss: 0.0018,                             L2 Loss: 0.0033\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [732/5000] -------------------- \n",
      "Epoch [732/5000], Step [1/47], Loss: 0.0015\n",
      "Epoch [732/5000], Step [9/47], Loss: 0.0030\n",
      "Epoch [732/5000], Step [18/47], Loss: 0.0072\n",
      "Epoch [732/5000], Step [27/47], Loss: 0.0018\n",
      "Epoch [732/5000], Step [36/47], Loss: 0.0383\n",
      "Epoch [732/5000], Step [45/47], Loss: 0.0024\n",
      "Epoch [732/5000], Avg. Train Sample Loss: 0.0102, Avg. Validate Sample Loss: 0.0018,                             L2 Loss: 0.0036\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [733/5000] -------------------- \n",
      "Epoch [733/5000], Step [1/47], Loss: 0.0023\n",
      "Epoch [733/5000], Step [9/47], Loss: 0.0020\n",
      "Epoch [733/5000], Step [18/47], Loss: 0.0102\n",
      "Epoch [733/5000], Step [27/47], Loss: 0.0018\n",
      "Epoch [733/5000], Step [36/47], Loss: 0.0018\n",
      "Epoch [733/5000], Step [45/47], Loss: 0.0036\n",
      "Epoch [733/5000], Avg. Train Sample Loss: 0.0030, Avg. Validate Sample Loss: 0.0015,                             L2 Loss: 0.0029\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [734/5000] -------------------- \n",
      "Epoch [734/5000], Step [1/47], Loss: 0.0016\n",
      "Epoch [734/5000], Step [9/47], Loss: 0.0069\n",
      "Epoch [734/5000], Step [18/47], Loss: 0.0048\n",
      "Epoch [734/5000], Step [27/47], Loss: 0.0013\n",
      "Epoch [734/5000], Step [36/47], Loss: 0.0084\n",
      "Epoch [734/5000], Step [45/47], Loss: 0.0037\n",
      "Epoch [734/5000], Avg. Train Sample Loss: 0.0034, Avg. Validate Sample Loss: 0.0017,                             L2 Loss: 0.0038\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [735/5000] -------------------- \n",
      "Epoch [735/5000], Step [1/47], Loss: 0.0010\n",
      "Epoch [735/5000], Step [9/47], Loss: 0.0103\n",
      "Epoch [735/5000], Step [18/47], Loss: 0.0016\n",
      "Epoch [735/5000], Step [27/47], Loss: 0.0021\n",
      "Epoch [735/5000], Step [36/47], Loss: 0.0017\n",
      "Epoch [735/5000], Step [45/47], Loss: 0.0020\n",
      "Epoch [735/5000], Avg. Train Sample Loss: 0.0029, Avg. Validate Sample Loss: 0.0016,                             L2 Loss: 0.0029\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [736/5000] -------------------- \n",
      "Epoch [736/5000], Step [1/47], Loss: 0.0020\n",
      "Epoch [736/5000], Step [9/47], Loss: 0.0022\n",
      "Epoch [736/5000], Step [18/47], Loss: 0.0007\n",
      "Epoch [736/5000], Step [27/47], Loss: 0.0066\n",
      "Epoch [736/5000], Step [36/47], Loss: 0.0021\n",
      "Epoch [736/5000], Step [45/47], Loss: 0.0032\n",
      "Epoch [736/5000], Avg. Train Sample Loss: 0.0033, Avg. Validate Sample Loss: 0.0015,                             L2 Loss: 0.0021\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [737/5000] -------------------- \n",
      "Epoch [737/5000], Step [1/47], Loss: 0.0011\n",
      "Epoch [737/5000], Step [9/47], Loss: 0.0018\n",
      "Epoch [737/5000], Step [18/47], Loss: 0.0028\n",
      "Epoch [737/5000], Step [27/47], Loss: 0.0027\n",
      "Epoch [737/5000], Step [36/47], Loss: 0.0025\n",
      "Epoch [737/5000], Step [45/47], Loss: 0.0047\n",
      "Epoch [737/5000], Avg. Train Sample Loss: 0.0044, Avg. Validate Sample Loss: 0.0089,                             L2 Loss: 0.0131\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [738/5000] -------------------- \n",
      "Epoch [738/5000], Step [1/47], Loss: 0.0070\n",
      "Epoch [738/5000], Step [9/47], Loss: 0.0019\n",
      "Epoch [738/5000], Step [18/47], Loss: 0.0029\n",
      "Epoch [738/5000], Step [27/47], Loss: 0.0010\n",
      "Epoch [738/5000], Step [36/47], Loss: 0.0045\n",
      "Epoch [738/5000], Step [45/47], Loss: 0.0010\n",
      "Epoch [738/5000], Avg. Train Sample Loss: 0.0067, Avg. Validate Sample Loss: 0.0015,                             L2 Loss: 0.0028\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [739/5000] -------------------- \n",
      "Epoch [739/5000], Step [1/47], Loss: 0.0016\n",
      "Epoch [739/5000], Step [9/47], Loss: 0.0123\n",
      "Epoch [739/5000], Step [18/47], Loss: 0.0474\n",
      "Epoch [739/5000], Step [27/47], Loss: 0.0447\n",
      "Epoch [739/5000], Step [36/47], Loss: 0.0376\n",
      "Epoch [739/5000], Step [45/47], Loss: 0.0712\n",
      "Epoch [739/5000], Avg. Train Sample Loss: 0.0281, Avg. Validate Sample Loss: 0.0588,                             L2 Loss: 0.0365\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [740/5000] -------------------- \n",
      "Epoch [740/5000], Step [1/47], Loss: 0.0462\n",
      "Epoch [740/5000], Step [9/47], Loss: 0.0447\n",
      "Epoch [740/5000], Step [18/47], Loss: 0.0085\n",
      "Epoch [740/5000], Step [27/47], Loss: 0.0027\n",
      "Epoch [740/5000], Step [36/47], Loss: 0.0035\n",
      "Epoch [740/5000], Step [45/47], Loss: 0.0018\n",
      "Epoch [740/5000], Avg. Train Sample Loss: 0.0131, Avg. Validate Sample Loss: 0.0012,                             L2 Loss: 0.0021\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [741/5000] -------------------- \n",
      "Epoch [741/5000], Step [1/47], Loss: 0.0011\n",
      "Epoch [741/5000], Step [9/47], Loss: 0.0072\n",
      "Epoch [741/5000], Step [18/47], Loss: 0.0017\n",
      "Epoch [741/5000], Step [27/47], Loss: 0.0116\n",
      "Epoch [741/5000], Step [36/47], Loss: 0.0204\n",
      "Epoch [741/5000], Step [45/47], Loss: 0.0026\n",
      "Epoch [741/5000], Avg. Train Sample Loss: 0.0081, Avg. Validate Sample Loss: 0.0027,                             L2 Loss: 0.0060\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [742/5000] -------------------- \n",
      "Epoch [742/5000], Step [1/47], Loss: 0.0026\n",
      "Epoch [742/5000], Step [9/47], Loss: 0.0035\n",
      "Epoch [742/5000], Step [18/47], Loss: 0.0023\n",
      "Epoch [742/5000], Step [27/47], Loss: 0.0018\n",
      "Epoch [742/5000], Step [36/47], Loss: 0.0032\n",
      "Epoch [742/5000], Step [45/47], Loss: 0.0014\n",
      "Epoch [742/5000], Avg. Train Sample Loss: 0.0031, Avg. Validate Sample Loss: 0.0014,                             L2 Loss: 0.0028\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [743/5000] -------------------- \n",
      "Epoch [743/5000], Step [1/47], Loss: 0.0015\n",
      "Epoch [743/5000], Step [9/47], Loss: 0.0021\n",
      "Epoch [743/5000], Step [18/47], Loss: 0.0047\n",
      "Epoch [743/5000], Step [27/47], Loss: 0.0014\n",
      "Epoch [743/5000], Step [36/47], Loss: 0.0043\n",
      "Epoch [743/5000], Step [45/47], Loss: 0.0010\n",
      "Epoch [743/5000], Avg. Train Sample Loss: 0.0022, Avg. Validate Sample Loss: 0.0013,                             L2 Loss: 0.0017\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [744/5000] -------------------- \n",
      "Epoch [744/5000], Step [1/47], Loss: 0.0016\n",
      "Epoch [744/5000], Step [9/47], Loss: 0.0012\n",
      "Epoch [744/5000], Step [18/47], Loss: 0.0011\n",
      "Epoch [744/5000], Step [27/47], Loss: 0.0021\n",
      "Epoch [744/5000], Step [36/47], Loss: 0.0043\n",
      "Epoch [744/5000], Step [45/47], Loss: 0.0025\n",
      "Epoch [744/5000], Avg. Train Sample Loss: 0.0037, Avg. Validate Sample Loss: 0.0032,                             L2 Loss: 0.0064\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [745/5000] -------------------- \n",
      "Epoch [745/5000], Step [1/47], Loss: 0.0037\n",
      "Epoch [745/5000], Step [9/47], Loss: 0.0090\n",
      "Epoch [745/5000], Step [18/47], Loss: 0.0066\n",
      "Epoch [745/5000], Step [27/47], Loss: 0.0019\n",
      "Epoch [745/5000], Step [36/47], Loss: 0.0047\n",
      "Epoch [745/5000], Step [45/47], Loss: 0.0193\n",
      "Epoch [745/5000], Avg. Train Sample Loss: 0.0054, Avg. Validate Sample Loss: 0.0225,                             L2 Loss: 0.0223\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [746/5000] -------------------- \n",
      "Epoch [746/5000], Step [1/47], Loss: 0.0221\n",
      "Epoch [746/5000], Step [9/47], Loss: 0.0024\n",
      "Epoch [746/5000], Step [18/47], Loss: 0.0188\n",
      "Epoch [746/5000], Step [27/47], Loss: 0.0135\n",
      "Epoch [746/5000], Step [36/47], Loss: 0.0437\n",
      "Epoch [746/5000], Step [45/47], Loss: 0.0093\n",
      "Epoch [746/5000], Avg. Train Sample Loss: 0.0147, Avg. Validate Sample Loss: 0.0033,                             L2 Loss: 0.0044\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [747/5000] -------------------- \n",
      "Epoch [747/5000], Step [1/47], Loss: 0.0023\n",
      "Epoch [747/5000], Step [9/47], Loss: 0.0077\n",
      "Epoch [747/5000], Step [18/47], Loss: 0.0062\n",
      "Epoch [747/5000], Step [27/47], Loss: 0.0047\n",
      "Epoch [747/5000], Step [36/47], Loss: 0.0012\n",
      "Epoch [747/5000], Step [45/47], Loss: 0.0008\n",
      "Epoch [747/5000], Avg. Train Sample Loss: 0.0033, Avg. Validate Sample Loss: 0.0012,                             L2 Loss: 0.0027\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [748/5000] -------------------- \n",
      "Epoch [748/5000], Step [1/47], Loss: 0.0009\n",
      "Epoch [748/5000], Step [9/47], Loss: 0.0022\n",
      "Epoch [748/5000], Step [18/47], Loss: 0.0018\n",
      "Epoch [748/5000], Step [27/47], Loss: 0.0016\n",
      "Epoch [748/5000], Step [36/47], Loss: 0.0023\n",
      "Epoch [748/5000], Step [45/47], Loss: 0.0008\n",
      "Epoch [748/5000], Avg. Train Sample Loss: 0.0015, Avg. Validate Sample Loss: 0.0011,                             L2 Loss: 0.0013\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [749/5000] -------------------- \n",
      "Epoch [749/5000], Step [1/47], Loss: 0.0010\n",
      "Epoch [749/5000], Step [9/47], Loss: 0.0030\n",
      "Epoch [749/5000], Step [18/47], Loss: 0.0060\n",
      "Epoch [749/5000], Step [27/47], Loss: 0.0006\n",
      "Epoch [749/5000], Step [36/47], Loss: 0.0007\n",
      "Epoch [749/5000], Step [45/47], Loss: 0.0015\n",
      "Epoch [749/5000], Avg. Train Sample Loss: 0.0023, Avg. Validate Sample Loss: 0.0013,                             L2 Loss: 0.0021\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [750/5000] -------------------- \n",
      "Epoch [750/5000], Step [1/47], Loss: 0.0012\n",
      "Epoch [750/5000], Step [9/47], Loss: 0.0017\n",
      "Epoch [750/5000], Step [18/47], Loss: 0.0016\n",
      "Epoch [750/5000], Step [27/47], Loss: 0.0034\n",
      "Epoch [750/5000], Step [36/47], Loss: 0.0030\n",
      "Epoch [750/5000], Step [45/47], Loss: 0.0024\n",
      "Epoch [750/5000], Avg. Train Sample Loss: 0.0018, Avg. Validate Sample Loss: 0.0015,                             L2 Loss: 0.0033\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [751/5000] -------------------- \n",
      "Epoch [751/5000], Step [1/47], Loss: 0.0020\n",
      "Epoch [751/5000], Step [9/47], Loss: 0.0126\n",
      "Epoch [751/5000], Step [18/47], Loss: 0.0032\n",
      "Epoch [751/5000], Step [27/47], Loss: 0.0048\n",
      "Epoch [751/5000], Step [36/47], Loss: 0.0011\n",
      "Epoch [751/5000], Step [45/47], Loss: 0.0009\n",
      "Epoch [751/5000], Avg. Train Sample Loss: 0.0025, Avg. Validate Sample Loss: 0.0017,                             L2 Loss: 0.0022\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [752/5000] -------------------- \n",
      "Epoch [752/5000], Step [1/47], Loss: 0.0008\n",
      "Epoch [752/5000], Step [9/47], Loss: 0.0011\n",
      "Epoch [752/5000], Step [18/47], Loss: 0.0024\n",
      "Epoch [752/5000], Step [27/47], Loss: 0.0091\n",
      "Epoch [752/5000], Step [36/47], Loss: 0.0013\n",
      "Epoch [752/5000], Step [45/47], Loss: 0.0053\n",
      "Epoch [752/5000], Avg. Train Sample Loss: 0.0051, Avg. Validate Sample Loss: 0.0091,                             L2 Loss: 0.0133\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [753/5000] -------------------- \n",
      "Epoch [753/5000], Step [1/47], Loss: 0.0093\n",
      "Epoch [753/5000], Step [9/47], Loss: 0.0041\n",
      "Epoch [753/5000], Step [18/47], Loss: 0.0016\n",
      "Epoch [753/5000], Step [27/47], Loss: 0.0039\n",
      "Epoch [753/5000], Step [36/47], Loss: 0.0045\n",
      "Epoch [753/5000], Step [45/47], Loss: 0.0094\n",
      "Epoch [753/5000], Avg. Train Sample Loss: 0.0190, Avg. Validate Sample Loss: 0.0096,                             L2 Loss: 0.0135\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [754/5000] -------------------- \n",
      "Epoch [754/5000], Step [1/47], Loss: 0.0098\n",
      "Epoch [754/5000], Step [9/47], Loss: 0.0018\n",
      "Epoch [754/5000], Step [18/47], Loss: 0.0022\n",
      "Epoch [754/5000], Step [27/47], Loss: 0.0016\n",
      "Epoch [754/5000], Step [36/47], Loss: 0.0010\n",
      "Epoch [754/5000], Step [45/47], Loss: 0.0017\n",
      "Epoch [754/5000], Avg. Train Sample Loss: 0.0027, Avg. Validate Sample Loss: 0.0012,                             L2 Loss: 0.0010\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [755/5000] -------------------- \n",
      "Epoch [755/5000], Step [1/47], Loss: 0.0011\n",
      "Epoch [755/5000], Step [9/47], Loss: 0.0009\n",
      "Epoch [755/5000], Step [18/47], Loss: 0.0039\n",
      "Epoch [755/5000], Step [27/47], Loss: 0.0015\n",
      "Epoch [755/5000], Step [36/47], Loss: 0.0016\n",
      "Epoch [755/5000], Step [45/47], Loss: 0.0020\n",
      "Epoch [755/5000], Avg. Train Sample Loss: 0.0023, Avg. Validate Sample Loss: 0.0028,                             L2 Loss: 0.0060\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [756/5000] -------------------- \n",
      "Epoch [756/5000], Step [1/47], Loss: 0.0021\n",
      "Epoch [756/5000], Step [9/47], Loss: 0.0016\n",
      "Epoch [756/5000], Step [18/47], Loss: 0.0031\n",
      "Epoch [756/5000], Step [27/47], Loss: 0.0033\n",
      "Epoch [756/5000], Step [36/47], Loss: 0.0009\n",
      "Epoch [756/5000], Step [45/47], Loss: 0.0009\n",
      "Epoch [756/5000], Avg. Train Sample Loss: 0.0040, Avg. Validate Sample Loss: 0.0019,                             L2 Loss: 0.0047\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [757/5000] -------------------- \n",
      "Epoch [757/5000], Step [1/47], Loss: 0.0018\n",
      "Epoch [757/5000], Step [9/47], Loss: 0.0014\n",
      "Epoch [757/5000], Step [18/47], Loss: 0.0021\n",
      "Epoch [757/5000], Step [27/47], Loss: 0.0015\n",
      "Epoch [757/5000], Step [36/47], Loss: 0.0011\n",
      "Epoch [757/5000], Step [45/47], Loss: 0.0022\n",
      "Epoch [757/5000], Avg. Train Sample Loss: 0.0023, Avg. Validate Sample Loss: 0.0106,                             L2 Loss: 0.0147\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [758/5000] -------------------- \n",
      "Epoch [758/5000], Step [1/47], Loss: 0.0094\n",
      "Epoch [758/5000], Step [9/47], Loss: 0.0079\n",
      "Epoch [758/5000], Step [18/47], Loss: 0.0037\n",
      "Epoch [758/5000], Step [27/47], Loss: 0.0013\n",
      "Epoch [758/5000], Step [36/47], Loss: 0.0018\n",
      "Epoch [758/5000], Step [45/47], Loss: 0.0009\n",
      "Epoch [758/5000], Avg. Train Sample Loss: 0.0034, Avg. Validate Sample Loss: 0.0021,                             L2 Loss: 0.0039\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [759/5000] -------------------- \n",
      "Epoch [759/5000], Step [1/47], Loss: 0.0012\n",
      "Epoch [759/5000], Step [9/47], Loss: 0.0010\n",
      "Epoch [759/5000], Step [18/47], Loss: 0.0007\n",
      "Epoch [759/5000], Step [27/47], Loss: 0.0017\n",
      "Epoch [759/5000], Step [36/47], Loss: 0.0024\n",
      "Epoch [759/5000], Step [45/47], Loss: 0.0018\n",
      "Epoch [759/5000], Avg. Train Sample Loss: 0.0019, Avg. Validate Sample Loss: 0.0025,                             L2 Loss: 0.0054\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [760/5000] -------------------- \n",
      "Epoch [760/5000], Step [1/47], Loss: 0.0023\n",
      "Epoch [760/5000], Step [9/47], Loss: 0.0044\n",
      "Epoch [760/5000], Step [18/47], Loss: 0.0024\n",
      "Epoch [760/5000], Step [27/47], Loss: 0.0031\n",
      "Epoch [760/5000], Step [36/47], Loss: 0.0018\n",
      "Epoch [760/5000], Step [45/47], Loss: 0.0020\n",
      "Epoch [760/5000], Avg. Train Sample Loss: 0.0030, Avg. Validate Sample Loss: 0.0012,                             L2 Loss: 0.0020\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [761/5000] -------------------- \n",
      "Epoch [761/5000], Step [1/47], Loss: 0.0008\n",
      "Epoch [761/5000], Step [9/47], Loss: 0.0009\n",
      "Epoch [761/5000], Step [18/47], Loss: 0.0010\n",
      "Epoch [761/5000], Step [27/47], Loss: 0.0017\n",
      "Epoch [761/5000], Step [36/47], Loss: 0.0017\n",
      "Epoch [761/5000], Step [45/47], Loss: 0.0014\n",
      "Epoch [761/5000], Avg. Train Sample Loss: 0.0020, Avg. Validate Sample Loss: 0.0012,                             L2 Loss: 0.0014\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [762/5000] -------------------- \n",
      "Epoch [762/5000], Step [1/47], Loss: 0.0012\n",
      "Epoch [762/5000], Step [9/47], Loss: 0.0007\n",
      "Epoch [762/5000], Step [18/47], Loss: 0.0062\n",
      "Epoch [762/5000], Step [27/47], Loss: 0.0097\n",
      "Epoch [762/5000], Step [36/47], Loss: 0.0045\n",
      "Epoch [762/5000], Step [45/47], Loss: 0.0021\n",
      "Epoch [762/5000], Avg. Train Sample Loss: 0.0033, Avg. Validate Sample Loss: 0.0034,                             L2 Loss: 0.0076\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [763/5000] -------------------- \n",
      "Epoch [763/5000], Step [1/47], Loss: 0.0036\n",
      "Epoch [763/5000], Step [9/47], Loss: 0.0009\n",
      "Epoch [763/5000], Step [18/47], Loss: 0.0039\n",
      "Epoch [763/5000], Step [27/47], Loss: 0.0015\n",
      "Epoch [763/5000], Step [36/47], Loss: 0.0092\n",
      "Epoch [763/5000], Step [45/47], Loss: 0.0026\n",
      "Epoch [763/5000], Avg. Train Sample Loss: 0.0033, Avg. Validate Sample Loss: 0.0015,                             L2 Loss: 0.0036\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [764/5000] -------------------- \n",
      "Epoch [764/5000], Step [1/47], Loss: 0.0023\n",
      "Epoch [764/5000], Step [9/47], Loss: 0.0014\n",
      "Epoch [764/5000], Step [18/47], Loss: 0.0057\n",
      "Epoch [764/5000], Step [27/47], Loss: 0.0018\n",
      "Epoch [764/5000], Step [36/47], Loss: 0.0014\n",
      "Epoch [764/5000], Step [45/47], Loss: 0.0016\n",
      "Epoch [764/5000], Avg. Train Sample Loss: 0.0019, Avg. Validate Sample Loss: 0.0018,                             L2 Loss: 0.0041\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [765/5000] -------------------- \n",
      "Epoch [765/5000], Step [1/47], Loss: 0.0016\n",
      "Epoch [765/5000], Step [9/47], Loss: 0.0008\n",
      "Epoch [765/5000], Step [18/47], Loss: 0.0015\n",
      "Epoch [765/5000], Step [27/47], Loss: 0.0024\n",
      "Epoch [765/5000], Step [36/47], Loss: 0.0009\n",
      "Epoch [765/5000], Step [45/47], Loss: 0.0017\n",
      "Epoch [765/5000], Avg. Train Sample Loss: 0.0021, Avg. Validate Sample Loss: 0.0018,                             L2 Loss: 0.0040\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [766/5000] -------------------- \n",
      "Epoch [766/5000], Step [1/47], Loss: 0.0019\n",
      "Epoch [766/5000], Step [9/47], Loss: 0.0016\n",
      "Epoch [766/5000], Step [18/47], Loss: 0.0085\n",
      "Epoch [766/5000], Step [27/47], Loss: 0.0072\n",
      "Epoch [766/5000], Step [36/47], Loss: 0.0027\n",
      "Epoch [766/5000], Step [45/47], Loss: 0.0012\n",
      "Epoch [766/5000], Avg. Train Sample Loss: 0.0045, Avg. Validate Sample Loss: 0.0073,                             L2 Loss: 0.0121\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [767/5000] -------------------- \n",
      "Epoch [767/5000], Step [1/47], Loss: 0.0070\n",
      "Epoch [767/5000], Step [9/47], Loss: 0.0080\n",
      "Epoch [767/5000], Step [18/47], Loss: 0.0042\n",
      "Epoch [767/5000], Step [27/47], Loss: 0.0056\n",
      "Epoch [767/5000], Step [36/47], Loss: 0.0130\n",
      "Epoch [767/5000], Step [45/47], Loss: 0.0010\n",
      "Epoch [767/5000], Avg. Train Sample Loss: 0.0130, Avg. Validate Sample Loss: 0.0017,                             L2 Loss: 0.0053\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [768/5000] -------------------- \n",
      "Epoch [768/5000], Step [1/47], Loss: 0.0010\n",
      "Epoch [768/5000], Step [9/47], Loss: 0.0604\n",
      "Epoch [768/5000], Step [18/47], Loss: 0.0427\n",
      "Epoch [768/5000], Step [27/47], Loss: 0.0011\n",
      "Epoch [768/5000], Step [36/47], Loss: 0.0021\n",
      "Epoch [768/5000], Step [45/47], Loss: 0.0032\n",
      "Epoch [768/5000], Avg. Train Sample Loss: 0.0132, Avg. Validate Sample Loss: 0.0017,                             L2 Loss: 0.0032\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [769/5000] -------------------- \n",
      "Epoch [769/5000], Step [1/47], Loss: 0.0018\n",
      "Epoch [769/5000], Step [9/47], Loss: 0.0119\n",
      "Epoch [769/5000], Step [18/47], Loss: 0.0019\n",
      "Epoch [769/5000], Step [27/47], Loss: 0.0009\n",
      "Epoch [769/5000], Step [36/47], Loss: 0.0087\n",
      "Epoch [769/5000], Step [45/47], Loss: 0.0021\n",
      "Epoch [769/5000], Avg. Train Sample Loss: 0.0051, Avg. Validate Sample Loss: 0.0011,                             L2 Loss: 0.0032\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [770/5000] -------------------- \n",
      "Epoch [770/5000], Step [1/47], Loss: 0.0008\n",
      "Epoch [770/5000], Step [9/47], Loss: 0.0106\n",
      "Epoch [770/5000], Step [18/47], Loss: 0.0027\n",
      "Epoch [770/5000], Step [27/47], Loss: 0.0052\n",
      "Epoch [770/5000], Step [36/47], Loss: 0.0018\n",
      "Epoch [770/5000], Step [45/47], Loss: 0.0013\n",
      "Epoch [770/5000], Avg. Train Sample Loss: 0.0063, Avg. Validate Sample Loss: 0.0053,                             L2 Loss: 0.0098\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [771/5000] -------------------- \n",
      "Epoch [771/5000], Step [1/47], Loss: 0.0049\n",
      "Epoch [771/5000], Step [9/47], Loss: 0.0018\n",
      "Epoch [771/5000], Step [18/47], Loss: 0.0054\n",
      "Epoch [771/5000], Step [27/47], Loss: 0.0039\n",
      "Epoch [771/5000], Step [36/47], Loss: 0.0017\n",
      "Epoch [771/5000], Step [45/47], Loss: 0.0009\n",
      "Epoch [771/5000], Avg. Train Sample Loss: 0.0023, Avg. Validate Sample Loss: 0.0013,                             L2 Loss: 0.0029\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [772/5000] -------------------- \n",
      "Epoch [772/5000], Step [1/47], Loss: 0.0014\n",
      "Epoch [772/5000], Step [9/47], Loss: 0.0076\n",
      "Epoch [772/5000], Step [18/47], Loss: 0.0032\n",
      "Epoch [772/5000], Step [27/47], Loss: 0.0065\n",
      "Epoch [772/5000], Step [36/47], Loss: 0.0012\n",
      "Epoch [772/5000], Step [45/47], Loss: 0.0015\n",
      "Epoch [772/5000], Avg. Train Sample Loss: 0.0031, Avg. Validate Sample Loss: 0.0028,                             L2 Loss: 0.0058\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [773/5000] -------------------- \n",
      "Epoch [773/5000], Step [1/47], Loss: 0.0021\n",
      "Epoch [773/5000], Step [9/47], Loss: 0.0009\n",
      "Epoch [773/5000], Step [18/47], Loss: 0.0009\n",
      "Epoch [773/5000], Step [27/47], Loss: 0.0015\n",
      "Epoch [773/5000], Step [36/47], Loss: 0.0083\n",
      "Epoch [773/5000], Step [45/47], Loss: 0.0152\n",
      "Epoch [773/5000], Avg. Train Sample Loss: 0.0042, Avg. Validate Sample Loss: 0.0140,                             L2 Loss: 0.0175\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [774/5000] -------------------- \n",
      "Epoch [774/5000], Step [1/47], Loss: 0.0144\n",
      "Epoch [774/5000], Step [9/47], Loss: 0.0015\n",
      "Epoch [774/5000], Step [18/47], Loss: 0.0009\n",
      "Epoch [774/5000], Step [27/47], Loss: 0.0014\n",
      "Epoch [774/5000], Step [36/47], Loss: 0.0039\n",
      "Epoch [774/5000], Step [45/47], Loss: 0.0015\n",
      "Epoch [774/5000], Avg. Train Sample Loss: 0.0027, Avg. Validate Sample Loss: 0.0026,                             L2 Loss: 0.0063\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [775/5000] -------------------- \n",
      "Epoch [775/5000], Step [1/47], Loss: 0.0026\n",
      "Epoch [775/5000], Step [9/47], Loss: 0.0016\n",
      "Epoch [775/5000], Step [18/47], Loss: 0.0013\n",
      "Epoch [775/5000], Step [27/47], Loss: 0.0012\n",
      "Epoch [775/5000], Step [36/47], Loss: 0.0037\n",
      "Epoch [775/5000], Step [45/47], Loss: 0.0037\n",
      "Epoch [775/5000], Avg. Train Sample Loss: 0.0017, Avg. Validate Sample Loss: 0.0014,                             L2 Loss: 0.0024\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [776/5000] -------------------- \n",
      "Epoch [776/5000], Step [1/47], Loss: 0.0009\n",
      "Epoch [776/5000], Step [9/47], Loss: 0.0115\n",
      "Epoch [776/5000], Step [18/47], Loss: 0.0344\n",
      "Epoch [776/5000], Step [27/47], Loss: 0.0189\n",
      "Epoch [776/5000], Step [36/47], Loss: 0.0016\n",
      "Epoch [776/5000], Step [45/47], Loss: 0.0081\n",
      "Epoch [776/5000], Avg. Train Sample Loss: 0.0106, Avg. Validate Sample Loss: 0.0014,                             L2 Loss: 0.0033\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [777/5000] -------------------- \n",
      "Epoch [777/5000], Step [1/47], Loss: 0.0010\n",
      "Epoch [777/5000], Step [9/47], Loss: 0.0012\n",
      "Epoch [777/5000], Step [18/47], Loss: 0.0009\n",
      "Epoch [777/5000], Step [27/47], Loss: 0.0026\n",
      "Epoch [777/5000], Step [36/47], Loss: 0.0029\n",
      "Epoch [777/5000], Step [45/47], Loss: 0.0049\n",
      "Epoch [777/5000], Avg. Train Sample Loss: 0.0027, Avg. Validate Sample Loss: 0.0021,                             L2 Loss: 0.0056\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [778/5000] -------------------- \n",
      "Epoch [778/5000], Step [1/47], Loss: 0.0018\n",
      "Epoch [778/5000], Step [9/47], Loss: 0.0041\n",
      "Epoch [778/5000], Step [18/47], Loss: 0.0051\n",
      "Epoch [778/5000], Step [27/47], Loss: 0.0188\n",
      "Epoch [778/5000], Step [36/47], Loss: 0.0094\n",
      "Epoch [778/5000], Step [45/47], Loss: 0.0015\n",
      "Epoch [778/5000], Avg. Train Sample Loss: 0.0062, Avg. Validate Sample Loss: 0.0041,                             L2 Loss: 0.0084\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [779/5000] -------------------- \n",
      "Epoch [779/5000], Step [1/47], Loss: 0.0040\n",
      "Epoch [779/5000], Step [9/47], Loss: 0.0015\n",
      "Epoch [779/5000], Step [18/47], Loss: 0.0018\n",
      "Epoch [779/5000], Step [27/47], Loss: 0.0012\n",
      "Epoch [779/5000], Step [36/47], Loss: 0.0252\n",
      "Epoch [779/5000], Step [45/47], Loss: 0.0054\n",
      "Epoch [779/5000], Avg. Train Sample Loss: 0.0062, Avg. Validate Sample Loss: 0.0083,                             L2 Loss: 0.0120\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [780/5000] -------------------- \n",
      "Epoch [780/5000], Step [1/47], Loss: 0.0078\n",
      "Epoch [780/5000], Step [9/47], Loss: 0.0128\n",
      "Epoch [780/5000], Step [18/47], Loss: 0.0012\n",
      "Epoch [780/5000], Step [27/47], Loss: 0.0028\n",
      "Epoch [780/5000], Step [36/47], Loss: 0.0009\n",
      "Epoch [780/5000], Step [45/47], Loss: 0.0009\n",
      "Epoch [780/5000], Avg. Train Sample Loss: 0.0034, Avg. Validate Sample Loss: 0.0026,                             L2 Loss: 0.0062\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [781/5000] -------------------- \n",
      "Epoch [781/5000], Step [1/47], Loss: 0.0029\n",
      "Epoch [781/5000], Step [9/47], Loss: 0.0047\n",
      "Epoch [781/5000], Step [18/47], Loss: 0.0010\n",
      "Epoch [781/5000], Step [27/47], Loss: 0.0009\n",
      "Epoch [781/5000], Step [36/47], Loss: 0.0043\n",
      "Epoch [781/5000], Step [45/47], Loss: 0.0028\n",
      "Epoch [781/5000], Avg. Train Sample Loss: 0.0038, Avg. Validate Sample Loss: 0.0013,                             L2 Loss: 0.0029\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [782/5000] -------------------- \n",
      "Epoch [782/5000], Step [1/47], Loss: 0.0011\n",
      "Epoch [782/5000], Step [9/47], Loss: 0.0019\n",
      "Epoch [782/5000], Step [18/47], Loss: 0.0022\n",
      "Epoch [782/5000], Step [27/47], Loss: 0.0015\n",
      "Epoch [782/5000], Step [36/47], Loss: 0.0015\n",
      "Epoch [782/5000], Step [45/47], Loss: 0.0019\n",
      "Epoch [782/5000], Avg. Train Sample Loss: 0.0022, Avg. Validate Sample Loss: 0.0025,                             L2 Loss: 0.0055\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [783/5000] -------------------- \n",
      "Epoch [783/5000], Step [1/47], Loss: 0.0029\n",
      "Epoch [783/5000], Step [9/47], Loss: 0.0014\n",
      "Epoch [783/5000], Step [18/47], Loss: 0.0028\n",
      "Epoch [783/5000], Step [27/47], Loss: 0.0024\n",
      "Epoch [783/5000], Step [36/47], Loss: 0.0037\n",
      "Epoch [783/5000], Step [45/47], Loss: 0.0174\n",
      "Epoch [783/5000], Avg. Train Sample Loss: 0.0038, Avg. Validate Sample Loss: 0.0039,                             L2 Loss: 0.0080\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [784/5000] -------------------- \n",
      "Epoch [784/5000], Step [1/47], Loss: 0.0040\n",
      "Epoch [784/5000], Step [9/47], Loss: 0.0010\n",
      "Epoch [784/5000], Step [18/47], Loss: 0.0124\n",
      "Epoch [784/5000], Step [27/47], Loss: 0.0011\n",
      "Epoch [784/5000], Step [36/47], Loss: 0.0011\n",
      "Epoch [784/5000], Step [45/47], Loss: 0.0012\n",
      "Epoch [784/5000], Avg. Train Sample Loss: 0.0028, Avg. Validate Sample Loss: 0.0012,                             L2 Loss: 0.0013\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [785/5000] -------------------- \n",
      "Epoch [785/5000], Step [1/47], Loss: 0.0009\n",
      "Epoch [785/5000], Step [9/47], Loss: 0.0025\n",
      "Epoch [785/5000], Step [18/47], Loss: 0.0095\n",
      "Epoch [785/5000], Step [27/47], Loss: 0.3097\n",
      "Epoch [785/5000], Step [36/47], Loss: 0.0620\n",
      "Epoch [785/5000], Step [45/47], Loss: 0.0195\n",
      "Epoch [785/5000], Avg. Train Sample Loss: 0.0408, Avg. Validate Sample Loss: 0.0265,                             L2 Loss: 0.0250\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [786/5000] -------------------- \n",
      "Epoch [786/5000], Step [1/47], Loss: 0.0290\n",
      "Epoch [786/5000], Step [9/47], Loss: 0.0304\n",
      "Epoch [786/5000], Step [18/47], Loss: 0.0192\n",
      "Epoch [786/5000], Step [27/47], Loss: 0.0114\n",
      "Epoch [786/5000], Step [36/47], Loss: 0.0024\n",
      "Epoch [786/5000], Step [45/47], Loss: 0.0038\n",
      "Epoch [786/5000], Avg. Train Sample Loss: 0.0155, Avg. Validate Sample Loss: 0.0020,                             L2 Loss: 0.0047\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [787/5000] -------------------- \n",
      "Epoch [787/5000], Step [1/47], Loss: 0.0013\n",
      "Epoch [787/5000], Step [9/47], Loss: 0.0011\n",
      "Epoch [787/5000], Step [18/47], Loss: 0.0019\n",
      "Epoch [787/5000], Step [27/47], Loss: 0.0009\n",
      "Epoch [787/5000], Step [36/47], Loss: 0.0012\n",
      "Epoch [787/5000], Step [45/47], Loss: 0.0012\n",
      "Epoch [787/5000], Avg. Train Sample Loss: 0.0015, Avg. Validate Sample Loss: 0.0009,                             L2 Loss: 0.0013\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [788/5000] -------------------- \n",
      "Epoch [788/5000], Step [1/47], Loss: 0.0009\n",
      "Epoch [788/5000], Step [9/47], Loss: 0.0011\n",
      "Epoch [788/5000], Step [18/47], Loss: 0.0034\n",
      "Epoch [788/5000], Step [27/47], Loss: 0.0017\n",
      "Epoch [788/5000], Step [36/47], Loss: 0.0063\n",
      "Epoch [788/5000], Step [45/47], Loss: 0.0014\n",
      "Epoch [788/5000], Avg. Train Sample Loss: 0.0031, Avg. Validate Sample Loss: 0.0020,                             L2 Loss: 0.0045\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [789/5000] -------------------- \n",
      "Epoch [789/5000], Step [1/47], Loss: 0.0014\n",
      "Epoch [789/5000], Step [9/47], Loss: 0.0014\n",
      "Epoch [789/5000], Step [18/47], Loss: 0.0018\n",
      "Epoch [789/5000], Step [27/47], Loss: 0.0008\n",
      "Epoch [789/5000], Step [36/47], Loss: 0.0011\n",
      "Epoch [789/5000], Step [45/47], Loss: 0.0008\n",
      "Epoch [789/5000], Avg. Train Sample Loss: 0.0018, Avg. Validate Sample Loss: 0.0032,                             L2 Loss: 0.0071\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [790/5000] -------------------- \n",
      "Epoch [790/5000], Step [1/47], Loss: 0.0032\n",
      "Epoch [790/5000], Step [9/47], Loss: 0.0024\n",
      "Epoch [790/5000], Step [18/47], Loss: 0.0031\n",
      "Epoch [790/5000], Step [27/47], Loss: 0.0014\n",
      "Epoch [790/5000], Step [36/47], Loss: 0.0007\n",
      "Epoch [790/5000], Step [45/47], Loss: 0.0013\n",
      "Epoch [790/5000], Avg. Train Sample Loss: 0.0019, Avg. Validate Sample Loss: 0.0018,                             L2 Loss: 0.0048\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [791/5000] -------------------- \n",
      "Epoch [791/5000], Step [1/47], Loss: 0.0032\n",
      "Epoch [791/5000], Step [9/47], Loss: 0.0033\n",
      "Epoch [791/5000], Step [18/47], Loss: 0.0029\n",
      "Epoch [791/5000], Step [27/47], Loss: 0.0040\n",
      "Epoch [791/5000], Step [36/47], Loss: 0.0020\n",
      "Epoch [791/5000], Step [45/47], Loss: 0.0009\n",
      "Epoch [791/5000], Avg. Train Sample Loss: 0.0038, Avg. Validate Sample Loss: 0.0012,                             L2 Loss: 0.0021\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [792/5000] -------------------- \n",
      "Epoch [792/5000], Step [1/47], Loss: 0.0008\n",
      "Epoch [792/5000], Step [9/47], Loss: 0.0014\n",
      "Epoch [792/5000], Step [18/47], Loss: 0.0090\n",
      "Epoch [792/5000], Step [27/47], Loss: 0.0030\n",
      "Epoch [792/5000], Step [36/47], Loss: 0.0011\n",
      "Epoch [792/5000], Step [45/47], Loss: 0.0039\n",
      "Epoch [792/5000], Avg. Train Sample Loss: 0.0034, Avg. Validate Sample Loss: 0.0019,                             L2 Loss: 0.0055\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [793/5000] -------------------- \n",
      "Epoch [793/5000], Step [1/47], Loss: 0.0016\n",
      "Epoch [793/5000], Step [9/47], Loss: 0.0011\n",
      "Epoch [793/5000], Step [18/47], Loss: 0.0078\n",
      "Epoch [793/5000], Step [27/47], Loss: 0.0060\n",
      "Epoch [793/5000], Step [36/47], Loss: 0.0012\n",
      "Epoch [793/5000], Step [45/47], Loss: 0.0018\n",
      "Epoch [793/5000], Avg. Train Sample Loss: 0.0026, Avg. Validate Sample Loss: 0.0026,                             L2 Loss: 0.0053\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [794/5000] -------------------- \n",
      "Epoch [794/5000], Step [1/47], Loss: 0.0020\n",
      "Epoch [794/5000], Step [9/47], Loss: 0.0025\n",
      "Epoch [794/5000], Step [18/47], Loss: 0.0032\n",
      "Epoch [794/5000], Step [27/47], Loss: 0.0043\n",
      "Epoch [794/5000], Step [36/47], Loss: 0.0016\n",
      "Epoch [794/5000], Step [45/47], Loss: 0.0015\n",
      "Epoch [794/5000], Avg. Train Sample Loss: 0.0033, Avg. Validate Sample Loss: 0.0022,                             L2 Loss: 0.0059\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [795/5000] -------------------- \n",
      "Epoch [795/5000], Step [1/47], Loss: 0.0018\n",
      "Epoch [795/5000], Step [9/47], Loss: 0.0028\n",
      "Epoch [795/5000], Step [18/47], Loss: 0.0028\n",
      "Epoch [795/5000], Step [27/47], Loss: 0.0029\n",
      "Epoch [795/5000], Step [36/47], Loss: 0.0012\n",
      "Epoch [795/5000], Step [45/47], Loss: 0.0010\n",
      "Epoch [795/5000], Avg. Train Sample Loss: 0.0022, Avg. Validate Sample Loss: 0.0026,                             L2 Loss: 0.0064\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [796/5000] -------------------- \n",
      "Epoch [796/5000], Step [1/47], Loss: 0.0026\n",
      "Epoch [796/5000], Step [9/47], Loss: 0.0017\n",
      "Epoch [796/5000], Step [18/47], Loss: 0.0011\n",
      "Epoch [796/5000], Step [27/47], Loss: 0.0021\n",
      "Epoch [796/5000], Step [36/47], Loss: 0.0014\n",
      "Epoch [796/5000], Step [45/47], Loss: 0.0067\n",
      "Epoch [796/5000], Avg. Train Sample Loss: 0.0028, Avg. Validate Sample Loss: 0.0030,                             L2 Loss: 0.0070\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [797/5000] -------------------- \n",
      "Epoch [797/5000], Step [1/47], Loss: 0.0034\n",
      "Epoch [797/5000], Step [9/47], Loss: 0.0026\n",
      "Epoch [797/5000], Step [18/47], Loss: 0.0013\n",
      "Epoch [797/5000], Step [27/47], Loss: 0.0079\n",
      "Epoch [797/5000], Step [36/47], Loss: 0.0063\n",
      "Epoch [797/5000], Step [45/47], Loss: 0.0035\n",
      "Epoch [797/5000], Avg. Train Sample Loss: 0.0026, Avg. Validate Sample Loss: 0.0024,                             L2 Loss: 0.0061\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [798/5000] -------------------- \n",
      "Epoch [798/5000], Step [1/47], Loss: 0.0021\n",
      "Epoch [798/5000], Step [9/47], Loss: 0.0019\n",
      "Epoch [798/5000], Step [18/47], Loss: 0.0011\n",
      "Epoch [798/5000], Step [27/47], Loss: 0.0030\n",
      "Epoch [798/5000], Step [36/47], Loss: 0.0017\n",
      "Epoch [798/5000], Step [45/47], Loss: 0.0023\n",
      "Epoch [798/5000], Avg. Train Sample Loss: 0.0019, Avg. Validate Sample Loss: 0.0022,                             L2 Loss: 0.0055\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [799/5000] -------------------- \n",
      "Epoch [799/5000], Step [1/47], Loss: 0.0018\n",
      "Epoch [799/5000], Step [9/47], Loss: 0.0011\n",
      "Epoch [799/5000], Step [18/47], Loss: 0.0011\n",
      "Epoch [799/5000], Step [27/47], Loss: 0.0009\n",
      "Epoch [799/5000], Step [36/47], Loss: 0.0013\n",
      "Epoch [799/5000], Step [45/47], Loss: 0.0074\n",
      "Epoch [799/5000], Avg. Train Sample Loss: 0.0024, Avg. Validate Sample Loss: 0.0023,                             L2 Loss: 0.0055\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [800/5000] -------------------- \n",
      "Epoch [800/5000], Step [1/47], Loss: 0.0026\n",
      "Epoch [800/5000], Step [9/47], Loss: 0.0028\n",
      "Epoch [800/5000], Step [18/47], Loss: 0.0028\n",
      "Epoch [800/5000], Step [27/47], Loss: 0.0017\n",
      "Epoch [800/5000], Step [36/47], Loss: 0.0015\n",
      "Epoch [800/5000], Step [45/47], Loss: 0.0146\n",
      "Epoch [800/5000], Avg. Train Sample Loss: 0.0066, Avg. Validate Sample Loss: 0.0053,                             L2 Loss: 0.0098\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [801/5000] -------------------- \n",
      "Epoch [801/5000], Step [1/47], Loss: 0.0055\n",
      "Epoch [801/5000], Step [9/47], Loss: 0.0011\n",
      "Epoch [801/5000], Step [18/47], Loss: 0.0061\n",
      "Epoch [801/5000], Step [27/47], Loss: 0.0116\n",
      "Epoch [801/5000], Step [36/47], Loss: 0.0018\n",
      "Epoch [801/5000], Step [45/47], Loss: 0.0046\n",
      "Epoch [801/5000], Avg. Train Sample Loss: 0.0081, Avg. Validate Sample Loss: 0.0057,                             L2 Loss: 0.0104\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [802/5000] -------------------- \n",
      "Epoch [802/5000], Step [1/47], Loss: 0.0055\n",
      "Epoch [802/5000], Step [9/47], Loss: 0.0025\n",
      "Epoch [802/5000], Step [18/47], Loss: 0.0124\n",
      "Epoch [802/5000], Step [27/47], Loss: 0.0035\n",
      "Epoch [802/5000], Step [36/47], Loss: 0.0081\n",
      "Epoch [802/5000], Step [45/47], Loss: 0.0060\n",
      "Epoch [802/5000], Avg. Train Sample Loss: 0.0056, Avg. Validate Sample Loss: 0.0132,                             L2 Loss: 0.0169\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [803/5000] -------------------- \n",
      "Epoch [803/5000], Step [1/47], Loss: 0.0122\n",
      "Epoch [803/5000], Step [9/47], Loss: 0.0037\n",
      "Epoch [803/5000], Step [18/47], Loss: 0.0028\n",
      "Epoch [803/5000], Step [27/47], Loss: 0.0023\n",
      "Epoch [803/5000], Step [36/47], Loss: 0.0012\n",
      "Epoch [803/5000], Step [45/47], Loss: 0.0011\n",
      "Epoch [803/5000], Avg. Train Sample Loss: 0.0082, Avg. Validate Sample Loss: 0.0024,                             L2 Loss: 0.0053\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [804/5000] -------------------- \n",
      "Epoch [804/5000], Step [1/47], Loss: 0.0023\n",
      "Epoch [804/5000], Step [9/47], Loss: 0.0015\n",
      "Epoch [804/5000], Step [18/47], Loss: 0.0009\n",
      "Epoch [804/5000], Step [27/47], Loss: 0.0008\n",
      "Epoch [804/5000], Step [36/47], Loss: 0.0014\n",
      "Epoch [804/5000], Step [45/47], Loss: 0.0009\n",
      "Epoch [804/5000], Avg. Train Sample Loss: 0.0022, Avg. Validate Sample Loss: 0.0017,                             L2 Loss: 0.0039\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [805/5000] -------------------- \n",
      "Epoch [805/5000], Step [1/47], Loss: 0.0014\n",
      "Epoch [805/5000], Step [9/47], Loss: 0.0039\n",
      "Epoch [805/5000], Step [18/47], Loss: 0.0029\n",
      "Epoch [805/5000], Step [27/47], Loss: 0.0081\n",
      "Epoch [805/5000], Step [36/47], Loss: 0.0154\n",
      "Epoch [805/5000], Step [45/47], Loss: 0.0258\n",
      "Epoch [805/5000], Avg. Train Sample Loss: 0.0049, Avg. Validate Sample Loss: 0.0188,                             L2 Loss: 0.0205\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [806/5000] -------------------- \n",
      "Epoch [806/5000], Step [1/47], Loss: 0.0227\n",
      "Epoch [806/5000], Step [9/47], Loss: 0.0150\n",
      "Epoch [806/5000], Step [18/47], Loss: 0.0070\n",
      "Epoch [806/5000], Step [27/47], Loss: 0.0008\n",
      "Epoch [806/5000], Step [36/47], Loss: 0.0025\n",
      "Epoch [806/5000], Step [45/47], Loss: 0.0011\n",
      "Epoch [806/5000], Avg. Train Sample Loss: 0.0043, Avg. Validate Sample Loss: 0.0017,                             L2 Loss: 0.0039\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [807/5000] -------------------- \n",
      "Epoch [807/5000], Step [1/47], Loss: 0.0013\n",
      "Epoch [807/5000], Step [9/47], Loss: 0.0017\n",
      "Epoch [807/5000], Step [18/47], Loss: 0.0009\n",
      "Epoch [807/5000], Step [27/47], Loss: 0.0024\n",
      "Epoch [807/5000], Step [36/47], Loss: 0.0019\n",
      "Epoch [807/5000], Step [45/47], Loss: 0.0010\n",
      "Epoch [807/5000], Avg. Train Sample Loss: 0.0018, Avg. Validate Sample Loss: 0.0019,                             L2 Loss: 0.0053\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [808/5000] -------------------- \n",
      "Epoch [808/5000], Step [1/47], Loss: 0.0028\n",
      "Epoch [808/5000], Step [9/47], Loss: 0.0023\n",
      "Epoch [808/5000], Step [18/47], Loss: 0.0010\n",
      "Epoch [808/5000], Step [27/47], Loss: 0.0011\n",
      "Epoch [808/5000], Step [36/47], Loss: 0.0008\n",
      "Epoch [808/5000], Step [45/47], Loss: 0.0021\n",
      "Epoch [808/5000], Avg. Train Sample Loss: 0.0015, Avg. Validate Sample Loss: 0.0024,                             L2 Loss: 0.0059\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [809/5000] -------------------- \n",
      "Epoch [809/5000], Step [1/47], Loss: 0.0032\n",
      "Epoch [809/5000], Step [9/47], Loss: 0.0019\n",
      "Epoch [809/5000], Step [18/47], Loss: 0.0015\n",
      "Epoch [809/5000], Step [27/47], Loss: 0.0009\n",
      "Epoch [809/5000], Step [36/47], Loss: 0.0015\n",
      "Epoch [809/5000], Step [45/47], Loss: 0.0046\n",
      "Epoch [809/5000], Avg. Train Sample Loss: 0.0035, Avg. Validate Sample Loss: 0.0032,                             L2 Loss: 0.0063\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [810/5000] -------------------- \n",
      "Epoch [810/5000], Step [1/47], Loss: 0.0032\n",
      "Epoch [810/5000], Step [9/47], Loss: 0.0061\n",
      "Epoch [810/5000], Step [18/47], Loss: 0.0158\n",
      "Epoch [810/5000], Step [27/47], Loss: 0.0025\n",
      "Epoch [810/5000], Step [36/47], Loss: 0.0042\n",
      "Epoch [810/5000], Step [45/47], Loss: 0.0006\n",
      "Epoch [810/5000], Avg. Train Sample Loss: 0.0032, Avg. Validate Sample Loss: 0.0124,                             L2 Loss: 0.0158\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [811/5000] -------------------- \n",
      "Epoch [811/5000], Step [1/47], Loss: 0.0101\n",
      "Epoch [811/5000], Step [9/47], Loss: 0.0078\n",
      "Epoch [811/5000], Step [18/47], Loss: 0.0017\n",
      "Epoch [811/5000], Step [27/47], Loss: 0.0027\n",
      "Epoch [811/5000], Step [36/47], Loss: 0.0025\n",
      "Epoch [811/5000], Step [45/47], Loss: 0.0121\n",
      "Epoch [811/5000], Avg. Train Sample Loss: 0.0082, Avg. Validate Sample Loss: 0.0126,                             L2 Loss: 0.0166\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [812/5000] -------------------- \n",
      "Epoch [812/5000], Step [1/47], Loss: 0.0132\n",
      "Epoch [812/5000], Step [9/47], Loss: 0.0162\n",
      "Epoch [812/5000], Step [18/47], Loss: 0.0015\n",
      "Epoch [812/5000], Step [27/47], Loss: 0.0027\n",
      "Epoch [812/5000], Step [36/47], Loss: 0.0010\n",
      "Epoch [812/5000], Step [45/47], Loss: 0.0121\n",
      "Epoch [812/5000], Avg. Train Sample Loss: 0.0065, Avg. Validate Sample Loss: 0.0094,                             L2 Loss: 0.0145\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [813/5000] -------------------- \n",
      "Epoch [813/5000], Step [1/47], Loss: 0.0097\n",
      "Epoch [813/5000], Step [9/47], Loss: 0.0007\n",
      "Epoch [813/5000], Step [18/47], Loss: 0.0046\n",
      "Epoch [813/5000], Step [27/47], Loss: 0.0062\n",
      "Epoch [813/5000], Step [36/47], Loss: 0.0013\n",
      "Epoch [813/5000], Step [45/47], Loss: 0.0011\n",
      "Epoch [813/5000], Avg. Train Sample Loss: 0.0034, Avg. Validate Sample Loss: 0.0010,                             L2 Loss: 0.0025\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [814/5000] -------------------- \n",
      "Epoch [814/5000], Step [1/47], Loss: 0.0017\n",
      "Epoch [814/5000], Step [9/47], Loss: 0.0162\n",
      "Epoch [814/5000], Step [18/47], Loss: 0.0159\n",
      "Epoch [814/5000], Step [27/47], Loss: 0.0074\n",
      "Epoch [814/5000], Step [36/47], Loss: 0.0017\n",
      "Epoch [814/5000], Step [45/47], Loss: 0.0043\n",
      "Epoch [814/5000], Avg. Train Sample Loss: 0.0086, Avg. Validate Sample Loss: 0.0029,                             L2 Loss: 0.0070\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [815/5000] -------------------- \n",
      "Epoch [815/5000], Step [1/47], Loss: 0.0024\n",
      "Epoch [815/5000], Step [9/47], Loss: 0.0016\n",
      "Epoch [815/5000], Step [18/47], Loss: 0.0023\n",
      "Epoch [815/5000], Step [27/47], Loss: 0.0023\n",
      "Epoch [815/5000], Step [36/47], Loss: 0.0038\n",
      "Epoch [815/5000], Step [45/47], Loss: 0.0014\n",
      "Epoch [815/5000], Avg. Train Sample Loss: 0.0020, Avg. Validate Sample Loss: 0.0009,                             L2 Loss: 0.0032\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [816/5000] -------------------- \n",
      "Epoch [816/5000], Step [1/47], Loss: 0.0008\n",
      "Epoch [816/5000], Step [9/47], Loss: 0.0026\n",
      "Epoch [816/5000], Step [18/47], Loss: 0.0034\n",
      "Epoch [816/5000], Step [27/47], Loss: 0.0016\n",
      "Epoch [816/5000], Step [36/47], Loss: 0.0047\n",
      "Epoch [816/5000], Step [45/47], Loss: 0.0108\n",
      "Epoch [816/5000], Avg. Train Sample Loss: 0.0022, Avg. Validate Sample Loss: 0.0216,                             L2 Loss: 0.0217\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [817/5000] -------------------- \n",
      "Epoch [817/5000], Step [1/47], Loss: 0.0193\n",
      "Epoch [817/5000], Step [9/47], Loss: 0.0008\n",
      "Epoch [817/5000], Step [18/47], Loss: 0.0033\n",
      "Epoch [817/5000], Step [27/47], Loss: 0.0011\n",
      "Epoch [817/5000], Step [36/47], Loss: 0.0102\n",
      "Epoch [817/5000], Step [45/47], Loss: 0.0029\n",
      "Epoch [817/5000], Avg. Train Sample Loss: 0.0034, Avg. Validate Sample Loss: 0.0047,                             L2 Loss: 0.0089\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [818/5000] -------------------- \n",
      "Epoch [818/5000], Step [1/47], Loss: 0.0041\n",
      "Epoch [818/5000], Step [9/47], Loss: 0.0014\n",
      "Epoch [818/5000], Step [18/47], Loss: 0.0014\n",
      "Epoch [818/5000], Step [27/47], Loss: 0.0011\n",
      "Epoch [818/5000], Step [36/47], Loss: 0.0010\n",
      "Epoch [818/5000], Step [45/47], Loss: 0.0113\n",
      "Epoch [818/5000], Avg. Train Sample Loss: 0.0019, Avg. Validate Sample Loss: 0.0084,                             L2 Loss: 0.0133\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [819/5000] -------------------- \n",
      "Epoch [819/5000], Step [1/47], Loss: 0.0070\n",
      "Epoch [819/5000], Step [9/47], Loss: 0.0013\n",
      "Epoch [819/5000], Step [18/47], Loss: 0.0011\n",
      "Epoch [819/5000], Step [27/47], Loss: 0.0008\n",
      "Epoch [819/5000], Step [36/47], Loss: 0.0016\n",
      "Epoch [819/5000], Step [45/47], Loss: 0.0007\n",
      "Epoch [819/5000], Avg. Train Sample Loss: 0.0021, Avg. Validate Sample Loss: 0.0047,                             L2 Loss: 0.0094\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [820/5000] -------------------- \n",
      "Epoch [820/5000], Step [1/47], Loss: 0.0058\n",
      "Epoch [820/5000], Step [9/47], Loss: 0.0012\n",
      "Epoch [820/5000], Step [18/47], Loss: 0.0109\n",
      "Epoch [820/5000], Step [27/47], Loss: 0.0011\n",
      "Epoch [820/5000], Step [36/47], Loss: 0.0036\n",
      "Epoch [820/5000], Step [45/47], Loss: 0.0010\n",
      "Epoch [820/5000], Avg. Train Sample Loss: 0.0054, Avg. Validate Sample Loss: 0.0023,                             L2 Loss: 0.0067\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [821/5000] -------------------- \n",
      "Epoch [821/5000], Step [1/47], Loss: 0.0026\n",
      "Epoch [821/5000], Step [9/47], Loss: 0.0010\n",
      "Epoch [821/5000], Step [18/47], Loss: 0.0022\n",
      "Epoch [821/5000], Step [27/47], Loss: 0.0110\n",
      "Epoch [821/5000], Step [36/47], Loss: 0.0175\n",
      "Epoch [821/5000], Step [45/47], Loss: 0.0029\n",
      "Epoch [821/5000], Avg. Train Sample Loss: 0.0050, Avg. Validate Sample Loss: 0.0075,                             L2 Loss: 0.0125\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [822/5000] -------------------- \n",
      "Epoch [822/5000], Step [1/47], Loss: 0.0093\n",
      "Epoch [822/5000], Step [9/47], Loss: 0.0521\n",
      "Epoch [822/5000], Step [18/47], Loss: 0.0408\n",
      "Epoch [822/5000], Step [27/47], Loss: 0.0764\n",
      "Epoch [822/5000], Step [36/47], Loss: 0.0198\n",
      "Epoch [822/5000], Step [45/47], Loss: 0.0115\n",
      "Epoch [822/5000], Avg. Train Sample Loss: 0.0303, Avg. Validate Sample Loss: 0.0229,                             L2 Loss: 0.0228\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [823/5000] -------------------- \n",
      "Epoch [823/5000], Step [1/47], Loss: 0.0259\n",
      "Epoch [823/5000], Step [9/47], Loss: 0.0260\n",
      "Epoch [823/5000], Step [18/47], Loss: 0.0296\n",
      "Epoch [823/5000], Step [27/47], Loss: 0.0166\n",
      "Epoch [823/5000], Step [36/47], Loss: 0.0014\n",
      "Epoch [823/5000], Step [45/47], Loss: 0.0030\n",
      "Epoch [823/5000], Avg. Train Sample Loss: 0.0114, Avg. Validate Sample Loss: 0.0010,                             L2 Loss: 0.0017\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [824/5000] -------------------- \n",
      "Epoch [824/5000], Step [1/47], Loss: 0.0007\n",
      "Epoch [824/5000], Step [9/47], Loss: 0.0013\n",
      "Epoch [824/5000], Step [18/47], Loss: 0.0009\n",
      "Epoch [824/5000], Step [27/47], Loss: 0.0020\n",
      "Epoch [824/5000], Step [36/47], Loss: 0.0008\n",
      "Epoch [824/5000], Step [45/47], Loss: 0.0008\n",
      "Epoch [824/5000], Avg. Train Sample Loss: 0.0024, Avg. Validate Sample Loss: 0.0023,                             L2 Loss: 0.0053\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [825/5000] -------------------- \n",
      "Epoch [825/5000], Step [1/47], Loss: 0.0016\n",
      "Epoch [825/5000], Step [9/47], Loss: 0.0196\n",
      "Epoch [825/5000], Step [18/47], Loss: 0.0187\n",
      "Epoch [825/5000], Step [27/47], Loss: 0.0167\n",
      "Epoch [825/5000], Step [36/47], Loss: 0.0118\n",
      "Epoch [825/5000], Step [45/47], Loss: 0.0008\n",
      "Epoch [825/5000], Avg. Train Sample Loss: 0.0083, Avg. Validate Sample Loss: 0.0013,                             L2 Loss: 0.0020\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [826/5000] -------------------- \n",
      "Epoch [826/5000], Step [1/47], Loss: 0.0009\n",
      "Epoch [826/5000], Step [9/47], Loss: 0.0008\n",
      "Epoch [826/5000], Step [18/47], Loss: 0.0012\n",
      "Epoch [826/5000], Step [27/47], Loss: 0.0009\n",
      "Epoch [826/5000], Step [36/47], Loss: 0.0014\n",
      "Epoch [826/5000], Step [45/47], Loss: 0.0009\n",
      "Epoch [826/5000], Avg. Train Sample Loss: 0.0018, Avg. Validate Sample Loss: 0.0058,                             L2 Loss: 0.0104\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [827/5000] -------------------- \n",
      "Epoch [827/5000], Step [1/47], Loss: 0.0064\n",
      "Epoch [827/5000], Step [9/47], Loss: 0.0011\n",
      "Epoch [827/5000], Step [18/47], Loss: 0.0008\n",
      "Epoch [827/5000], Step [27/47], Loss: 0.0008\n",
      "Epoch [827/5000], Step [36/47], Loss: 0.0007\n",
      "Epoch [827/5000], Step [45/47], Loss: 0.0043\n",
      "Epoch [827/5000], Avg. Train Sample Loss: 0.0029, Avg. Validate Sample Loss: 0.0014,                             L2 Loss: 0.0028\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [828/5000] -------------------- \n",
      "Epoch [828/5000], Step [1/47], Loss: 0.0011\n",
      "Epoch [828/5000], Step [9/47], Loss: 0.0014\n",
      "Epoch [828/5000], Step [18/47], Loss: 0.0014\n",
      "Epoch [828/5000], Step [27/47], Loss: 0.0017\n",
      "Epoch [828/5000], Step [36/47], Loss: 0.0052\n",
      "Epoch [828/5000], Step [45/47], Loss: 0.0019\n",
      "Epoch [828/5000], Avg. Train Sample Loss: 0.0034, Avg. Validate Sample Loss: 0.0047,                             L2 Loss: 0.0092\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [829/5000] -------------------- \n",
      "Epoch [829/5000], Step [1/47], Loss: 0.0085\n",
      "Epoch [829/5000], Step [9/47], Loss: 0.0016\n",
      "Epoch [829/5000], Step [18/47], Loss: 0.0009\n",
      "Epoch [829/5000], Step [27/47], Loss: 0.0049\n",
      "Epoch [829/5000], Step [36/47], Loss: 0.0008\n",
      "Epoch [829/5000], Step [45/47], Loss: 0.0050\n",
      "Epoch [829/5000], Avg. Train Sample Loss: 0.0024, Avg. Validate Sample Loss: 0.0085,                             L2 Loss: 0.0136\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [830/5000] -------------------- \n",
      "Epoch [830/5000], Step [1/47], Loss: 0.0069\n",
      "Epoch [830/5000], Step [9/47], Loss: 0.0050\n",
      "Epoch [830/5000], Step [18/47], Loss: 0.0034\n",
      "Epoch [830/5000], Step [27/47], Loss: 0.0025\n",
      "Epoch [830/5000], Step [36/47], Loss: 0.0011\n",
      "Epoch [830/5000], Step [45/47], Loss: 0.0009\n",
      "Epoch [830/5000], Avg. Train Sample Loss: 0.0030, Avg. Validate Sample Loss: 0.0038,                             L2 Loss: 0.0084\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [831/5000] -------------------- \n",
      "Epoch [831/5000], Step [1/47], Loss: 0.0046\n",
      "Epoch [831/5000], Step [9/47], Loss: 0.0015\n",
      "Epoch [831/5000], Step [18/47], Loss: 0.0024\n",
      "Epoch [831/5000], Step [27/47], Loss: 0.0048\n",
      "Epoch [831/5000], Step [36/47], Loss: 0.0032\n",
      "Epoch [831/5000], Step [45/47], Loss: 0.0017\n",
      "Epoch [831/5000], Avg. Train Sample Loss: 0.0032, Avg. Validate Sample Loss: 0.0024,                             L2 Loss: 0.0054\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [832/5000] -------------------- \n",
      "Epoch [832/5000], Step [1/47], Loss: 0.0017\n",
      "Epoch [832/5000], Step [9/47], Loss: 0.0083\n",
      "Epoch [832/5000], Step [18/47], Loss: 0.0106\n",
      "Epoch [832/5000], Step [27/47], Loss: 0.0115\n",
      "Epoch [832/5000], Step [36/47], Loss: 0.0017\n",
      "Epoch [832/5000], Step [45/47], Loss: 0.0063\n",
      "Epoch [832/5000], Avg. Train Sample Loss: 0.0091, Avg. Validate Sample Loss: 0.0170,                             L2 Loss: 0.0195\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [833/5000] -------------------- \n",
      "Epoch [833/5000], Step [1/47], Loss: 0.0187\n",
      "Epoch [833/5000], Step [9/47], Loss: 0.0026\n",
      "Epoch [833/5000], Step [18/47], Loss: 0.0013\n",
      "Epoch [833/5000], Step [27/47], Loss: 0.0006\n",
      "Epoch [833/5000], Step [36/47], Loss: 0.0008\n",
      "Epoch [833/5000], Step [45/47], Loss: 0.0007\n",
      "Epoch [833/5000], Avg. Train Sample Loss: 0.0036, Avg. Validate Sample Loss: 0.0023,                             L2 Loss: 0.0051\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [834/5000] -------------------- \n",
      "Epoch [834/5000], Step [1/47], Loss: 0.0014\n",
      "Epoch [834/5000], Step [9/47], Loss: 0.0025\n",
      "Epoch [834/5000], Step [18/47], Loss: 0.0010\n",
      "Epoch [834/5000], Step [27/47], Loss: 0.0008\n",
      "Epoch [834/5000], Step [36/47], Loss: 0.0018\n",
      "Epoch [834/5000], Step [45/47], Loss: 0.0035\n",
      "Epoch [834/5000], Avg. Train Sample Loss: 0.0016, Avg. Validate Sample Loss: 0.0022,                             L2 Loss: 0.0053\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [835/5000] -------------------- \n",
      "Epoch [835/5000], Step [1/47], Loss: 0.0028\n",
      "Epoch [835/5000], Step [9/47], Loss: 0.0023\n",
      "Epoch [835/5000], Step [18/47], Loss: 0.0028\n",
      "Epoch [835/5000], Step [27/47], Loss: 0.0017\n",
      "Epoch [835/5000], Step [36/47], Loss: 0.0009\n",
      "Epoch [835/5000], Step [45/47], Loss: 0.0010\n",
      "Epoch [835/5000], Avg. Train Sample Loss: 0.0019, Avg. Validate Sample Loss: 0.0012,                             L2 Loss: 0.0029\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [836/5000] -------------------- \n",
      "Epoch [836/5000], Step [1/47], Loss: 0.0009\n",
      "Epoch [836/5000], Step [9/47], Loss: 0.0013\n",
      "Epoch [836/5000], Step [18/47], Loss: 0.0013\n",
      "Epoch [836/5000], Step [27/47], Loss: 0.0026\n",
      "Epoch [836/5000], Step [36/47], Loss: 0.0007\n",
      "Epoch [836/5000], Step [45/47], Loss: 0.0020\n",
      "Epoch [836/5000], Avg. Train Sample Loss: 0.0020, Avg. Validate Sample Loss: 0.0014,                             L2 Loss: 0.0031\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [837/5000] -------------------- \n",
      "Epoch [837/5000], Step [1/47], Loss: 0.0010\n",
      "Epoch [837/5000], Step [9/47], Loss: 0.0019\n",
      "Epoch [837/5000], Step [18/47], Loss: 0.0007\n",
      "Epoch [837/5000], Step [27/47], Loss: 0.0018\n",
      "Epoch [837/5000], Step [36/47], Loss: 0.0027\n",
      "Epoch [837/5000], Step [45/47], Loss: 0.0019\n",
      "Epoch [837/5000], Avg. Train Sample Loss: 0.0017, Avg. Validate Sample Loss: 0.0009,                             L2 Loss: 0.0015\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [838/5000] -------------------- \n",
      "Epoch [838/5000], Step [1/47], Loss: 0.0009\n",
      "Epoch [838/5000], Step [9/47], Loss: 0.0016\n",
      "Epoch [838/5000], Step [18/47], Loss: 0.0015\n",
      "Epoch [838/5000], Step [27/47], Loss: 0.0009\n",
      "Epoch [838/5000], Step [36/47], Loss: 0.0030\n",
      "Epoch [838/5000], Step [45/47], Loss: 0.0010\n",
      "Epoch [838/5000], Avg. Train Sample Loss: 0.0016, Avg. Validate Sample Loss: 0.0017,                             L2 Loss: 0.0047\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [839/5000] -------------------- \n",
      "Epoch [839/5000], Step [1/47], Loss: 0.0029\n",
      "Epoch [839/5000], Step [9/47], Loss: 0.0021\n",
      "Epoch [839/5000], Step [18/47], Loss: 0.0013\n",
      "Epoch [839/5000], Step [27/47], Loss: 0.0020\n",
      "Epoch [839/5000], Step [36/47], Loss: 0.0033\n",
      "Epoch [839/5000], Step [45/47], Loss: 0.0009\n",
      "Epoch [839/5000], Avg. Train Sample Loss: 0.0026, Avg. Validate Sample Loss: 0.0022,                             L2 Loss: 0.0053\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [840/5000] -------------------- \n",
      "Epoch [840/5000], Step [1/47], Loss: 0.0021\n",
      "Epoch [840/5000], Step [9/47], Loss: 0.0007\n",
      "Epoch [840/5000], Step [18/47], Loss: 0.0022\n",
      "Epoch [840/5000], Step [27/47], Loss: 0.0021\n",
      "Epoch [840/5000], Step [36/47], Loss: 0.0082\n",
      "Epoch [840/5000], Step [45/47], Loss: 0.0024\n",
      "Epoch [840/5000], Avg. Train Sample Loss: 0.0032, Avg. Validate Sample Loss: 0.0113,                             L2 Loss: 0.0153\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [841/5000] -------------------- \n",
      "Epoch [841/5000], Step [1/47], Loss: 0.0085\n",
      "Epoch [841/5000], Step [9/47], Loss: 0.0027\n",
      "Epoch [841/5000], Step [18/47], Loss: 0.0110\n",
      "Epoch [841/5000], Step [27/47], Loss: 0.0010\n",
      "Epoch [841/5000], Step [36/47], Loss: 0.0038\n",
      "Epoch [841/5000], Step [45/47], Loss: 0.0037\n",
      "Epoch [841/5000], Avg. Train Sample Loss: 0.0039, Avg. Validate Sample Loss: 0.0034,                             L2 Loss: 0.0073\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [842/5000] -------------------- \n",
      "Epoch [842/5000], Step [1/47], Loss: 0.0028\n",
      "Epoch [842/5000], Step [9/47], Loss: 0.0023\n",
      "Epoch [842/5000], Step [18/47], Loss: 0.0060\n",
      "Epoch [842/5000], Step [27/47], Loss: 0.0094\n",
      "Epoch [842/5000], Step [36/47], Loss: 0.0101\n",
      "Epoch [842/5000], Step [45/47], Loss: 0.0206\n",
      "Epoch [842/5000], Avg. Train Sample Loss: 0.0042, Avg. Validate Sample Loss: 0.0280,                             L2 Loss: 0.0257\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [843/5000] -------------------- \n",
      "Epoch [843/5000], Step [1/47], Loss: 0.0244\n",
      "Epoch [843/5000], Step [9/47], Loss: 0.0136\n",
      "Epoch [843/5000], Step [18/47], Loss: 0.0016\n",
      "Epoch [843/5000], Step [27/47], Loss: 0.0059\n",
      "Epoch [843/5000], Step [36/47], Loss: 0.0222\n",
      "Epoch [843/5000], Step [45/47], Loss: 0.0197\n",
      "Epoch [843/5000], Avg. Train Sample Loss: 0.0143, Avg. Validate Sample Loss: 0.0108,                             L2 Loss: 0.0154\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [844/5000] -------------------- \n",
      "Epoch [844/5000], Step [1/47], Loss: 0.0115\n",
      "Epoch [844/5000], Step [9/47], Loss: 0.0239\n",
      "Epoch [844/5000], Step [18/47], Loss: 0.0023\n",
      "Epoch [844/5000], Step [27/47], Loss: 0.0089\n",
      "Epoch [844/5000], Step [36/47], Loss: 0.0096\n",
      "Epoch [844/5000], Step [45/47], Loss: 0.0034\n",
      "Epoch [844/5000], Avg. Train Sample Loss: 0.0077, Avg. Validate Sample Loss: 0.0050,                             L2 Loss: 0.0099\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [845/5000] -------------------- \n",
      "Epoch [845/5000], Step [1/47], Loss: 0.0060\n",
      "Epoch [845/5000], Step [9/47], Loss: 0.0023\n",
      "Epoch [845/5000], Step [18/47], Loss: 0.0006\n",
      "Epoch [845/5000], Step [27/47], Loss: 0.0033\n",
      "Epoch [845/5000], Step [36/47], Loss: 0.0052\n",
      "Epoch [845/5000], Step [45/47], Loss: 0.0026\n",
      "Epoch [845/5000], Avg. Train Sample Loss: 0.0058, Avg. Validate Sample Loss: 0.0065,                             L2 Loss: 0.0118\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [846/5000] -------------------- \n",
      "Epoch [846/5000], Step [1/47], Loss: 0.0067\n",
      "Epoch [846/5000], Step [9/47], Loss: 0.0026\n",
      "Epoch [846/5000], Step [18/47], Loss: 0.0075\n",
      "Epoch [846/5000], Step [27/47], Loss: 0.0021\n",
      "Epoch [846/5000], Step [36/47], Loss: 0.0012\n",
      "Epoch [846/5000], Step [45/47], Loss: 0.0066\n",
      "Epoch [846/5000], Avg. Train Sample Loss: 0.0226, Avg. Validate Sample Loss: 0.0085,                             L2 Loss: 0.0130\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [847/5000] -------------------- \n",
      "Epoch [847/5000], Step [1/47], Loss: 0.0075\n",
      "Epoch [847/5000], Step [9/47], Loss: 0.0100\n",
      "Epoch [847/5000], Step [18/47], Loss: 0.0021\n",
      "Epoch [847/5000], Step [27/47], Loss: 0.0039\n",
      "Epoch [847/5000], Step [36/47], Loss: 0.0008\n",
      "Epoch [847/5000], Step [45/47], Loss: 0.0012\n",
      "Epoch [847/5000], Avg. Train Sample Loss: 0.0043, Avg. Validate Sample Loss: 0.0017,                             L2 Loss: 0.0041\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [848/5000] -------------------- \n",
      "Epoch [848/5000], Step [1/47], Loss: 0.0016\n",
      "Epoch [848/5000], Step [9/47], Loss: 0.0050\n",
      "Epoch [848/5000], Step [18/47], Loss: 0.0049\n",
      "Epoch [848/5000], Step [27/47], Loss: 0.0006\n",
      "Epoch [848/5000], Step [36/47], Loss: 0.0017\n",
      "Epoch [848/5000], Step [45/47], Loss: 0.0011\n",
      "Epoch [848/5000], Avg. Train Sample Loss: 0.0025, Avg. Validate Sample Loss: 0.0014,                             L2 Loss: 0.0034\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [849/5000] -------------------- \n",
      "Epoch [849/5000], Step [1/47], Loss: 0.0010\n",
      "Epoch [849/5000], Step [9/47], Loss: 0.0012\n",
      "Epoch [849/5000], Step [18/47], Loss: 0.0007\n",
      "Epoch [849/5000], Step [27/47], Loss: 0.0007\n",
      "Epoch [849/5000], Step [36/47], Loss: 0.0011\n",
      "Epoch [849/5000], Step [45/47], Loss: 0.0012\n",
      "Epoch [849/5000], Avg. Train Sample Loss: 0.0016, Avg. Validate Sample Loss: 0.0023,                             L2 Loss: 0.0058\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [850/5000] -------------------- \n",
      "Epoch [850/5000], Step [1/47], Loss: 0.0021\n",
      "Epoch [850/5000], Step [9/47], Loss: 0.0009\n",
      "Epoch [850/5000], Step [18/47], Loss: 0.0041\n",
      "Epoch [850/5000], Step [27/47], Loss: 0.0052\n",
      "Epoch [850/5000], Step [36/47], Loss: 0.0026\n",
      "Epoch [850/5000], Step [45/47], Loss: 0.0057\n",
      "Epoch [850/5000], Avg. Train Sample Loss: 0.0040, Avg. Validate Sample Loss: 0.0083,                             L2 Loss: 0.0126\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [851/5000] -------------------- \n",
      "Epoch [851/5000], Step [1/47], Loss: 0.0080\n",
      "Epoch [851/5000], Step [9/47], Loss: 0.0046\n",
      "Epoch [851/5000], Step [18/47], Loss: 0.0011\n",
      "Epoch [851/5000], Step [27/47], Loss: 0.0007\n",
      "Epoch [851/5000], Step [36/47], Loss: 0.0007\n",
      "Epoch [851/5000], Step [45/47], Loss: 0.0010\n",
      "Epoch [851/5000], Avg. Train Sample Loss: 0.0020, Avg. Validate Sample Loss: 0.0036,                             L2 Loss: 0.0084\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [852/5000] -------------------- \n",
      "Epoch [852/5000], Step [1/47], Loss: 0.0036\n",
      "Epoch [852/5000], Step [9/47], Loss: 0.0015\n",
      "Epoch [852/5000], Step [18/47], Loss: 0.0018\n",
      "Epoch [852/5000], Step [27/47], Loss: 0.0070\n",
      "Epoch [852/5000], Step [36/47], Loss: 0.0006\n",
      "Epoch [852/5000], Step [45/47], Loss: 0.0020\n",
      "Epoch [852/5000], Avg. Train Sample Loss: 0.0028, Avg. Validate Sample Loss: 0.0013,                             L2 Loss: 0.0043\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [853/5000] -------------------- \n",
      "Epoch [853/5000], Step [1/47], Loss: 0.0015\n",
      "Epoch [853/5000], Step [9/47], Loss: 0.0060\n",
      "Epoch [853/5000], Step [18/47], Loss: 0.0035\n",
      "Epoch [853/5000], Step [27/47], Loss: 0.0013\n",
      "Epoch [853/5000], Step [36/47], Loss: 0.0018\n",
      "Epoch [853/5000], Step [45/47], Loss: 0.0006\n",
      "Epoch [853/5000], Avg. Train Sample Loss: 0.0032, Avg. Validate Sample Loss: 0.0049,                             L2 Loss: 0.0093\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [854/5000] -------------------- \n",
      "Epoch [854/5000], Step [1/47], Loss: 0.0044\n",
      "Epoch [854/5000], Step [9/47], Loss: 0.0016\n",
      "Epoch [854/5000], Step [18/47], Loss: 0.0042\n",
      "Epoch [854/5000], Step [27/47], Loss: 0.0134\n",
      "Epoch [854/5000], Step [36/47], Loss: 0.0011\n",
      "Epoch [854/5000], Step [45/47], Loss: 0.0030\n",
      "Epoch [854/5000], Avg. Train Sample Loss: 0.0041, Avg. Validate Sample Loss: 0.0015,                             L2 Loss: 0.0045\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [855/5000] -------------------- \n",
      "Epoch [855/5000], Step [1/47], Loss: 0.0013\n",
      "Epoch [855/5000], Step [9/47], Loss: 0.0042\n",
      "Epoch [855/5000], Step [18/47], Loss: 0.0030\n",
      "Epoch [855/5000], Step [27/47], Loss: 0.0025\n",
      "Epoch [855/5000], Step [36/47], Loss: 0.0076\n",
      "Epoch [855/5000], Step [45/47], Loss: 0.0016\n",
      "Epoch [855/5000], Avg. Train Sample Loss: 0.0031, Avg. Validate Sample Loss: 0.0032,                             L2 Loss: 0.0074\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [856/5000] -------------------- \n",
      "Epoch [856/5000], Step [1/47], Loss: 0.0033\n",
      "Epoch [856/5000], Step [9/47], Loss: 0.0041\n",
      "Epoch [856/5000], Step [18/47], Loss: 0.0028\n",
      "Epoch [856/5000], Step [27/47], Loss: 0.0015\n",
      "Epoch [856/5000], Step [36/47], Loss: 0.0007\n",
      "Epoch [856/5000], Step [45/47], Loss: 0.0031\n",
      "Epoch [856/5000], Avg. Train Sample Loss: 0.0030, Avg. Validate Sample Loss: 0.0024,                             L2 Loss: 0.0061\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [857/5000] -------------------- \n",
      "Epoch [857/5000], Step [1/47], Loss: 0.0033\n",
      "Epoch [857/5000], Step [9/47], Loss: 0.0020\n",
      "Epoch [857/5000], Step [18/47], Loss: 0.0016\n",
      "Epoch [857/5000], Step [27/47], Loss: 0.0007\n",
      "Epoch [857/5000], Step [36/47], Loss: 0.0012\n",
      "Epoch [857/5000], Step [45/47], Loss: 0.0007\n",
      "Epoch [857/5000], Avg. Train Sample Loss: 0.0020, Avg. Validate Sample Loss: 0.0011,                             L2 Loss: 0.0023\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [858/5000] -------------------- \n",
      "Epoch [858/5000], Step [1/47], Loss: 0.0018\n",
      "Epoch [858/5000], Step [9/47], Loss: 0.0022\n",
      "Epoch [858/5000], Step [18/47], Loss: 0.0020\n",
      "Epoch [858/5000], Step [27/47], Loss: 0.0157\n",
      "Epoch [858/5000], Step [36/47], Loss: 0.0024\n",
      "Epoch [858/5000], Step [45/47], Loss: 0.0604\n",
      "Epoch [858/5000], Avg. Train Sample Loss: 0.0081, Avg. Validate Sample Loss: 0.0119,                             L2 Loss: 0.0162\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [859/5000] -------------------- \n",
      "Epoch [859/5000], Step [1/47], Loss: 0.0119\n",
      "Epoch [859/5000], Step [9/47], Loss: 0.0256\n",
      "Epoch [859/5000], Step [18/47], Loss: 0.0157\n",
      "Epoch [859/5000], Step [27/47], Loss: 0.0164\n",
      "Epoch [859/5000], Step [36/47], Loss: 0.0071\n",
      "Epoch [859/5000], Step [45/47], Loss: 0.0053\n",
      "Epoch [859/5000], Avg. Train Sample Loss: 0.0138, Avg. Validate Sample Loss: 0.0045,                             L2 Loss: 0.0091\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [860/5000] -------------------- \n",
      "Epoch [860/5000], Step [1/47], Loss: 0.0040\n",
      "Epoch [860/5000], Step [9/47], Loss: 0.0014\n",
      "Epoch [860/5000], Step [18/47], Loss: 0.0024\n",
      "Epoch [860/5000], Step [27/47], Loss: 0.0007\n",
      "Epoch [860/5000], Step [36/47], Loss: 0.0074\n",
      "Epoch [860/5000], Step [45/47], Loss: 0.0056\n",
      "Epoch [860/5000], Avg. Train Sample Loss: 0.0025, Avg. Validate Sample Loss: 0.0030,                             L2 Loss: 0.0067\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [861/5000] -------------------- \n",
      "Epoch [861/5000], Step [1/47], Loss: 0.0030\n",
      "Epoch [861/5000], Step [9/47], Loss: 0.0331\n",
      "Epoch [861/5000], Step [18/47], Loss: 0.0066\n",
      "Epoch [861/5000], Step [27/47], Loss: 0.0021\n",
      "Epoch [861/5000], Step [36/47], Loss: 0.0014\n",
      "Epoch [861/5000], Step [45/47], Loss: 0.0007\n",
      "Epoch [861/5000], Avg. Train Sample Loss: 0.0049, Avg. Validate Sample Loss: 0.0010,                             L2 Loss: 0.0018\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [862/5000] -------------------- \n",
      "Epoch [862/5000], Step [1/47], Loss: 0.0012\n",
      "Epoch [862/5000], Step [9/47], Loss: 0.0010\n",
      "Epoch [862/5000], Step [18/47], Loss: 0.0009\n",
      "Epoch [862/5000], Step [27/47], Loss: 0.0012\n",
      "Epoch [862/5000], Step [36/47], Loss: 0.0023\n",
      "Epoch [862/5000], Step [45/47], Loss: 0.0082\n",
      "Epoch [862/5000], Avg. Train Sample Loss: 0.0017, Avg. Validate Sample Loss: 0.0174,                             L2 Loss: 0.0199\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [863/5000] -------------------- \n",
      "Epoch [863/5000], Step [1/47], Loss: 0.0152\n",
      "Epoch [863/5000], Step [9/47], Loss: 0.0042\n",
      "Epoch [863/5000], Step [18/47], Loss: 0.0083\n",
      "Epoch [863/5000], Step [27/47], Loss: 0.0021\n",
      "Epoch [863/5000], Step [36/47], Loss: 0.0026\n",
      "Epoch [863/5000], Step [45/47], Loss: 0.0046\n",
      "Epoch [863/5000], Avg. Train Sample Loss: 0.0067, Avg. Validate Sample Loss: 0.0084,                             L2 Loss: 0.0133\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [864/5000] -------------------- \n",
      "Epoch [864/5000], Step [1/47], Loss: 0.0071\n",
      "Epoch [864/5000], Step [9/47], Loss: 0.0065\n",
      "Epoch [864/5000], Step [18/47], Loss: 0.0097\n",
      "Epoch [864/5000], Step [27/47], Loss: 0.0013\n",
      "Epoch [864/5000], Step [36/47], Loss: 0.0027\n",
      "Epoch [864/5000], Step [45/47], Loss: 0.0093\n",
      "Epoch [864/5000], Avg. Train Sample Loss: 0.0043, Avg. Validate Sample Loss: 0.0034,                             L2 Loss: 0.0076\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [865/5000] -------------------- \n",
      "Epoch [865/5000], Step [1/47], Loss: 0.0027\n",
      "Epoch [865/5000], Step [9/47], Loss: 0.0011\n",
      "Epoch [865/5000], Step [18/47], Loss: 0.0038\n",
      "Epoch [865/5000], Step [27/47], Loss: 0.0007\n",
      "Epoch [865/5000], Step [36/47], Loss: 0.0009\n",
      "Epoch [865/5000], Step [45/47], Loss: 0.0106\n",
      "Epoch [865/5000], Avg. Train Sample Loss: 0.0028, Avg. Validate Sample Loss: 0.0077,                             L2 Loss: 0.0124\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [866/5000] -------------------- \n",
      "Epoch [866/5000], Step [1/47], Loss: 0.0052\n",
      "Epoch [866/5000], Step [9/47], Loss: 0.0040\n",
      "Epoch [866/5000], Step [18/47], Loss: 0.0099\n",
      "Epoch [866/5000], Step [27/47], Loss: 0.0011\n",
      "Epoch [866/5000], Step [36/47], Loss: 0.0010\n",
      "Epoch [866/5000], Step [45/47], Loss: 0.0101\n",
      "Epoch [866/5000], Avg. Train Sample Loss: 0.0057, Avg. Validate Sample Loss: 0.0097,                             L2 Loss: 0.0140\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [867/5000] -------------------- \n",
      "Epoch [867/5000], Step [1/47], Loss: 0.0103\n",
      "Epoch [867/5000], Step [9/47], Loss: 0.0066\n",
      "Epoch [867/5000], Step [18/47], Loss: 0.0073\n",
      "Epoch [867/5000], Step [27/47], Loss: 0.0017\n",
      "Epoch [867/5000], Step [36/47], Loss: 0.0014\n",
      "Epoch [867/5000], Step [45/47], Loss: 0.0016\n",
      "Epoch [867/5000], Avg. Train Sample Loss: 0.0042, Avg. Validate Sample Loss: 0.0012,                             L2 Loss: 0.0022\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [868/5000] -------------------- \n",
      "Epoch [868/5000], Step [1/47], Loss: 0.0014\n",
      "Epoch [868/5000], Step [9/47], Loss: 0.0007\n",
      "Epoch [868/5000], Step [18/47], Loss: 0.0031\n",
      "Epoch [868/5000], Step [27/47], Loss: 0.0021\n",
      "Epoch [868/5000], Step [36/47], Loss: 0.0023\n",
      "Epoch [868/5000], Step [45/47], Loss: 0.0133\n",
      "Epoch [868/5000], Avg. Train Sample Loss: 0.0028, Avg. Validate Sample Loss: 0.0117,                             L2 Loss: 0.0160\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [869/5000] -------------------- \n",
      "Epoch [869/5000], Step [1/47], Loss: 0.0113\n",
      "Epoch [869/5000], Step [9/47], Loss: 0.0018\n",
      "Epoch [869/5000], Step [18/47], Loss: 0.0020\n",
      "Epoch [869/5000], Step [27/47], Loss: 0.0043\n",
      "Epoch [869/5000], Step [36/47], Loss: 0.0022\n",
      "Epoch [869/5000], Step [45/47], Loss: 0.0018\n",
      "Epoch [869/5000], Avg. Train Sample Loss: 0.0026, Avg. Validate Sample Loss: 0.0014,                             L2 Loss: 0.0029\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [870/5000] -------------------- \n",
      "Epoch [870/5000], Step [1/47], Loss: 0.0008\n",
      "Epoch [870/5000], Step [9/47], Loss: 0.0009\n",
      "Epoch [870/5000], Step [18/47], Loss: 0.0021\n",
      "Epoch [870/5000], Step [27/47], Loss: 0.0014\n",
      "Epoch [870/5000], Step [36/47], Loss: 0.0028\n",
      "Epoch [870/5000], Step [45/47], Loss: 0.0014\n",
      "Epoch [870/5000], Avg. Train Sample Loss: 0.0022, Avg. Validate Sample Loss: 0.0014,                             L2 Loss: 0.0043\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [871/5000] -------------------- \n",
      "Epoch [871/5000], Step [1/47], Loss: 0.0016\n",
      "Epoch [871/5000], Step [9/47], Loss: 0.0019\n",
      "Epoch [871/5000], Step [18/47], Loss: 0.0109\n",
      "Epoch [871/5000], Step [27/47], Loss: 0.0023\n",
      "Epoch [871/5000], Step [36/47], Loss: 0.0011\n",
      "Epoch [871/5000], Step [45/47], Loss: 0.0318\n",
      "Epoch [871/5000], Avg. Train Sample Loss: 0.0061, Avg. Validate Sample Loss: 0.0202,                             L2 Loss: 0.0205\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [872/5000] -------------------- \n",
      "Epoch [872/5000], Step [1/47], Loss: 0.0174\n",
      "Epoch [872/5000], Step [9/47], Loss: 0.0427\n",
      "Epoch [872/5000], Step [18/47], Loss: 0.0018\n",
      "Epoch [872/5000], Step [27/47], Loss: 0.0008\n",
      "Epoch [872/5000], Step [36/47], Loss: 0.0042\n",
      "Epoch [872/5000], Step [45/47], Loss: 0.0021\n",
      "Epoch [872/5000], Avg. Train Sample Loss: 0.0062, Avg. Validate Sample Loss: 0.0013,                             L2 Loss: 0.0033\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [873/5000] -------------------- \n",
      "Epoch [873/5000], Step [1/47], Loss: 0.0007\n",
      "Epoch [873/5000], Step [9/47], Loss: 0.0032\n",
      "Epoch [873/5000], Step [18/47], Loss: 0.0041\n",
      "Epoch [873/5000], Step [27/47], Loss: 0.0064\n",
      "Epoch [873/5000], Step [36/47], Loss: 0.0019\n",
      "Epoch [873/5000], Step [45/47], Loss: 0.0076\n",
      "Epoch [873/5000], Avg. Train Sample Loss: 0.0045, Avg. Validate Sample Loss: 0.0027,                             L2 Loss: 0.0063\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [874/5000] -------------------- \n",
      "Epoch [874/5000], Step [1/47], Loss: 0.0034\n",
      "Epoch [874/5000], Step [9/47], Loss: 0.0264\n",
      "Epoch [874/5000], Step [18/47], Loss: 0.0337\n",
      "Epoch [874/5000], Step [27/47], Loss: 0.0073\n",
      "Epoch [874/5000], Step [36/47], Loss: 0.0021\n",
      "Epoch [874/5000], Step [45/47], Loss: 0.0041\n",
      "Epoch [874/5000], Avg. Train Sample Loss: 0.0097, Avg. Validate Sample Loss: 0.0018,                             L2 Loss: 0.0054\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [875/5000] -------------------- \n",
      "Epoch [875/5000], Step [1/47], Loss: 0.0015\n",
      "Epoch [875/5000], Step [9/47], Loss: 0.0015\n",
      "Epoch [875/5000], Step [18/47], Loss: 0.0046\n",
      "Epoch [875/5000], Step [27/47], Loss: 0.0013\n",
      "Epoch [875/5000], Step [36/47], Loss: 0.0008\n",
      "Epoch [875/5000], Step [45/47], Loss: 0.0005\n",
      "Epoch [875/5000], Avg. Train Sample Loss: 0.0019, Avg. Validate Sample Loss: 0.0016,                             L2 Loss: 0.0042\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [876/5000] -------------------- \n",
      "Epoch [876/5000], Step [1/47], Loss: 0.0011\n",
      "Epoch [876/5000], Step [9/47], Loss: 0.0008\n",
      "Epoch [876/5000], Step [18/47], Loss: 0.0082\n",
      "Epoch [876/5000], Step [27/47], Loss: 0.0020\n",
      "Epoch [876/5000], Step [36/47], Loss: 0.0089\n",
      "Epoch [876/5000], Step [45/47], Loss: 0.0079\n",
      "Epoch [876/5000], Avg. Train Sample Loss: 0.0059, Avg. Validate Sample Loss: 0.0033,                             L2 Loss: 0.0078\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [877/5000] -------------------- \n",
      "Epoch [877/5000], Step [1/47], Loss: 0.0031\n",
      "Epoch [877/5000], Step [9/47], Loss: 0.0021\n",
      "Epoch [877/5000], Step [18/47], Loss: 0.0047\n",
      "Epoch [877/5000], Step [27/47], Loss: 0.0010\n",
      "Epoch [877/5000], Step [36/47], Loss: 0.0009\n",
      "Epoch [877/5000], Step [45/47], Loss: 0.0094\n",
      "Epoch [877/5000], Avg. Train Sample Loss: 0.0030, Avg. Validate Sample Loss: 0.0030,                             L2 Loss: 0.0074\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [878/5000] -------------------- \n",
      "Epoch [878/5000], Step [1/47], Loss: 0.0033\n",
      "Epoch [878/5000], Step [9/47], Loss: 0.0007\n",
      "Epoch [878/5000], Step [18/47], Loss: 0.0012\n",
      "Epoch [878/5000], Step [27/47], Loss: 0.0069\n",
      "Epoch [878/5000], Step [36/47], Loss: 0.0072\n",
      "Epoch [878/5000], Step [45/47], Loss: 0.0210\n",
      "Epoch [878/5000], Avg. Train Sample Loss: 0.0062, Avg. Validate Sample Loss: 0.0212,                             L2 Loss: 0.0216\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [879/5000] -------------------- \n",
      "Epoch [879/5000], Step [1/47], Loss: 0.0232\n",
      "Epoch [879/5000], Step [9/47], Loss: 0.0197\n",
      "Epoch [879/5000], Step [18/47], Loss: 0.0037\n",
      "Epoch [879/5000], Step [27/47], Loss: 0.0009\n",
      "Epoch [879/5000], Step [36/47], Loss: 0.0021\n",
      "Epoch [879/5000], Step [45/47], Loss: 0.0080\n",
      "Epoch [879/5000], Avg. Train Sample Loss: 0.0049, Avg. Validate Sample Loss: 0.0062,                             L2 Loss: 0.0108\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [880/5000] -------------------- \n",
      "Epoch [880/5000], Step [1/47], Loss: 0.0071\n",
      "Epoch [880/5000], Step [9/47], Loss: 0.0017\n",
      "Epoch [880/5000], Step [18/47], Loss: 0.0031\n",
      "Epoch [880/5000], Step [27/47], Loss: 0.0012\n",
      "Epoch [880/5000], Step [36/47], Loss: 0.0006\n",
      "Epoch [880/5000], Step [45/47], Loss: 0.0018\n",
      "Epoch [880/5000], Avg. Train Sample Loss: 0.0018, Avg. Validate Sample Loss: 0.0011,                             L2 Loss: 0.0014\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [881/5000] -------------------- \n",
      "Epoch [881/5000], Step [1/47], Loss: 0.0014\n",
      "Epoch [881/5000], Step [9/47], Loss: 0.0010\n",
      "Epoch [881/5000], Step [18/47], Loss: 0.0027\n",
      "Epoch [881/5000], Step [27/47], Loss: 0.0054\n",
      "Epoch [881/5000], Step [36/47], Loss: 0.0007\n",
      "Epoch [881/5000], Step [45/47], Loss: 0.0026\n",
      "Epoch [881/5000], Avg. Train Sample Loss: 0.0021, Avg. Validate Sample Loss: 0.0021,                             L2 Loss: 0.0055\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [882/5000] -------------------- \n",
      "Epoch [882/5000], Step [1/47], Loss: 0.0017\n",
      "Epoch [882/5000], Step [9/47], Loss: 0.0039\n",
      "Epoch [882/5000], Step [18/47], Loss: 0.0028\n",
      "Epoch [882/5000], Step [27/47], Loss: 0.0025\n",
      "Epoch [882/5000], Step [36/47], Loss: 0.0007\n",
      "Epoch [882/5000], Step [45/47], Loss: 0.0009\n",
      "Epoch [882/5000], Avg. Train Sample Loss: 0.0020, Avg. Validate Sample Loss: 0.0070,                             L2 Loss: 0.0118\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [883/5000] -------------------- \n",
      "Epoch [883/5000], Step [1/47], Loss: 0.0062\n",
      "Epoch [883/5000], Step [9/47], Loss: 0.0034\n",
      "Epoch [883/5000], Step [18/47], Loss: 0.0014\n",
      "Epoch [883/5000], Step [27/47], Loss: 0.0069\n",
      "Epoch [883/5000], Step [36/47], Loss: 0.0124\n",
      "Epoch [883/5000], Step [45/47], Loss: 0.0111\n",
      "Epoch [883/5000], Avg. Train Sample Loss: 0.0068, Avg. Validate Sample Loss: 0.0027,                             L2 Loss: 0.0062\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [884/5000] -------------------- \n",
      "Epoch [884/5000], Step [1/47], Loss: 0.0021\n",
      "Epoch [884/5000], Step [9/47], Loss: 0.0237\n",
      "Epoch [884/5000], Step [18/47], Loss: 0.1883\n",
      "Epoch [884/5000], Step [27/47], Loss: 0.0216\n",
      "Epoch [884/5000], Step [36/47], Loss: 0.0504\n",
      "Epoch [884/5000], Step [45/47], Loss: 0.0037\n",
      "Epoch [884/5000], Avg. Train Sample Loss: 0.0486, Avg. Validate Sample Loss: 0.0015,                             L2 Loss: 0.0032\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [885/5000] -------------------- \n",
      "Epoch [885/5000], Step [1/47], Loss: 0.0021\n",
      "Epoch [885/5000], Step [9/47], Loss: 0.0014\n",
      "Epoch [885/5000], Step [18/47], Loss: 0.0034\n",
      "Epoch [885/5000], Step [27/47], Loss: 0.0071\n",
      "Epoch [885/5000], Step [36/47], Loss: 0.0053\n",
      "Epoch [885/5000], Step [45/47], Loss: 0.0019\n",
      "Epoch [885/5000], Avg. Train Sample Loss: 0.0039, Avg. Validate Sample Loss: 0.0013,                             L2 Loss: 0.0023\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [886/5000] -------------------- \n",
      "Epoch [886/5000], Step [1/47], Loss: 0.0008\n",
      "Epoch [886/5000], Step [9/47], Loss: 0.0032\n",
      "Epoch [886/5000], Step [18/47], Loss: 0.0017\n",
      "Epoch [886/5000], Step [27/47], Loss: 0.0143\n",
      "Epoch [886/5000], Step [36/47], Loss: 0.0050\n",
      "Epoch [886/5000], Step [45/47], Loss: 0.0037\n",
      "Epoch [886/5000], Avg. Train Sample Loss: 0.0053, Avg. Validate Sample Loss: 0.0101,                             L2 Loss: 0.0144\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [887/5000] -------------------- \n",
      "Epoch [887/5000], Step [1/47], Loss: 0.0112\n",
      "Epoch [887/5000], Step [9/47], Loss: 0.0116\n",
      "Epoch [887/5000], Step [18/47], Loss: 0.0031\n",
      "Epoch [887/5000], Step [27/47], Loss: 0.0010\n",
      "Epoch [887/5000], Step [36/47], Loss: 0.0015\n",
      "Epoch [887/5000], Step [45/47], Loss: 0.0016\n",
      "Epoch [887/5000], Avg. Train Sample Loss: 0.0041, Avg. Validate Sample Loss: 0.0017,                             L2 Loss: 0.0044\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [888/5000] -------------------- \n",
      "Epoch [888/5000], Step [1/47], Loss: 0.0019\n",
      "Epoch [888/5000], Step [9/47], Loss: 0.0012\n",
      "Epoch [888/5000], Step [18/47], Loss: 0.0012\n",
      "Epoch [888/5000], Step [27/47], Loss: 0.0006\n",
      "Epoch [888/5000], Step [36/47], Loss: 0.0013\n",
      "Epoch [888/5000], Step [45/47], Loss: 0.0014\n",
      "Epoch [888/5000], Avg. Train Sample Loss: 0.0012, Avg. Validate Sample Loss: 0.0010,                             L2 Loss: 0.0022\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [889/5000] -------------------- \n",
      "Epoch [889/5000], Step [1/47], Loss: 0.0017\n",
      "Epoch [889/5000], Step [9/47], Loss: 0.0007\n",
      "Epoch [889/5000], Step [18/47], Loss: 0.0007\n",
      "Epoch [889/5000], Step [27/47], Loss: 0.0011\n",
      "Epoch [889/5000], Step [36/47], Loss: 0.0006\n",
      "Epoch [889/5000], Step [45/47], Loss: 0.0005\n",
      "Epoch [889/5000], Avg. Train Sample Loss: 0.0011, Avg. Validate Sample Loss: 0.0021,                             L2 Loss: 0.0053\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [890/5000] -------------------- \n",
      "Epoch [890/5000], Step [1/47], Loss: 0.0011\n",
      "Epoch [890/5000], Step [9/47], Loss: 0.0012\n",
      "Epoch [890/5000], Step [18/47], Loss: 0.0010\n",
      "Epoch [890/5000], Step [27/47], Loss: 0.0012\n",
      "Epoch [890/5000], Step [36/47], Loss: 0.0020\n",
      "Epoch [890/5000], Step [45/47], Loss: 0.0099\n",
      "Epoch [890/5000], Avg. Train Sample Loss: 0.0026, Avg. Validate Sample Loss: 0.0045,                             L2 Loss: 0.0091\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [891/5000] -------------------- \n",
      "Epoch [891/5000], Step [1/47], Loss: 0.0044\n",
      "Epoch [891/5000], Step [9/47], Loss: 0.0037\n",
      "Epoch [891/5000], Step [18/47], Loss: 0.0035\n",
      "Epoch [891/5000], Step [27/47], Loss: 0.0023\n",
      "Epoch [891/5000], Step [36/47], Loss: 0.0011\n",
      "Epoch [891/5000], Step [45/47], Loss: 0.0030\n",
      "Epoch [891/5000], Avg. Train Sample Loss: 0.0020, Avg. Validate Sample Loss: 0.0027,                             L2 Loss: 0.0068\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [892/5000] -------------------- \n",
      "Epoch [892/5000], Step [1/47], Loss: 0.0027\n",
      "Epoch [892/5000], Step [9/47], Loss: 0.0011\n",
      "Epoch [892/5000], Step [18/47], Loss: 0.0008\n",
      "Epoch [892/5000], Step [27/47], Loss: 0.0017\n",
      "Epoch [892/5000], Step [36/47], Loss: 0.0009\n",
      "Epoch [892/5000], Step [45/47], Loss: 0.0018\n",
      "Epoch [892/5000], Avg. Train Sample Loss: 0.0021, Avg. Validate Sample Loss: 0.0010,                             L2 Loss: 0.0020\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [893/5000] -------------------- \n",
      "Epoch [893/5000], Step [1/47], Loss: 0.0011\n",
      "Epoch [893/5000], Step [9/47], Loss: 0.0008\n",
      "Epoch [893/5000], Step [18/47], Loss: 0.0018\n",
      "Epoch [893/5000], Step [27/47], Loss: 0.0047\n",
      "Epoch [893/5000], Step [36/47], Loss: 0.0020\n",
      "Epoch [893/5000], Step [45/47], Loss: 0.0007\n",
      "Epoch [893/5000], Avg. Train Sample Loss: 0.0015, Avg. Validate Sample Loss: 0.0009,                             L2 Loss: 0.0007\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [894/5000] -------------------- \n",
      "Epoch [894/5000], Step [1/47], Loss: 0.0009\n",
      "Epoch [894/5000], Step [9/47], Loss: 0.0012\n",
      "Epoch [894/5000], Step [18/47], Loss: 0.0019\n",
      "Epoch [894/5000], Step [27/47], Loss: 0.0016\n",
      "Epoch [894/5000], Step [36/47], Loss: 0.0010\n",
      "Epoch [894/5000], Step [45/47], Loss: 0.0010\n",
      "Epoch [894/5000], Avg. Train Sample Loss: 0.0014, Avg. Validate Sample Loss: 0.0009,                             L2 Loss: 0.0009\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [895/5000] -------------------- \n",
      "Epoch [895/5000], Step [1/47], Loss: 0.0005\n",
      "Epoch [895/5000], Step [9/47], Loss: 0.0007\n",
      "Epoch [895/5000], Step [18/47], Loss: 0.0068\n",
      "Epoch [895/5000], Step [27/47], Loss: 0.0030\n",
      "Epoch [895/5000], Step [36/47], Loss: 0.0012\n",
      "Epoch [895/5000], Step [45/47], Loss: 0.0063\n",
      "Epoch [895/5000], Avg. Train Sample Loss: 0.0023, Avg. Validate Sample Loss: 0.0089,                             L2 Loss: 0.0134\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [896/5000] -------------------- \n",
      "Epoch [896/5000], Step [1/47], Loss: 0.0081\n",
      "Epoch [896/5000], Step [9/47], Loss: 0.0011\n",
      "Epoch [896/5000], Step [18/47], Loss: 0.0009\n",
      "Epoch [896/5000], Step [27/47], Loss: 0.0018\n",
      "Epoch [896/5000], Step [36/47], Loss: 0.0007\n",
      "Epoch [896/5000], Step [45/47], Loss: 0.0094\n",
      "Epoch [896/5000], Avg. Train Sample Loss: 0.0031, Avg. Validate Sample Loss: 0.0141,                             L2 Loss: 0.0175\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [897/5000] -------------------- \n",
      "Epoch [897/5000], Step [1/47], Loss: 0.0146\n",
      "Epoch [897/5000], Step [9/47], Loss: 0.0111\n",
      "Epoch [897/5000], Step [18/47], Loss: 0.0171\n",
      "Epoch [897/5000], Step [27/47], Loss: 0.0030\n",
      "Epoch [897/5000], Step [36/47], Loss: 0.0020\n",
      "Epoch [897/5000], Step [45/47], Loss: 0.0017\n",
      "Epoch [897/5000], Avg. Train Sample Loss: 0.0065, Avg. Validate Sample Loss: 0.0013,                             L2 Loss: 0.0020\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [898/5000] -------------------- \n",
      "Epoch [898/5000], Step [1/47], Loss: 0.0012\n",
      "Epoch [898/5000], Step [9/47], Loss: 0.0009\n",
      "Epoch [898/5000], Step [18/47], Loss: 0.0010\n",
      "Epoch [898/5000], Step [27/47], Loss: 0.0043\n",
      "Epoch [898/5000], Step [36/47], Loss: 0.0016\n",
      "Epoch [898/5000], Step [45/47], Loss: 0.0015\n",
      "Epoch [898/5000], Avg. Train Sample Loss: 0.0019, Avg. Validate Sample Loss: 0.0051,                             L2 Loss: 0.0101\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [899/5000] -------------------- \n",
      "Epoch [899/5000], Step [1/47], Loss: 0.0065\n",
      "Epoch [899/5000], Step [9/47], Loss: 0.0009\n",
      "Epoch [899/5000], Step [18/47], Loss: 0.0061\n",
      "Epoch [899/5000], Step [27/47], Loss: 0.0028\n",
      "Epoch [899/5000], Step [36/47], Loss: 0.0017\n",
      "Epoch [899/5000], Step [45/47], Loss: 0.0010\n",
      "Epoch [899/5000], Avg. Train Sample Loss: 0.0044, Avg. Validate Sample Loss: 0.0010,                             L2 Loss: 0.0012\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [900/5000] -------------------- \n",
      "Epoch [900/5000], Step [1/47], Loss: 0.0010\n",
      "Epoch [900/5000], Step [9/47], Loss: 0.0017\n",
      "Epoch [900/5000], Step [18/47], Loss: 0.0018\n",
      "Epoch [900/5000], Step [27/47], Loss: 0.0009\n",
      "Epoch [900/5000], Step [36/47], Loss: 0.0053\n",
      "Epoch [900/5000], Step [45/47], Loss: 0.0011\n",
      "Epoch [900/5000], Avg. Train Sample Loss: 0.0018, Avg. Validate Sample Loss: 0.0020,                             L2 Loss: 0.0054\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [901/5000] -------------------- \n",
      "Epoch [901/5000], Step [1/47], Loss: 0.0017\n",
      "Epoch [901/5000], Step [9/47], Loss: 0.0018\n",
      "Epoch [901/5000], Step [18/47], Loss: 0.0030\n",
      "Epoch [901/5000], Step [27/47], Loss: 0.0041\n",
      "Epoch [901/5000], Step [36/47], Loss: 0.0084\n",
      "Epoch [901/5000], Step [45/47], Loss: 0.0010\n",
      "Epoch [901/5000], Avg. Train Sample Loss: 0.0029, Avg. Validate Sample Loss: 0.0009,                             L2 Loss: 0.0021\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [902/5000] -------------------- \n",
      "Epoch [902/5000], Step [1/47], Loss: 0.0007\n",
      "Epoch [902/5000], Step [9/47], Loss: 0.0014\n",
      "Epoch [902/5000], Step [18/47], Loss: 0.0063\n",
      "Epoch [902/5000], Step [27/47], Loss: 0.0020\n",
      "Epoch [902/5000], Step [36/47], Loss: 0.0013\n",
      "Epoch [902/5000], Step [45/47], Loss: 0.0010\n",
      "Epoch [902/5000], Avg. Train Sample Loss: 0.0021, Avg. Validate Sample Loss: 0.0011,                             L2 Loss: 0.0024\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [903/5000] -------------------- \n",
      "Epoch [903/5000], Step [1/47], Loss: 0.0010\n",
      "Epoch [903/5000], Step [9/47], Loss: 0.0006\n",
      "Epoch [903/5000], Step [18/47], Loss: 0.0010\n",
      "Epoch [903/5000], Step [27/47], Loss: 0.0006\n",
      "Epoch [903/5000], Step [36/47], Loss: 0.0015\n",
      "Epoch [903/5000], Step [45/47], Loss: 0.0053\n",
      "Epoch [903/5000], Avg. Train Sample Loss: 0.0031, Avg. Validate Sample Loss: 0.0016,                             L2 Loss: 0.0049\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [904/5000] -------------------- \n",
      "Epoch [904/5000], Step [1/47], Loss: 0.0015\n",
      "Epoch [904/5000], Step [9/47], Loss: 0.0059\n",
      "Epoch [904/5000], Step [18/47], Loss: 0.0020\n",
      "Epoch [904/5000], Step [27/47], Loss: 0.0010\n",
      "Epoch [904/5000], Step [36/47], Loss: 0.0024\n",
      "Epoch [904/5000], Step [45/47], Loss: 0.0029\n",
      "Epoch [904/5000], Avg. Train Sample Loss: 0.0117, Avg. Validate Sample Loss: 0.0030,                             L2 Loss: 0.0056\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [905/5000] -------------------- \n",
      "Epoch [905/5000], Step [1/47], Loss: 0.0023\n",
      "Epoch [905/5000], Step [9/47], Loss: 0.0079\n",
      "Epoch [905/5000], Step [18/47], Loss: 0.0037\n",
      "Epoch [905/5000], Step [27/47], Loss: 0.0170\n",
      "Epoch [905/5000], Step [36/47], Loss: 0.0154\n",
      "Epoch [905/5000], Step [45/47], Loss: 0.0048\n",
      "Epoch [905/5000], Avg. Train Sample Loss: 0.0118, Avg. Validate Sample Loss: 0.0011,                             L2 Loss: 0.0032\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [906/5000] -------------------- \n",
      "Epoch [906/5000], Step [1/47], Loss: 0.0010\n",
      "Epoch [906/5000], Step [9/47], Loss: 0.0010\n",
      "Epoch [906/5000], Step [18/47], Loss: 0.0055\n",
      "Epoch [906/5000], Step [27/47], Loss: 0.0013\n",
      "Epoch [906/5000], Step [36/47], Loss: 0.0013\n",
      "Epoch [906/5000], Step [45/47], Loss: 0.0010\n",
      "Epoch [906/5000], Avg. Train Sample Loss: 0.0020, Avg. Validate Sample Loss: 0.0009,                             L2 Loss: 0.0013\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [907/5000] -------------------- \n",
      "Epoch [907/5000], Step [1/47], Loss: 0.0006\n",
      "Epoch [907/5000], Step [9/47], Loss: 0.0016\n",
      "Epoch [907/5000], Step [18/47], Loss: 0.0008\n",
      "Epoch [907/5000], Step [27/47], Loss: 0.0008\n",
      "Epoch [907/5000], Step [36/47], Loss: 0.0042\n",
      "Epoch [907/5000], Step [45/47], Loss: 0.0022\n",
      "Epoch [907/5000], Avg. Train Sample Loss: 0.0016, Avg. Validate Sample Loss: 0.0012,                             L2 Loss: 0.0031\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [908/5000] -------------------- \n",
      "Epoch [908/5000], Step [1/47], Loss: 0.0012\n",
      "Epoch [908/5000], Step [9/47], Loss: 0.0012\n",
      "Epoch [908/5000], Step [18/47], Loss: 0.0180\n",
      "Epoch [908/5000], Step [27/47], Loss: 0.0102\n",
      "Epoch [908/5000], Step [36/47], Loss: 0.0031\n",
      "Epoch [908/5000], Step [45/47], Loss: 0.0015\n",
      "Epoch [908/5000], Avg. Train Sample Loss: 0.0070, Avg. Validate Sample Loss: 0.0022,                             L2 Loss: 0.0054\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [909/5000] -------------------- \n",
      "Epoch [909/5000], Step [1/47], Loss: 0.0022\n",
      "Epoch [909/5000], Step [9/47], Loss: 0.0017\n",
      "Epoch [909/5000], Step [18/47], Loss: 0.0056\n",
      "Epoch [909/5000], Step [27/47], Loss: 0.0009\n",
      "Epoch [909/5000], Step [36/47], Loss: 0.0189\n",
      "Epoch [909/5000], Step [45/47], Loss: 0.0380\n",
      "Epoch [909/5000], Avg. Train Sample Loss: 0.0074, Avg. Validate Sample Loss: 0.0244,                             L2 Loss: 0.0240\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [910/5000] -------------------- \n",
      "Epoch [910/5000], Step [1/47], Loss: 0.0252\n",
      "Epoch [910/5000], Step [9/47], Loss: 0.0063\n",
      "Epoch [910/5000], Step [18/47], Loss: 0.0010\n",
      "Epoch [910/5000], Step [27/47], Loss: 0.0051\n",
      "Epoch [910/5000], Step [36/47], Loss: 0.0007\n",
      "Epoch [910/5000], Step [45/47], Loss: 0.0013\n",
      "Epoch [910/5000], Avg. Train Sample Loss: 0.0050, Avg. Validate Sample Loss: 0.0013,                             L2 Loss: 0.0021\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [911/5000] -------------------- \n",
      "Epoch [911/5000], Step [1/47], Loss: 0.0010\n",
      "Epoch [911/5000], Step [9/47], Loss: 0.0035\n",
      "Epoch [911/5000], Step [18/47], Loss: 0.0012\n",
      "Epoch [911/5000], Step [27/47], Loss: 0.0014\n",
      "Epoch [911/5000], Step [36/47], Loss: 0.0012\n",
      "Epoch [911/5000], Step [45/47], Loss: 0.0013\n",
      "Epoch [911/5000], Avg. Train Sample Loss: 0.0013, Avg. Validate Sample Loss: 0.0011,                             L2 Loss: 0.0009\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [912/5000] -------------------- \n",
      "Epoch [912/5000], Step [1/47], Loss: 0.0013\n",
      "Epoch [912/5000], Step [9/47], Loss: 0.0011\n",
      "Epoch [912/5000], Step [18/47], Loss: 0.0008\n",
      "Epoch [912/5000], Step [27/47], Loss: 0.0006\n",
      "Epoch [912/5000], Step [36/47], Loss: 0.0038\n",
      "Epoch [912/5000], Step [45/47], Loss: 0.0022\n",
      "Epoch [912/5000], Avg. Train Sample Loss: 0.0019, Avg. Validate Sample Loss: 0.0033,                             L2 Loss: 0.0071\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [913/5000] -------------------- \n",
      "Epoch [913/5000], Step [1/47], Loss: 0.0022\n",
      "Epoch [913/5000], Step [9/47], Loss: 0.0013\n",
      "Epoch [913/5000], Step [18/47], Loss: 0.0025\n",
      "Epoch [913/5000], Step [27/47], Loss: 0.0022\n",
      "Epoch [913/5000], Step [36/47], Loss: 0.0055\n",
      "Epoch [913/5000], Step [45/47], Loss: 0.0012\n",
      "Epoch [913/5000], Avg. Train Sample Loss: 0.0030, Avg. Validate Sample Loss: 0.0010,                             L2 Loss: 0.0019\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [914/5000] -------------------- \n",
      "Epoch [914/5000], Step [1/47], Loss: 0.0009\n",
      "Epoch [914/5000], Step [9/47], Loss: 0.0013\n",
      "Epoch [914/5000], Step [18/47], Loss: 0.0024\n",
      "Epoch [914/5000], Step [27/47], Loss: 0.0009\n",
      "Epoch [914/5000], Step [36/47], Loss: 0.0042\n",
      "Epoch [914/5000], Step [45/47], Loss: 0.0013\n",
      "Epoch [914/5000], Avg. Train Sample Loss: 0.0045, Avg. Validate Sample Loss: 0.0039,                             L2 Loss: 0.0075\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [915/5000] -------------------- \n",
      "Epoch [915/5000], Step [1/47], Loss: 0.0037\n",
      "Epoch [915/5000], Step [9/47], Loss: 0.0056\n",
      "Epoch [915/5000], Step [18/47], Loss: 0.0022\n",
      "Epoch [915/5000], Step [27/47], Loss: 0.0009\n",
      "Epoch [915/5000], Step [36/47], Loss: 0.0007\n",
      "Epoch [915/5000], Step [45/47], Loss: 0.0063\n",
      "Epoch [915/5000], Avg. Train Sample Loss: 0.0047, Avg. Validate Sample Loss: 0.0016,                             L2 Loss: 0.0036\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [916/5000] -------------------- \n",
      "Epoch [916/5000], Step [1/47], Loss: 0.0014\n",
      "Epoch [916/5000], Step [9/47], Loss: 0.0037\n",
      "Epoch [916/5000], Step [18/47], Loss: 0.0010\n",
      "Epoch [916/5000], Step [27/47], Loss: 0.0008\n",
      "Epoch [916/5000], Step [36/47], Loss: 0.0017\n",
      "Epoch [916/5000], Step [45/47], Loss: 0.0009\n",
      "Epoch [916/5000], Avg. Train Sample Loss: 0.0024, Avg. Validate Sample Loss: 0.0010,                             L2 Loss: 0.0014\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [917/5000] -------------------- \n",
      "Epoch [917/5000], Step [1/47], Loss: 0.0008\n",
      "Epoch [917/5000], Step [9/47], Loss: 0.0013\n",
      "Epoch [917/5000], Step [18/47], Loss: 0.0036\n",
      "Epoch [917/5000], Step [27/47], Loss: 0.0039\n",
      "Epoch [917/5000], Step [36/47], Loss: 0.0087\n",
      "Epoch [917/5000], Step [45/47], Loss: 0.0016\n",
      "Epoch [917/5000], Avg. Train Sample Loss: 0.0027, Avg. Validate Sample Loss: 0.0013,                             L2 Loss: 0.0038\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [918/5000] -------------------- \n",
      "Epoch [918/5000], Step [1/47], Loss: 0.0011\n",
      "Epoch [918/5000], Step [9/47], Loss: 0.0062\n",
      "Epoch [918/5000], Step [18/47], Loss: 0.0013\n",
      "Epoch [918/5000], Step [27/47], Loss: 0.0022\n",
      "Epoch [918/5000], Step [36/47], Loss: 0.0019\n",
      "Epoch [918/5000], Step [45/47], Loss: 0.0020\n",
      "Epoch [918/5000], Avg. Train Sample Loss: 0.0034, Avg. Validate Sample Loss: 0.0030,                             L2 Loss: 0.0074\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [919/5000] -------------------- \n",
      "Epoch [919/5000], Step [1/47], Loss: 0.0028\n",
      "Epoch [919/5000], Step [9/47], Loss: 0.0033\n",
      "Epoch [919/5000], Step [18/47], Loss: 0.0018\n",
      "Epoch [919/5000], Step [27/47], Loss: 0.0011\n",
      "Epoch [919/5000], Step [36/47], Loss: 0.0064\n",
      "Epoch [919/5000], Step [45/47], Loss: 0.0177\n",
      "Epoch [919/5000], Avg. Train Sample Loss: 0.0036, Avg. Validate Sample Loss: 0.0061,                             L2 Loss: 0.0120\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [920/5000] -------------------- \n",
      "Epoch [920/5000], Step [1/47], Loss: 0.0063\n",
      "Epoch [920/5000], Step [9/47], Loss: 0.0033\n",
      "Epoch [920/5000], Step [18/47], Loss: 0.0025\n",
      "Epoch [920/5000], Step [27/47], Loss: 0.0026\n",
      "Epoch [920/5000], Step [36/47], Loss: 0.0009\n",
      "Epoch [920/5000], Step [45/47], Loss: 0.0009\n",
      "Epoch [920/5000], Avg. Train Sample Loss: 0.0085, Avg. Validate Sample Loss: 0.0034,                             L2 Loss: 0.0068\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [921/5000] -------------------- \n",
      "Epoch [921/5000], Step [1/47], Loss: 0.0035\n",
      "Epoch [921/5000], Step [9/47], Loss: 0.0017\n",
      "Epoch [921/5000], Step [18/47], Loss: 0.0070\n",
      "Epoch [921/5000], Step [27/47], Loss: 0.0085\n",
      "Epoch [921/5000], Step [36/47], Loss: 0.0032\n",
      "Epoch [921/5000], Step [45/47], Loss: 0.0114\n",
      "Epoch [921/5000], Avg. Train Sample Loss: 0.0069, Avg. Validate Sample Loss: 0.0282,                             L2 Loss: 0.0259\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [922/5000] -------------------- \n",
      "Epoch [922/5000], Step [1/47], Loss: 0.0324\n",
      "Epoch [922/5000], Step [9/47], Loss: 0.0107\n",
      "Epoch [922/5000], Step [18/47], Loss: 0.0076\n",
      "Epoch [922/5000], Step [27/47], Loss: 0.0039\n",
      "Epoch [922/5000], Step [36/47], Loss: 0.0062\n",
      "Epoch [922/5000], Step [45/47], Loss: 0.0014\n",
      "Epoch [922/5000], Avg. Train Sample Loss: 0.0109, Avg. Validate Sample Loss: 0.0015,                             L2 Loss: 0.0047\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [923/5000] -------------------- \n",
      "Epoch [923/5000], Step [1/47], Loss: 0.0021\n",
      "Epoch [923/5000], Step [9/47], Loss: 0.0124\n",
      "Epoch [923/5000], Step [18/47], Loss: 0.0022\n",
      "Epoch [923/5000], Step [27/47], Loss: 0.0026\n",
      "Epoch [923/5000], Step [36/47], Loss: 0.0356\n",
      "Epoch [923/5000], Step [45/47], Loss: 0.0358\n",
      "Epoch [923/5000], Avg. Train Sample Loss: 0.0101, Avg. Validate Sample Loss: 0.0378,                             L2 Loss: 0.0299\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [924/5000] -------------------- \n",
      "Epoch [924/5000], Step [1/47], Loss: 0.0389\n",
      "Epoch [924/5000], Step [9/47], Loss: 0.0124\n",
      "Epoch [924/5000], Step [18/47], Loss: 0.0196\n",
      "Epoch [924/5000], Step [27/47], Loss: 0.0045\n",
      "Epoch [924/5000], Step [36/47], Loss: 0.0066\n",
      "Epoch [924/5000], Step [45/47], Loss: 0.0034\n",
      "Epoch [924/5000], Avg. Train Sample Loss: 0.0129, Avg. Validate Sample Loss: 0.0097,                             L2 Loss: 0.0145\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [925/5000] -------------------- \n",
      "Epoch [925/5000], Step [1/47], Loss: 0.0108\n",
      "Epoch [925/5000], Step [9/47], Loss: 0.0025\n",
      "Epoch [925/5000], Step [18/47], Loss: 0.0046\n",
      "Epoch [925/5000], Step [27/47], Loss: 0.0010\n",
      "Epoch [925/5000], Step [36/47], Loss: 0.0035\n",
      "Epoch [925/5000], Step [45/47], Loss: 0.0211\n",
      "Epoch [925/5000], Avg. Train Sample Loss: 0.0063, Avg. Validate Sample Loss: 0.0162,                             L2 Loss: 0.0189\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [926/5000] -------------------- \n",
      "Epoch [926/5000], Step [1/47], Loss: 0.0168\n",
      "Epoch [926/5000], Step [9/47], Loss: 0.0025\n",
      "Epoch [926/5000], Step [18/47], Loss: 0.0026\n",
      "Epoch [926/5000], Step [27/47], Loss: 0.0032\n",
      "Epoch [926/5000], Step [36/47], Loss: 0.0013\n",
      "Epoch [926/5000], Step [45/47], Loss: 0.0022\n",
      "Epoch [926/5000], Avg. Train Sample Loss: 0.0030, Avg. Validate Sample Loss: 0.0017,                             L2 Loss: 0.0047\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [927/5000] -------------------- \n",
      "Epoch [927/5000], Step [1/47], Loss: 0.0012\n",
      "Epoch [927/5000], Step [9/47], Loss: 0.0010\n",
      "Epoch [927/5000], Step [18/47], Loss: 0.0066\n",
      "Epoch [927/5000], Step [27/47], Loss: 0.0178\n",
      "Epoch [927/5000], Step [36/47], Loss: 0.0028\n",
      "Epoch [927/5000], Step [45/47], Loss: 0.0031\n",
      "Epoch [927/5000], Avg. Train Sample Loss: 0.0037, Avg. Validate Sample Loss: 0.0050,                             L2 Loss: 0.0100\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [928/5000] -------------------- \n",
      "Epoch [928/5000], Step [1/47], Loss: 0.0047\n",
      "Epoch [928/5000], Step [9/47], Loss: 0.0014\n",
      "Epoch [928/5000], Step [18/47], Loss: 0.0070\n",
      "Epoch [928/5000], Step [27/47], Loss: 0.0031\n",
      "Epoch [928/5000], Step [36/47], Loss: 0.0014\n",
      "Epoch [928/5000], Step [45/47], Loss: 0.0006\n",
      "Epoch [928/5000], Avg. Train Sample Loss: 0.0019, Avg. Validate Sample Loss: 0.0010,                             L2 Loss: 0.0005\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [929/5000] -------------------- \n",
      "Epoch [929/5000], Step [1/47], Loss: 0.0008\n",
      "Epoch [929/5000], Step [9/47], Loss: 0.0025\n",
      "Epoch [929/5000], Step [18/47], Loss: 0.0010\n",
      "Epoch [929/5000], Step [27/47], Loss: 0.0023\n",
      "Epoch [929/5000], Step [36/47], Loss: 0.0022\n",
      "Epoch [929/5000], Step [45/47], Loss: 0.0007\n",
      "Epoch [929/5000], Avg. Train Sample Loss: 0.0025, Avg. Validate Sample Loss: 0.0016,                             L2 Loss: 0.0041\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [930/5000] -------------------- \n",
      "Epoch [930/5000], Step [1/47], Loss: 0.0015\n",
      "Epoch [930/5000], Step [9/47], Loss: 0.0041\n",
      "Epoch [930/5000], Step [18/47], Loss: 0.0035\n",
      "Epoch [930/5000], Step [27/47], Loss: 0.0011\n",
      "Epoch [930/5000], Step [36/47], Loss: 0.0006\n",
      "Epoch [930/5000], Step [45/47], Loss: 0.0029\n",
      "Epoch [930/5000], Avg. Train Sample Loss: 0.0020, Avg. Validate Sample Loss: 0.0024,                             L2 Loss: 0.0064\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [931/5000] -------------------- \n",
      "Epoch [931/5000], Step [1/47], Loss: 0.0027\n",
      "Epoch [931/5000], Step [9/47], Loss: 0.0010\n",
      "Epoch [931/5000], Step [18/47], Loss: 0.0075\n",
      "Epoch [931/5000], Step [27/47], Loss: 0.0015\n",
      "Epoch [931/5000], Step [36/47], Loss: 0.0321\n",
      "Epoch [931/5000], Step [45/47], Loss: 0.0165\n",
      "Epoch [931/5000], Avg. Train Sample Loss: 0.0085, Avg. Validate Sample Loss: 0.0030,                             L2 Loss: 0.0075\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [932/5000] -------------------- \n",
      "Epoch [932/5000], Step [1/47], Loss: 0.0030\n",
      "Epoch [932/5000], Step [9/47], Loss: 0.0101\n",
      "Epoch [932/5000], Step [18/47], Loss: 0.0036\n",
      "Epoch [932/5000], Step [27/47], Loss: 0.0074\n",
      "Epoch [932/5000], Step [36/47], Loss: 0.0049\n",
      "Epoch [932/5000], Step [45/47], Loss: 0.0051\n",
      "Epoch [932/5000], Avg. Train Sample Loss: 0.0071, Avg. Validate Sample Loss: 0.0029,                             L2 Loss: 0.0066\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [933/5000] -------------------- \n",
      "Epoch [933/5000], Step [1/47], Loss: 0.0024\n",
      "Epoch [933/5000], Step [9/47], Loss: 0.0020\n",
      "Epoch [933/5000], Step [18/47], Loss: 0.0035\n",
      "Epoch [933/5000], Step [27/47], Loss: 0.0077\n",
      "Epoch [933/5000], Step [36/47], Loss: 0.0026\n",
      "Epoch [933/5000], Step [45/47], Loss: 0.0011\n",
      "Epoch [933/5000], Avg. Train Sample Loss: 0.0030, Avg. Validate Sample Loss: 0.0011,                             L2 Loss: 0.0020\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [934/5000] -------------------- \n",
      "Epoch [934/5000], Step [1/47], Loss: 0.0020\n",
      "Epoch [934/5000], Step [9/47], Loss: 0.0011\n",
      "Epoch [934/5000], Step [18/47], Loss: 0.0009\n",
      "Epoch [934/5000], Step [27/47], Loss: 0.0007\n",
      "Epoch [934/5000], Step [36/47], Loss: 0.0005\n",
      "Epoch [934/5000], Step [45/47], Loss: 0.0010\n",
      "Epoch [934/5000], Avg. Train Sample Loss: 0.0012, Avg. Validate Sample Loss: 0.0009,                             L2 Loss: 0.0017\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [935/5000] -------------------- \n",
      "Epoch [935/5000], Step [1/47], Loss: 0.0007\n",
      "Epoch [935/5000], Step [9/47], Loss: 0.0018\n",
      "Epoch [935/5000], Step [18/47], Loss: 0.0031\n",
      "Epoch [935/5000], Step [27/47], Loss: 0.0024\n",
      "Epoch [935/5000], Step [36/47], Loss: 0.0010\n",
      "Epoch [935/5000], Step [45/47], Loss: 0.0029\n",
      "Epoch [935/5000], Avg. Train Sample Loss: 0.0027, Avg. Validate Sample Loss: 0.0037,                             L2 Loss: 0.0086\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [936/5000] -------------------- \n",
      "Epoch [936/5000], Step [1/47], Loss: 0.0042\n",
      "Epoch [936/5000], Step [9/47], Loss: 0.0008\n",
      "Epoch [936/5000], Step [18/47], Loss: 0.0037\n",
      "Epoch [936/5000], Step [27/47], Loss: 0.0012\n",
      "Epoch [936/5000], Step [36/47], Loss: 0.0019\n",
      "Epoch [936/5000], Step [45/47], Loss: 0.0009\n",
      "Epoch [936/5000], Avg. Train Sample Loss: 0.0014, Avg. Validate Sample Loss: 0.0009,                             L2 Loss: 0.0017\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [937/5000] -------------------- \n",
      "Epoch [937/5000], Step [1/47], Loss: 0.0008\n",
      "Epoch [937/5000], Step [9/47], Loss: 0.0044\n",
      "Epoch [937/5000], Step [18/47], Loss: 0.0008\n",
      "Epoch [937/5000], Step [27/47], Loss: 0.0075\n",
      "Epoch [937/5000], Step [36/47], Loss: 0.0016\n",
      "Epoch [937/5000], Step [45/47], Loss: 0.0016\n",
      "Epoch [937/5000], Avg. Train Sample Loss: 0.0033, Avg. Validate Sample Loss: 0.0106,                             L2 Loss: 0.0147\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [938/5000] -------------------- \n",
      "Epoch [938/5000], Step [1/47], Loss: 0.0108\n",
      "Epoch [938/5000], Step [9/47], Loss: 0.0021\n",
      "Epoch [938/5000], Step [18/47], Loss: 0.0013\n",
      "Epoch [938/5000], Step [27/47], Loss: 0.0017\n",
      "Epoch [938/5000], Step [36/47], Loss: 0.0045\n",
      "Epoch [938/5000], Step [45/47], Loss: 0.0083\n",
      "Epoch [938/5000], Avg. Train Sample Loss: 0.0048, Avg. Validate Sample Loss: 0.0012,                             L2 Loss: 0.0029\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [939/5000] -------------------- \n",
      "Epoch [939/5000], Step [1/47], Loss: 0.0007\n",
      "Epoch [939/5000], Step [9/47], Loss: 0.0018\n",
      "Epoch [939/5000], Step [18/47], Loss: 0.0022\n",
      "Epoch [939/5000], Step [27/47], Loss: 0.0039\n",
      "Epoch [939/5000], Step [36/47], Loss: 0.0011\n",
      "Epoch [939/5000], Step [45/47], Loss: 0.0045\n",
      "Epoch [939/5000], Avg. Train Sample Loss: 0.0053, Avg. Validate Sample Loss: 0.0094,                             L2 Loss: 0.0132\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [940/5000] -------------------- \n",
      "Epoch [940/5000], Step [1/47], Loss: 0.0073\n",
      "Epoch [940/5000], Step [9/47], Loss: 0.0030\n",
      "Epoch [940/5000], Step [18/47], Loss: 0.0007\n",
      "Epoch [940/5000], Step [27/47], Loss: 0.0007\n",
      "Epoch [940/5000], Step [36/47], Loss: 0.0029\n",
      "Epoch [940/5000], Step [45/47], Loss: 0.0018\n",
      "Epoch [940/5000], Avg. Train Sample Loss: 0.0028, Avg. Validate Sample Loss: 0.0021,                             L2 Loss: 0.0055\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [941/5000] -------------------- \n",
      "Epoch [941/5000], Step [1/47], Loss: 0.0018\n",
      "Epoch [941/5000], Step [9/47], Loss: 0.0010\n",
      "Epoch [941/5000], Step [18/47], Loss: 0.0019\n",
      "Epoch [941/5000], Step [27/47], Loss: 0.0019\n",
      "Epoch [941/5000], Step [36/47], Loss: 0.0013\n",
      "Epoch [941/5000], Step [45/47], Loss: 0.0018\n",
      "Epoch [941/5000], Avg. Train Sample Loss: 0.0018, Avg. Validate Sample Loss: 0.0022,                             L2 Loss: 0.0057\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [942/5000] -------------------- \n",
      "Epoch [942/5000], Step [1/47], Loss: 0.0020\n",
      "Epoch [942/5000], Step [9/47], Loss: 0.0032\n",
      "Epoch [942/5000], Step [18/47], Loss: 0.0027\n",
      "Epoch [942/5000], Step [27/47], Loss: 0.0045\n",
      "Epoch [942/5000], Step [36/47], Loss: 0.0059\n",
      "Epoch [942/5000], Step [45/47], Loss: 0.0045\n",
      "Epoch [942/5000], Avg. Train Sample Loss: 0.0031, Avg. Validate Sample Loss: 0.0031,                             L2 Loss: 0.0071\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [943/5000] -------------------- \n",
      "Epoch [943/5000], Step [1/47], Loss: 0.0022\n",
      "Epoch [943/5000], Step [9/47], Loss: 0.0009\n",
      "Epoch [943/5000], Step [18/47], Loss: 0.0048\n",
      "Epoch [943/5000], Step [27/47], Loss: 0.0083\n",
      "Epoch [943/5000], Step [36/47], Loss: 0.0014\n",
      "Epoch [943/5000], Step [45/47], Loss: 0.0059\n",
      "Epoch [943/5000], Avg. Train Sample Loss: 0.0067, Avg. Validate Sample Loss: 0.0024,                             L2 Loss: 0.0069\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [944/5000] -------------------- \n",
      "Epoch [944/5000], Step [1/47], Loss: 0.0020\n",
      "Epoch [944/5000], Step [9/47], Loss: 0.0005\n",
      "Epoch [944/5000], Step [18/47], Loss: 0.0012\n",
      "Epoch [944/5000], Step [27/47], Loss: 0.0012\n",
      "Epoch [944/5000], Step [36/47], Loss: 0.0128\n",
      "Epoch [944/5000], Step [45/47], Loss: 0.0116\n",
      "Epoch [944/5000], Avg. Train Sample Loss: 0.0076, Avg. Validate Sample Loss: 0.0016,                             L2 Loss: 0.0031\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [945/5000] -------------------- \n",
      "Epoch [945/5000], Step [1/47], Loss: 0.0014\n",
      "Epoch [945/5000], Step [9/47], Loss: 0.0163\n",
      "Epoch [945/5000], Step [18/47], Loss: 0.0144\n",
      "Epoch [945/5000], Step [27/47], Loss: 0.0023\n",
      "Epoch [945/5000], Step [36/47], Loss: 0.0016\n",
      "Epoch [945/5000], Step [45/47], Loss: 0.0017\n",
      "Epoch [945/5000], Avg. Train Sample Loss: 0.0066, Avg. Validate Sample Loss: 0.0020,                             L2 Loss: 0.0049\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [946/5000] -------------------- \n",
      "Epoch [946/5000], Step [1/47], Loss: 0.0020\n",
      "Epoch [946/5000], Step [9/47], Loss: 0.0086\n",
      "Epoch [946/5000], Step [18/47], Loss: 0.0074\n",
      "Epoch [946/5000], Step [27/47], Loss: 0.0019\n",
      "Epoch [946/5000], Step [36/47], Loss: 0.0116\n",
      "Epoch [946/5000], Step [45/47], Loss: 0.0073\n",
      "Epoch [946/5000], Avg. Train Sample Loss: 0.0069, Avg. Validate Sample Loss: 0.0181,                             L2 Loss: 0.0208\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [947/5000] -------------------- \n",
      "Epoch [947/5000], Step [1/47], Loss: 0.0208\n",
      "Epoch [947/5000], Step [9/47], Loss: 0.0064\n",
      "Epoch [947/5000], Step [18/47], Loss: 0.0051\n",
      "Epoch [947/5000], Step [27/47], Loss: 0.0005\n",
      "Epoch [947/5000], Step [36/47], Loss: 0.0033\n",
      "Epoch [947/5000], Step [45/47], Loss: 0.0030\n",
      "Epoch [947/5000], Avg. Train Sample Loss: 0.0034, Avg. Validate Sample Loss: 0.0010,                             L2 Loss: 0.0011\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [948/5000] -------------------- \n",
      "Epoch [948/5000], Step [1/47], Loss: 0.0017\n",
      "Epoch [948/5000], Step [9/47], Loss: 0.0017\n",
      "Epoch [948/5000], Step [18/47], Loss: 0.0009\n",
      "Epoch [948/5000], Step [27/47], Loss: 0.0027\n",
      "Epoch [948/5000], Step [36/47], Loss: 0.0009\n",
      "Epoch [948/5000], Step [45/47], Loss: 0.0071\n",
      "Epoch [948/5000], Avg. Train Sample Loss: 0.0027, Avg. Validate Sample Loss: 0.0080,                             L2 Loss: 0.0130\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [949/5000] -------------------- \n",
      "Epoch [949/5000], Step [1/47], Loss: 0.0081\n",
      "Epoch [949/5000], Step [9/47], Loss: 0.0039\n",
      "Epoch [949/5000], Step [18/47], Loss: 0.0026\n",
      "Epoch [949/5000], Step [27/47], Loss: 0.0115\n",
      "Epoch [949/5000], Step [36/47], Loss: 0.0012\n",
      "Epoch [949/5000], Step [45/47], Loss: 0.0020\n",
      "Epoch [949/5000], Avg. Train Sample Loss: 0.0061, Avg. Validate Sample Loss: 0.0008,                             L2 Loss: 0.0015\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [950/5000] -------------------- \n",
      "Epoch [950/5000], Step [1/47], Loss: 0.0009\n",
      "Epoch [950/5000], Step [9/47], Loss: 0.0008\n",
      "Epoch [950/5000], Step [18/47], Loss: 0.0010\n",
      "Epoch [950/5000], Step [27/47], Loss: 0.0019\n",
      "Epoch [950/5000], Step [36/47], Loss: 0.0008\n",
      "Epoch [950/5000], Step [45/47], Loss: 0.0026\n",
      "Epoch [950/5000], Avg. Train Sample Loss: 0.0014, Avg. Validate Sample Loss: 0.0042,                             L2 Loss: 0.0093\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [951/5000] -------------------- \n",
      "Epoch [951/5000], Step [1/47], Loss: 0.0035\n",
      "Epoch [951/5000], Step [9/47], Loss: 0.0008\n",
      "Epoch [951/5000], Step [18/47], Loss: 0.0174\n",
      "Epoch [951/5000], Step [27/47], Loss: 0.0017\n",
      "Epoch [951/5000], Step [36/47], Loss: 0.0197\n",
      "Epoch [951/5000], Step [45/47], Loss: 0.0063\n",
      "Epoch [951/5000], Avg. Train Sample Loss: 0.0091, Avg. Validate Sample Loss: 0.0028,                             L2 Loss: 0.0072\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [952/5000] -------------------- \n",
      "Epoch [952/5000], Step [1/47], Loss: 0.0031\n",
      "Epoch [952/5000], Step [9/47], Loss: 0.0121\n",
      "Epoch [952/5000], Step [18/47], Loss: 0.0010\n",
      "Epoch [952/5000], Step [27/47], Loss: 0.0024\n",
      "Epoch [952/5000], Step [36/47], Loss: 0.0023\n",
      "Epoch [952/5000], Step [45/47], Loss: 0.0026\n",
      "Epoch [952/5000], Avg. Train Sample Loss: 0.0042, Avg. Validate Sample Loss: 0.0013,                             L2 Loss: 0.0027\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [953/5000] -------------------- \n",
      "Epoch [953/5000], Step [1/47], Loss: 0.0013\n",
      "Epoch [953/5000], Step [9/47], Loss: 0.0011\n",
      "Epoch [953/5000], Step [18/47], Loss: 0.0014\n",
      "Epoch [953/5000], Step [27/47], Loss: 0.0013\n",
      "Epoch [953/5000], Step [36/47], Loss: 0.0128\n",
      "Epoch [953/5000], Step [45/47], Loss: 0.0031\n",
      "Epoch [953/5000], Avg. Train Sample Loss: 0.0033, Avg. Validate Sample Loss: 0.0037,                             L2 Loss: 0.0077\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [954/5000] -------------------- \n",
      "Epoch [954/5000], Step [1/47], Loss: 0.0036\n",
      "Epoch [954/5000], Step [9/47], Loss: 0.0015\n",
      "Epoch [954/5000], Step [18/47], Loss: 0.0068\n",
      "Epoch [954/5000], Step [27/47], Loss: 0.0167\n",
      "Epoch [954/5000], Step [36/47], Loss: 0.0013\n",
      "Epoch [954/5000], Step [45/47], Loss: 0.0010\n",
      "Epoch [954/5000], Avg. Train Sample Loss: 0.0069, Avg. Validate Sample Loss: 0.0061,                             L2 Loss: 0.0110\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [955/5000] -------------------- \n",
      "Epoch [955/5000], Step [1/47], Loss: 0.0058\n",
      "Epoch [955/5000], Step [9/47], Loss: 0.0014\n",
      "Epoch [955/5000], Step [18/47], Loss: 0.0022\n",
      "Epoch [955/5000], Step [27/47], Loss: 0.0006\n",
      "Epoch [955/5000], Step [36/47], Loss: 0.0054\n",
      "Epoch [955/5000], Step [45/47], Loss: 0.0021\n",
      "Epoch [955/5000], Avg. Train Sample Loss: 0.0020, Avg. Validate Sample Loss: 0.0009,                             L2 Loss: 0.0024\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [956/5000] -------------------- \n",
      "Epoch [956/5000], Step [1/47], Loss: 0.0009\n",
      "Epoch [956/5000], Step [9/47], Loss: 0.0014\n",
      "Epoch [956/5000], Step [18/47], Loss: 0.0024\n",
      "Epoch [956/5000], Step [27/47], Loss: 0.0099\n",
      "Epoch [956/5000], Step [36/47], Loss: 0.0029\n",
      "Epoch [956/5000], Step [45/47], Loss: 0.0042\n",
      "Epoch [956/5000], Avg. Train Sample Loss: 0.0030, Avg. Validate Sample Loss: 0.0092,                             L2 Loss: 0.0136\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [957/5000] -------------------- \n",
      "Epoch [957/5000], Step [1/47], Loss: 0.0079\n",
      "Epoch [957/5000], Step [9/47], Loss: 0.0006\n",
      "Epoch [957/5000], Step [18/47], Loss: 0.0021\n",
      "Epoch [957/5000], Step [27/47], Loss: 0.0180\n",
      "Epoch [957/5000], Step [36/47], Loss: 0.0047\n",
      "Epoch [957/5000], Step [45/47], Loss: 0.0720\n",
      "Epoch [957/5000], Avg. Train Sample Loss: 0.0119, Avg. Validate Sample Loss: 0.0554,                             L2 Loss: 0.0343\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [958/5000] -------------------- \n",
      "Epoch [958/5000], Step [1/47], Loss: 0.0523\n",
      "Epoch [958/5000], Step [9/47], Loss: 0.0290\n",
      "Epoch [958/5000], Step [18/47], Loss: 0.0969\n",
      "Epoch [958/5000], Step [27/47], Loss: 0.0497\n",
      "Epoch [958/5000], Step [36/47], Loss: 0.0211\n",
      "Epoch [958/5000], Step [45/47], Loss: 0.0013\n",
      "Epoch [958/5000], Avg. Train Sample Loss: 0.0344, Avg. Validate Sample Loss: 0.0018,                             L2 Loss: 0.0040\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [959/5000] -------------------- \n",
      "Epoch [959/5000], Step [1/47], Loss: 0.0023\n",
      "Epoch [959/5000], Step [9/47], Loss: 0.0083\n",
      "Epoch [959/5000], Step [18/47], Loss: 0.0140\n",
      "Epoch [959/5000], Step [27/47], Loss: 0.0070\n",
      "Epoch [959/5000], Step [36/47], Loss: 0.0007\n",
      "Epoch [959/5000], Step [45/47], Loss: 0.0006\n",
      "Epoch [959/5000], Avg. Train Sample Loss: 0.0072, Avg. Validate Sample Loss: 0.0015,                             L2 Loss: 0.0041\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [960/5000] -------------------- \n",
      "Epoch [960/5000], Step [1/47], Loss: 0.0016\n",
      "Epoch [960/5000], Step [9/47], Loss: 0.0010\n",
      "Epoch [960/5000], Step [18/47], Loss: 0.0008\n",
      "Epoch [960/5000], Step [27/47], Loss: 0.0027\n",
      "Epoch [960/5000], Step [36/47], Loss: 0.0008\n",
      "Epoch [960/5000], Step [45/47], Loss: 0.0021\n",
      "Epoch [960/5000], Avg. Train Sample Loss: 0.0011, Avg. Validate Sample Loss: 0.0025,                             L2 Loss: 0.0061\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [961/5000] -------------------- \n",
      "Epoch [961/5000], Step [1/47], Loss: 0.0028\n",
      "Epoch [961/5000], Step [9/47], Loss: 0.0012\n",
      "Epoch [961/5000], Step [18/47], Loss: 0.0007\n",
      "Epoch [961/5000], Step [27/47], Loss: 0.0026\n",
      "Epoch [961/5000], Step [36/47], Loss: 0.0024\n",
      "Epoch [961/5000], Step [45/47], Loss: 0.0017\n",
      "Epoch [961/5000], Avg. Train Sample Loss: 0.0021, Avg. Validate Sample Loss: 0.0014,                             L2 Loss: 0.0044\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [962/5000] -------------------- \n",
      "Epoch [962/5000], Step [1/47], Loss: 0.0009\n",
      "Epoch [962/5000], Step [9/47], Loss: 0.0010\n",
      "Epoch [962/5000], Step [18/47], Loss: 0.0008\n",
      "Epoch [962/5000], Step [27/47], Loss: 0.0018\n",
      "Epoch [962/5000], Step [36/47], Loss: 0.0012\n",
      "Epoch [962/5000], Step [45/47], Loss: 0.0006\n",
      "Epoch [962/5000], Avg. Train Sample Loss: 0.0017, Avg. Validate Sample Loss: 0.0010,                             L2 Loss: 0.0032\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [963/5000] -------------------- \n",
      "Epoch [963/5000], Step [1/47], Loss: 0.0008\n",
      "Epoch [963/5000], Step [9/47], Loss: 0.0009\n",
      "Epoch [963/5000], Step [18/47], Loss: 0.0019\n",
      "Epoch [963/5000], Step [27/47], Loss: 0.0015\n",
      "Epoch [963/5000], Step [36/47], Loss: 0.0015\n",
      "Epoch [963/5000], Step [45/47], Loss: 0.0026\n",
      "Epoch [963/5000], Avg. Train Sample Loss: 0.0014, Avg. Validate Sample Loss: 0.0025,                             L2 Loss: 0.0063\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [964/5000] -------------------- \n",
      "Epoch [964/5000], Step [1/47], Loss: 0.0025\n",
      "Epoch [964/5000], Step [9/47], Loss: 0.0048\n",
      "Epoch [964/5000], Step [18/47], Loss: 0.0145\n",
      "Epoch [964/5000], Step [27/47], Loss: 0.0101\n",
      "Epoch [964/5000], Step [36/47], Loss: 0.0011\n",
      "Epoch [964/5000], Step [45/47], Loss: 0.0091\n",
      "Epoch [964/5000], Avg. Train Sample Loss: 0.0069, Avg. Validate Sample Loss: 0.0050,                             L2 Loss: 0.0103\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [965/5000] -------------------- \n",
      "Epoch [965/5000], Step [1/47], Loss: 0.0075\n",
      "Epoch [965/5000], Step [9/47], Loss: 0.0016\n",
      "Epoch [965/5000], Step [18/47], Loss: 0.0025\n",
      "Epoch [965/5000], Step [27/47], Loss: 0.0011\n",
      "Epoch [965/5000], Step [36/47], Loss: 0.0009\n",
      "Epoch [965/5000], Step [45/47], Loss: 0.0012\n",
      "Epoch [965/5000], Avg. Train Sample Loss: 0.0035, Avg. Validate Sample Loss: 0.0008,                             L2 Loss: 0.0015\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [966/5000] -------------------- \n",
      "Epoch [966/5000], Step [1/47], Loss: 0.0022\n",
      "Epoch [966/5000], Step [9/47], Loss: 0.0007\n",
      "Epoch [966/5000], Step [18/47], Loss: 0.0022\n",
      "Epoch [966/5000], Step [27/47], Loss: 0.0007\n",
      "Epoch [966/5000], Step [36/47], Loss: 0.0012\n",
      "Epoch [966/5000], Step [45/47], Loss: 0.0020\n",
      "Epoch [966/5000], Avg. Train Sample Loss: 0.0016, Avg. Validate Sample Loss: 0.0048,                             L2 Loss: 0.0096\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [967/5000] -------------------- \n",
      "Epoch [967/5000], Step [1/47], Loss: 0.0045\n",
      "Epoch [967/5000], Step [9/47], Loss: 0.0015\n",
      "Epoch [967/5000], Step [18/47], Loss: 0.0020\n",
      "Epoch [967/5000], Step [27/47], Loss: 0.0015\n",
      "Epoch [967/5000], Step [36/47], Loss: 0.0018\n",
      "Epoch [967/5000], Step [45/47], Loss: 0.0004\n",
      "Epoch [967/5000], Avg. Train Sample Loss: 0.0019, Avg. Validate Sample Loss: 0.0008,                             L2 Loss: 0.0014\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [968/5000] -------------------- \n",
      "Epoch [968/5000], Step [1/47], Loss: 0.0015\n",
      "Epoch [968/5000], Step [9/47], Loss: 0.0013\n",
      "Epoch [968/5000], Step [18/47], Loss: 0.0006\n",
      "Epoch [968/5000], Step [27/47], Loss: 0.0008\n",
      "Epoch [968/5000], Step [36/47], Loss: 0.0010\n",
      "Epoch [968/5000], Step [45/47], Loss: 0.0010\n",
      "Epoch [968/5000], Avg. Train Sample Loss: 0.0019, Avg. Validate Sample Loss: 0.0008,                             L2 Loss: 0.0009\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [969/5000] -------------------- \n",
      "Epoch [969/5000], Step [1/47], Loss: 0.0007\n",
      "Epoch [969/5000], Step [9/47], Loss: 0.0005\n",
      "Epoch [969/5000], Step [18/47], Loss: 0.0014\n",
      "Epoch [969/5000], Step [27/47], Loss: 0.0017\n",
      "Epoch [969/5000], Step [36/47], Loss: 0.0010\n",
      "Epoch [969/5000], Step [45/47], Loss: 0.0017\n",
      "Epoch [969/5000], Avg. Train Sample Loss: 0.0015, Avg. Validate Sample Loss: 0.0018,                             L2 Loss: 0.0049\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [970/5000] -------------------- \n",
      "Epoch [970/5000], Step [1/47], Loss: 0.0015\n",
      "Epoch [970/5000], Step [9/47], Loss: 0.0014\n",
      "Epoch [970/5000], Step [18/47], Loss: 0.0026\n",
      "Epoch [970/5000], Step [27/47], Loss: 0.0006\n",
      "Epoch [970/5000], Step [36/47], Loss: 0.0007\n",
      "Epoch [970/5000], Step [45/47], Loss: 0.0054\n",
      "Epoch [970/5000], Avg. Train Sample Loss: 0.0020, Avg. Validate Sample Loss: 0.0114,                             L2 Loss: 0.0158\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [971/5000] -------------------- \n",
      "Epoch [971/5000], Step [1/47], Loss: 0.0136\n",
      "Epoch [971/5000], Step [9/47], Loss: 0.0083\n",
      "Epoch [971/5000], Step [18/47], Loss: 0.0054\n",
      "Epoch [971/5000], Step [27/47], Loss: 0.0012\n",
      "Epoch [971/5000], Step [36/47], Loss: 0.0009\n",
      "Epoch [971/5000], Step [45/47], Loss: 0.0021\n",
      "Epoch [971/5000], Avg. Train Sample Loss: 0.0037, Avg. Validate Sample Loss: 0.0007,                             L2 Loss: 0.0016\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [972/5000] -------------------- \n",
      "Epoch [972/5000], Step [1/47], Loss: 0.0007\n",
      "Epoch [972/5000], Step [9/47], Loss: 0.0022\n",
      "Epoch [972/5000], Step [18/47], Loss: 0.0019\n",
      "Epoch [972/5000], Step [27/47], Loss: 0.0010\n",
      "Epoch [972/5000], Step [36/47], Loss: 0.0072\n",
      "Epoch [972/5000], Step [45/47], Loss: 0.0014\n",
      "Epoch [972/5000], Avg. Train Sample Loss: 0.0030, Avg. Validate Sample Loss: 0.0063,                             L2 Loss: 0.0112\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [973/5000] -------------------- \n",
      "Epoch [973/5000], Step [1/47], Loss: 0.0049\n",
      "Epoch [973/5000], Step [9/47], Loss: 0.0026\n",
      "Epoch [973/5000], Step [18/47], Loss: 0.0015\n",
      "Epoch [973/5000], Step [27/47], Loss: 0.0051\n",
      "Epoch [973/5000], Step [36/47], Loss: 0.0048\n",
      "Epoch [973/5000], Step [45/47], Loss: 0.0015\n",
      "Epoch [973/5000], Avg. Train Sample Loss: 0.0026, Avg. Validate Sample Loss: 0.0018,                             L2 Loss: 0.0047\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [974/5000] -------------------- \n",
      "Epoch [974/5000], Step [1/47], Loss: 0.0012\n",
      "Epoch [974/5000], Step [9/47], Loss: 0.0023\n",
      "Epoch [974/5000], Step [18/47], Loss: 0.0035\n",
      "Epoch [974/5000], Step [27/47], Loss: 0.0016\n",
      "Epoch [974/5000], Step [36/47], Loss: 0.0013\n",
      "Epoch [974/5000], Step [45/47], Loss: 0.0014\n",
      "Epoch [974/5000], Avg. Train Sample Loss: 0.0022, Avg. Validate Sample Loss: 0.0019,                             L2 Loss: 0.0050\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [975/5000] -------------------- \n",
      "Epoch [975/5000], Step [1/47], Loss: 0.0023\n",
      "Epoch [975/5000], Step [9/47], Loss: 0.0008\n",
      "Epoch [975/5000], Step [18/47], Loss: 0.0006\n",
      "Epoch [975/5000], Step [27/47], Loss: 0.0012\n",
      "Epoch [975/5000], Step [36/47], Loss: 0.0035\n",
      "Epoch [975/5000], Step [45/47], Loss: 0.0019\n",
      "Epoch [975/5000], Avg. Train Sample Loss: 0.0043, Avg. Validate Sample Loss: 0.0277,                             L2 Loss: 0.0247\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [976/5000] -------------------- \n",
      "Epoch [976/5000], Step [1/47], Loss: 0.0297\n",
      "Epoch [976/5000], Step [9/47], Loss: 0.0016\n",
      "Epoch [976/5000], Step [18/47], Loss: 0.0247\n",
      "Epoch [976/5000], Step [27/47], Loss: 0.0101\n",
      "Epoch [976/5000], Step [36/47], Loss: 0.0018\n",
      "Epoch [976/5000], Step [45/47], Loss: 0.0020\n",
      "Epoch [976/5000], Avg. Train Sample Loss: 0.0076, Avg. Validate Sample Loss: 0.0010,                             L2 Loss: 0.0015\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [977/5000] -------------------- \n",
      "Epoch [977/5000], Step [1/47], Loss: 0.0007\n",
      "Epoch [977/5000], Step [9/47], Loss: 0.0009\n",
      "Epoch [977/5000], Step [18/47], Loss: 0.0007\n",
      "Epoch [977/5000], Step [27/47], Loss: 0.0013\n",
      "Epoch [977/5000], Step [36/47], Loss: 0.0013\n",
      "Epoch [977/5000], Step [45/47], Loss: 0.0015\n",
      "Epoch [977/5000], Avg. Train Sample Loss: 0.0027, Avg. Validate Sample Loss: 0.0034,                             L2 Loss: 0.0078\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [978/5000] -------------------- \n",
      "Epoch [978/5000], Step [1/47], Loss: 0.0026\n",
      "Epoch [978/5000], Step [9/47], Loss: 0.0027\n",
      "Epoch [978/5000], Step [18/47], Loss: 0.0013\n",
      "Epoch [978/5000], Step [27/47], Loss: 0.0010\n",
      "Epoch [978/5000], Step [36/47], Loss: 0.0019\n",
      "Epoch [978/5000], Step [45/47], Loss: 0.0018\n",
      "Epoch [978/5000], Avg. Train Sample Loss: 0.0021, Avg. Validate Sample Loss: 0.0009,                             L2 Loss: 0.0009\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [979/5000] -------------------- \n",
      "Epoch [979/5000], Step [1/47], Loss: 0.0007\n",
      "Epoch [979/5000], Step [9/47], Loss: 0.0006\n",
      "Epoch [979/5000], Step [18/47], Loss: 0.0026\n",
      "Epoch [979/5000], Step [27/47], Loss: 0.0007\n",
      "Epoch [979/5000], Step [36/47], Loss: 0.0051\n",
      "Epoch [979/5000], Step [45/47], Loss: 0.0112\n",
      "Epoch [979/5000], Avg. Train Sample Loss: 0.0053, Avg. Validate Sample Loss: 0.0174,                             L2 Loss: 0.0198\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [980/5000] -------------------- \n",
      "Epoch [980/5000], Step [1/47], Loss: 0.0162\n",
      "Epoch [980/5000], Step [9/47], Loss: 0.0061\n",
      "Epoch [980/5000], Step [18/47], Loss: 0.0159\n",
      "Epoch [980/5000], Step [27/47], Loss: 0.0201\n",
      "Epoch [980/5000], Step [36/47], Loss: 0.0021\n",
      "Epoch [980/5000], Step [45/47], Loss: 0.0029\n",
      "Epoch [980/5000], Avg. Train Sample Loss: 0.0134, Avg. Validate Sample Loss: 0.0044,                             L2 Loss: 0.0087\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [981/5000] -------------------- \n",
      "Epoch [981/5000], Step [1/47], Loss: 0.0041\n",
      "Epoch [981/5000], Step [9/47], Loss: 0.0031\n",
      "Epoch [981/5000], Step [18/47], Loss: 0.0016\n",
      "Epoch [981/5000], Step [27/47], Loss: 0.0022\n",
      "Epoch [981/5000], Step [36/47], Loss: 0.0007\n",
      "Epoch [981/5000], Step [45/47], Loss: 0.0017\n",
      "Epoch [981/5000], Avg. Train Sample Loss: 0.0026, Avg. Validate Sample Loss: 0.0016,                             L2 Loss: 0.0045\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [982/5000] -------------------- \n",
      "Epoch [982/5000], Step [1/47], Loss: 0.0022\n",
      "Epoch [982/5000], Step [9/47], Loss: 0.0021\n",
      "Epoch [982/5000], Step [18/47], Loss: 0.0047\n",
      "Epoch [982/5000], Step [27/47], Loss: 0.0179\n",
      "Epoch [982/5000], Step [36/47], Loss: 0.0068\n",
      "Epoch [982/5000], Step [45/47], Loss: 0.0118\n",
      "Epoch [982/5000], Avg. Train Sample Loss: 0.0064, Avg. Validate Sample Loss: 0.0103,                             L2 Loss: 0.0151\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [983/5000] -------------------- \n",
      "Epoch [983/5000], Step [1/47], Loss: 0.0112\n",
      "Epoch [983/5000], Step [9/47], Loss: 0.0128\n",
      "Epoch [983/5000], Step [18/47], Loss: 0.0090\n",
      "Epoch [983/5000], Step [27/47], Loss: 0.0010\n",
      "Epoch [983/5000], Step [36/47], Loss: 0.0026\n",
      "Epoch [983/5000], Step [45/47], Loss: 0.0019\n",
      "Epoch [983/5000], Avg. Train Sample Loss: 0.0043, Avg. Validate Sample Loss: 0.0007,                             L2 Loss: 0.0011\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [984/5000] -------------------- \n",
      "Epoch [984/5000], Step [1/47], Loss: 0.0005\n",
      "Epoch [984/5000], Step [9/47], Loss: 0.0004\n",
      "Epoch [984/5000], Step [18/47], Loss: 0.0006\n",
      "Epoch [984/5000], Step [27/47], Loss: 0.0069\n",
      "Epoch [984/5000], Step [36/47], Loss: 0.0178\n",
      "Epoch [984/5000], Step [45/47], Loss: 0.0022\n",
      "Epoch [984/5000], Avg. Train Sample Loss: 0.0057, Avg. Validate Sample Loss: 0.0015,                             L2 Loss: 0.0043\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [985/5000] -------------------- \n",
      "Epoch [985/5000], Step [1/47], Loss: 0.0015\n",
      "Epoch [985/5000], Step [9/47], Loss: 0.0102\n",
      "Epoch [985/5000], Step [18/47], Loss: 0.0164\n",
      "Epoch [985/5000], Step [27/47], Loss: 0.0015\n",
      "Epoch [985/5000], Step [36/47], Loss: 0.0030\n",
      "Epoch [985/5000], Step [45/47], Loss: 0.0041\n",
      "Epoch [985/5000], Avg. Train Sample Loss: 0.0077, Avg. Validate Sample Loss: 0.0069,                             L2 Loss: 0.0116\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [986/5000] -------------------- \n",
      "Epoch [986/5000], Step [1/47], Loss: 0.0074\n",
      "Epoch [986/5000], Step [9/47], Loss: 0.0028\n",
      "Epoch [986/5000], Step [18/47], Loss: 0.0034\n",
      "Epoch [986/5000], Step [27/47], Loss: 0.0039\n",
      "Epoch [986/5000], Step [36/47], Loss: 0.0015\n",
      "Epoch [986/5000], Step [45/47], Loss: 0.0021\n",
      "Epoch [986/5000], Avg. Train Sample Loss: 0.0044, Avg. Validate Sample Loss: 0.0047,                             L2 Loss: 0.0095\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [987/5000] -------------------- \n",
      "Epoch [987/5000], Step [1/47], Loss: 0.0038\n",
      "Epoch [987/5000], Step [9/47], Loss: 0.0030\n",
      "Epoch [987/5000], Step [18/47], Loss: 0.0026\n",
      "Epoch [987/5000], Step [27/47], Loss: 0.0021\n",
      "Epoch [987/5000], Step [36/47], Loss: 0.0039\n",
      "Epoch [987/5000], Step [45/47], Loss: 0.0161\n",
      "Epoch [987/5000], Avg. Train Sample Loss: 0.0103, Avg. Validate Sample Loss: 0.0142,                             L2 Loss: 0.0178\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [988/5000] -------------------- \n",
      "Epoch [988/5000], Step [1/47], Loss: 0.0157\n",
      "Epoch [988/5000], Step [9/47], Loss: 0.0026\n",
      "Epoch [988/5000], Step [18/47], Loss: 0.0032\n",
      "Epoch [988/5000], Step [27/47], Loss: 0.0024\n",
      "Epoch [988/5000], Step [36/47], Loss: 0.0020\n",
      "Epoch [988/5000], Step [45/47], Loss: 0.0008\n",
      "Epoch [988/5000], Avg. Train Sample Loss: 0.0024, Avg. Validate Sample Loss: 0.0014,                             L2 Loss: 0.0037\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [989/5000] -------------------- \n",
      "Epoch [989/5000], Step [1/47], Loss: 0.0018\n",
      "Epoch [989/5000], Step [9/47], Loss: 0.0015\n",
      "Epoch [989/5000], Step [18/47], Loss: 0.0008\n",
      "Epoch [989/5000], Step [27/47], Loss: 0.0005\n",
      "Epoch [989/5000], Step [36/47], Loss: 0.0146\n",
      "Epoch [989/5000], Step [45/47], Loss: 0.0021\n",
      "Epoch [989/5000], Avg. Train Sample Loss: 0.0030, Avg. Validate Sample Loss: 0.0040,                             L2 Loss: 0.0087\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [990/5000] -------------------- \n",
      "Epoch [990/5000], Step [1/47], Loss: 0.0035\n",
      "Epoch [990/5000], Step [9/47], Loss: 0.0033\n",
      "Epoch [990/5000], Step [18/47], Loss: 0.0033\n",
      "Epoch [990/5000], Step [27/47], Loss: 0.0135\n",
      "Epoch [990/5000], Step [36/47], Loss: 0.0016\n",
      "Epoch [990/5000], Step [45/47], Loss: 0.0011\n",
      "Epoch [990/5000], Avg. Train Sample Loss: 0.0037, Avg. Validate Sample Loss: 0.0022,                             L2 Loss: 0.0057\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [991/5000] -------------------- \n",
      "Epoch [991/5000], Step [1/47], Loss: 0.0022\n",
      "Epoch [991/5000], Step [9/47], Loss: 0.0074\n",
      "Epoch [991/5000], Step [18/47], Loss: 0.0006\n",
      "Epoch [991/5000], Step [27/47], Loss: 0.0009\n",
      "Epoch [991/5000], Step [36/47], Loss: 0.0006\n",
      "Epoch [991/5000], Step [45/47], Loss: 0.0072\n",
      "Epoch [991/5000], Avg. Train Sample Loss: 0.0030, Avg. Validate Sample Loss: 0.0019,                             L2 Loss: 0.0051\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [992/5000] -------------------- \n",
      "Epoch [992/5000], Step [1/47], Loss: 0.0021\n",
      "Epoch [992/5000], Step [9/47], Loss: 0.0007\n",
      "Epoch [992/5000], Step [18/47], Loss: 0.0021\n",
      "Epoch [992/5000], Step [27/47], Loss: 0.0015\n",
      "Epoch [992/5000], Step [36/47], Loss: 0.0022\n",
      "Epoch [992/5000], Step [45/47], Loss: 0.0010\n",
      "Epoch [992/5000], Avg. Train Sample Loss: 0.0033, Avg. Validate Sample Loss: 0.0008,                             L2 Loss: 0.0011\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [993/5000] -------------------- \n",
      "Epoch [993/5000], Step [1/47], Loss: 0.0007\n",
      "Epoch [993/5000], Step [9/47], Loss: 0.0011\n",
      "Epoch [993/5000], Step [18/47], Loss: 0.0023\n",
      "Epoch [993/5000], Step [27/47], Loss: 0.0015\n",
      "Epoch [993/5000], Step [36/47], Loss: 0.0199\n",
      "Epoch [993/5000], Step [45/47], Loss: 0.0026\n",
      "Epoch [993/5000], Avg. Train Sample Loss: 0.0046, Avg. Validate Sample Loss: 0.0041,                             L2 Loss: 0.0089\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [994/5000] -------------------- \n",
      "Epoch [994/5000], Step [1/47], Loss: 0.0037\n",
      "Epoch [994/5000], Step [9/47], Loss: 0.0026\n",
      "Epoch [994/5000], Step [18/47], Loss: 0.0049\n",
      "Epoch [994/5000], Step [27/47], Loss: 0.0015\n",
      "Epoch [994/5000], Step [36/47], Loss: 0.0048\n",
      "Epoch [994/5000], Step [45/47], Loss: 0.0136\n",
      "Epoch [994/5000], Avg. Train Sample Loss: 0.0037, Avg. Validate Sample Loss: 0.0080,                             L2 Loss: 0.0126\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [995/5000] -------------------- \n",
      "Epoch [995/5000], Step [1/47], Loss: 0.0073\n",
      "Epoch [995/5000], Step [9/47], Loss: 0.0018\n",
      "Epoch [995/5000], Step [18/47], Loss: 0.0059\n",
      "Epoch [995/5000], Step [27/47], Loss: 0.0009\n",
      "Epoch [995/5000], Step [36/47], Loss: 0.0005\n",
      "Epoch [995/5000], Step [45/47], Loss: 0.0015\n",
      "Epoch [995/5000], Avg. Train Sample Loss: 0.0022, Avg. Validate Sample Loss: 0.0009,                             L2 Loss: 0.0015\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [996/5000] -------------------- \n",
      "Epoch [996/5000], Step [1/47], Loss: 0.0006\n",
      "Epoch [996/5000], Step [9/47], Loss: 0.0015\n",
      "Epoch [996/5000], Step [18/47], Loss: 0.0012\n",
      "Epoch [996/5000], Step [27/47], Loss: 0.0011\n",
      "Epoch [996/5000], Step [36/47], Loss: 0.0008\n",
      "Epoch [996/5000], Step [45/47], Loss: 0.0015\n",
      "Epoch [996/5000], Avg. Train Sample Loss: 0.0016, Avg. Validate Sample Loss: 0.0017,                             L2 Loss: 0.0045\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [997/5000] -------------------- \n",
      "Epoch [997/5000], Step [1/47], Loss: 0.0013\n",
      "Epoch [997/5000], Step [9/47], Loss: 0.0009\n",
      "Epoch [997/5000], Step [18/47], Loss: 0.0018\n",
      "Epoch [997/5000], Step [27/47], Loss: 0.0009\n",
      "Epoch [997/5000], Step [36/47], Loss: 0.0032\n",
      "Epoch [997/5000], Step [45/47], Loss: 0.0066\n",
      "Epoch [997/5000], Avg. Train Sample Loss: 0.0028, Avg. Validate Sample Loss: 0.0096,                             L2 Loss: 0.0148\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [998/5000] -------------------- \n",
      "Epoch [998/5000], Step [1/47], Loss: 0.0099\n",
      "Epoch [998/5000], Step [9/47], Loss: 0.0025\n",
      "Epoch [998/5000], Step [18/47], Loss: 0.0126\n",
      "Epoch [998/5000], Step [27/47], Loss: 0.0018\n",
      "Epoch [998/5000], Step [36/47], Loss: 0.0018\n",
      "Epoch [998/5000], Step [45/47], Loss: 0.0038\n",
      "Epoch [998/5000], Avg. Train Sample Loss: 0.0065, Avg. Validate Sample Loss: 0.0126,                             L2 Loss: 0.0169\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [999/5000] -------------------- \n",
      "Epoch [999/5000], Step [1/47], Loss: 0.0117\n",
      "Epoch [999/5000], Step [9/47], Loss: 0.0075\n",
      "Epoch [999/5000], Step [18/47], Loss: 0.0020\n",
      "Epoch [999/5000], Step [27/47], Loss: 0.0023\n",
      "Epoch [999/5000], Step [36/47], Loss: 0.0029\n",
      "Epoch [999/5000], Step [45/47], Loss: 0.0012\n",
      "Epoch [999/5000], Avg. Train Sample Loss: 0.0059, Avg. Validate Sample Loss: 0.0015,                             L2 Loss: 0.0040\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1000/5000] -------------------- \n",
      "Epoch [1000/5000], Step [1/47], Loss: 0.0027\n",
      "Epoch [1000/5000], Step [9/47], Loss: 0.0005\n",
      "Epoch [1000/5000], Step [18/47], Loss: 0.0009\n",
      "Epoch [1000/5000], Step [27/47], Loss: 0.0012\n",
      "Epoch [1000/5000], Step [36/47], Loss: 0.0013\n",
      "Epoch [1000/5000], Step [45/47], Loss: 0.0030\n",
      "Epoch [1000/5000], Avg. Train Sample Loss: 0.0022, Avg. Validate Sample Loss: 0.0021,                             L2 Loss: 0.0056\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1001/5000] -------------------- \n",
      "Epoch [1001/5000], Step [1/47], Loss: 0.0014\n",
      "Epoch [1001/5000], Step [9/47], Loss: 0.0055\n",
      "Epoch [1001/5000], Step [18/47], Loss: 0.0020\n",
      "Epoch [1001/5000], Step [27/47], Loss: 0.0073\n",
      "Epoch [1001/5000], Step [36/47], Loss: 0.0068\n",
      "Epoch [1001/5000], Step [45/47], Loss: 0.0040\n",
      "Epoch [1001/5000], Avg. Train Sample Loss: 0.0064, Avg. Validate Sample Loss: 0.0014,                             L2 Loss: 0.0036\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1002/5000] -------------------- \n",
      "Epoch [1002/5000], Step [1/47], Loss: 0.0020\n",
      "Epoch [1002/5000], Step [9/47], Loss: 0.0012\n",
      "Epoch [1002/5000], Step [18/47], Loss: 0.0013\n",
      "Epoch [1002/5000], Step [27/47], Loss: 0.0036\n",
      "Epoch [1002/5000], Step [36/47], Loss: 0.0006\n",
      "Epoch [1002/5000], Step [45/47], Loss: 0.0014\n",
      "Epoch [1002/5000], Avg. Train Sample Loss: 0.0031, Avg. Validate Sample Loss: 0.0180,                             L2 Loss: 0.0200\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1003/5000] -------------------- \n",
      "Epoch [1003/5000], Step [1/47], Loss: 0.0181\n",
      "Epoch [1003/5000], Step [9/47], Loss: 0.0150\n",
      "Epoch [1003/5000], Step [18/47], Loss: 0.0315\n",
      "Epoch [1003/5000], Step [27/47], Loss: 0.0277\n",
      "Epoch [1003/5000], Step [36/47], Loss: 0.0073\n",
      "Epoch [1003/5000], Step [45/47], Loss: 0.0036\n",
      "Epoch [1003/5000], Avg. Train Sample Loss: 0.0122, Avg. Validate Sample Loss: 0.0040,                             L2 Loss: 0.0107\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1004/5000] -------------------- \n",
      "Epoch [1004/5000], Step [1/47], Loss: 0.0033\n",
      "Epoch [1004/5000], Step [9/47], Loss: 0.0023\n",
      "Epoch [1004/5000], Step [18/47], Loss: 0.0023\n",
      "Epoch [1004/5000], Step [27/47], Loss: 0.0009\n",
      "Epoch [1004/5000], Step [36/47], Loss: 0.0039\n",
      "Epoch [1004/5000], Step [45/47], Loss: 0.0007\n",
      "Epoch [1004/5000], Avg. Train Sample Loss: 0.0026, Avg. Validate Sample Loss: 0.0011,                             L2 Loss: 0.0032\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1005/5000] -------------------- \n",
      "Epoch [1005/5000], Step [1/47], Loss: 0.0016\n",
      "Epoch [1005/5000], Step [9/47], Loss: 0.0014\n",
      "Epoch [1005/5000], Step [18/47], Loss: 0.0007\n",
      "Epoch [1005/5000], Step [27/47], Loss: 0.0036\n",
      "Epoch [1005/5000], Step [36/47], Loss: 0.0009\n",
      "Epoch [1005/5000], Step [45/47], Loss: 0.0017\n",
      "Epoch [1005/5000], Avg. Train Sample Loss: 0.0020, Avg. Validate Sample Loss: 0.0041,                             L2 Loss: 0.0088\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1006/5000] -------------------- \n",
      "Epoch [1006/5000], Step [1/47], Loss: 0.0040\n",
      "Epoch [1006/5000], Step [9/47], Loss: 0.0017\n",
      "Epoch [1006/5000], Step [18/47], Loss: 0.0010\n",
      "Epoch [1006/5000], Step [27/47], Loss: 0.0007\n",
      "Epoch [1006/5000], Step [36/47], Loss: 0.0007\n",
      "Epoch [1006/5000], Step [45/47], Loss: 0.0127\n",
      "Epoch [1006/5000], Avg. Train Sample Loss: 0.0026, Avg. Validate Sample Loss: 0.0067,                             L2 Loss: 0.0118\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1007/5000] -------------------- \n",
      "Epoch [1007/5000], Step [1/47], Loss: 0.0049\n",
      "Epoch [1007/5000], Step [9/47], Loss: 0.0016\n",
      "Epoch [1007/5000], Step [18/47], Loss: 0.0053\n",
      "Epoch [1007/5000], Step [27/47], Loss: 0.0011\n",
      "Epoch [1007/5000], Step [36/47], Loss: 0.0027\n",
      "Epoch [1007/5000], Step [45/47], Loss: 0.0044\n",
      "Epoch [1007/5000], Avg. Train Sample Loss: 0.0026, Avg. Validate Sample Loss: 0.0071,                             L2 Loss: 0.0117\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1008/5000] -------------------- \n",
      "Epoch [1008/5000], Step [1/47], Loss: 0.0061\n",
      "Epoch [1008/5000], Step [9/47], Loss: 0.0144\n",
      "Epoch [1008/5000], Step [18/47], Loss: 0.0102\n",
      "Epoch [1008/5000], Step [27/47], Loss: 0.0013\n",
      "Epoch [1008/5000], Step [36/47], Loss: 0.0014\n",
      "Epoch [1008/5000], Step [45/47], Loss: 0.0013\n",
      "Epoch [1008/5000], Avg. Train Sample Loss: 0.0070, Avg. Validate Sample Loss: 0.0015,                             L2 Loss: 0.0040\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1009/5000] -------------------- \n",
      "Epoch [1009/5000], Step [1/47], Loss: 0.0014\n",
      "Epoch [1009/5000], Step [9/47], Loss: 0.0026\n",
      "Epoch [1009/5000], Step [18/47], Loss: 0.0016\n",
      "Epoch [1009/5000], Step [27/47], Loss: 0.0168\n",
      "Epoch [1009/5000], Step [36/47], Loss: 0.0063\n",
      "Epoch [1009/5000], Step [45/47], Loss: 0.0027\n",
      "Epoch [1009/5000], Avg. Train Sample Loss: 0.0059, Avg. Validate Sample Loss: 0.0026,                             L2 Loss: 0.0060\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1010/5000] -------------------- \n",
      "Epoch [1010/5000], Step [1/47], Loss: 0.0023\n",
      "Epoch [1010/5000], Step [9/47], Loss: 0.0008\n",
      "Epoch [1010/5000], Step [18/47], Loss: 0.0049\n",
      "Epoch [1010/5000], Step [27/47], Loss: 0.0008\n",
      "Epoch [1010/5000], Step [36/47], Loss: 0.0025\n",
      "Epoch [1010/5000], Step [45/47], Loss: 0.0006\n",
      "Epoch [1010/5000], Avg. Train Sample Loss: 0.0039, Avg. Validate Sample Loss: 0.0016,                             L2 Loss: 0.0044\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1011/5000] -------------------- \n",
      "Epoch [1011/5000], Step [1/47], Loss: 0.0015\n",
      "Epoch [1011/5000], Step [9/47], Loss: 0.0026\n",
      "Epoch [1011/5000], Step [18/47], Loss: 0.0008\n",
      "Epoch [1011/5000], Step [27/47], Loss: 0.0014\n",
      "Epoch [1011/5000], Step [36/47], Loss: 0.0006\n",
      "Epoch [1011/5000], Step [45/47], Loss: 0.0054\n",
      "Epoch [1011/5000], Avg. Train Sample Loss: 0.0023, Avg. Validate Sample Loss: 0.0162,                             L2 Loss: 0.0192\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1012/5000] -------------------- \n",
      "Epoch [1012/5000], Step [1/47], Loss: 0.0174\n",
      "Epoch [1012/5000], Step [9/47], Loss: 0.0086\n",
      "Epoch [1012/5000], Step [18/47], Loss: 0.0071\n",
      "Epoch [1012/5000], Step [27/47], Loss: 0.0015\n",
      "Epoch [1012/5000], Step [36/47], Loss: 0.0394\n",
      "Epoch [1012/5000], Step [45/47], Loss: 0.0398\n",
      "Epoch [1012/5000], Avg. Train Sample Loss: 0.0125, Avg. Validate Sample Loss: 0.0050,                             L2 Loss: 0.0101\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1013/5000] -------------------- \n",
      "Epoch [1013/5000], Step [1/47], Loss: 0.0050\n",
      "Epoch [1013/5000], Step [9/47], Loss: 0.0129\n",
      "Epoch [1013/5000], Step [18/47], Loss: 0.0101\n",
      "Epoch [1013/5000], Step [27/47], Loss: 0.0030\n",
      "Epoch [1013/5000], Step [36/47], Loss: 0.0008\n",
      "Epoch [1013/5000], Step [45/47], Loss: 0.0018\n",
      "Epoch [1013/5000], Avg. Train Sample Loss: 0.0064, Avg. Validate Sample Loss: 0.0016,                             L2 Loss: 0.0050\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1014/5000] -------------------- \n",
      "Epoch [1014/5000], Step [1/47], Loss: 0.0014\n",
      "Epoch [1014/5000], Step [9/47], Loss: 0.0077\n",
      "Epoch [1014/5000], Step [18/47], Loss: 0.0026\n",
      "Epoch [1014/5000], Step [27/47], Loss: 0.0027\n",
      "Epoch [1014/5000], Step [36/47], Loss: 0.0043\n",
      "Epoch [1014/5000], Step [45/47], Loss: 0.0026\n",
      "Epoch [1014/5000], Avg. Train Sample Loss: 0.0028, Avg. Validate Sample Loss: 0.0021,                             L2 Loss: 0.0051\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1015/5000] -------------------- \n",
      "Epoch [1015/5000], Step [1/47], Loss: 0.0017\n",
      "Epoch [1015/5000], Step [9/47], Loss: 0.0103\n",
      "Epoch [1015/5000], Step [18/47], Loss: 0.0045\n",
      "Epoch [1015/5000], Step [27/47], Loss: 0.0022\n",
      "Epoch [1015/5000], Step [36/47], Loss: 0.0007\n",
      "Epoch [1015/5000], Step [45/47], Loss: 0.0029\n",
      "Epoch [1015/5000], Avg. Train Sample Loss: 0.0030, Avg. Validate Sample Loss: 0.0038,                             L2 Loss: 0.0088\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1016/5000] -------------------- \n",
      "Epoch [1016/5000], Step [1/47], Loss: 0.0039\n",
      "Epoch [1016/5000], Step [9/47], Loss: 0.0010\n",
      "Epoch [1016/5000], Step [18/47], Loss: 0.0012\n",
      "Epoch [1016/5000], Step [27/47], Loss: 0.0011\n",
      "Epoch [1016/5000], Step [36/47], Loss: 0.0010\n",
      "Epoch [1016/5000], Step [45/47], Loss: 0.0009\n",
      "Epoch [1016/5000], Avg. Train Sample Loss: 0.0018, Avg. Validate Sample Loss: 0.0012,                             L2 Loss: 0.0035\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1017/5000] -------------------- \n",
      "Epoch [1017/5000], Step [1/47], Loss: 0.0011\n",
      "Epoch [1017/5000], Step [9/47], Loss: 0.0010\n",
      "Epoch [1017/5000], Step [18/47], Loss: 0.0041\n",
      "Epoch [1017/5000], Step [27/47], Loss: 0.0007\n",
      "Epoch [1017/5000], Step [36/47], Loss: 0.0013\n",
      "Epoch [1017/5000], Step [45/47], Loss: 0.0007\n",
      "Epoch [1017/5000], Avg. Train Sample Loss: 0.0014, Avg. Validate Sample Loss: 0.0008,                             L2 Loss: 0.0011\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1018/5000] -------------------- \n",
      "Epoch [1018/5000], Step [1/47], Loss: 0.0007\n",
      "Epoch [1018/5000], Step [9/47], Loss: 0.0017\n",
      "Epoch [1018/5000], Step [18/47], Loss: 0.0024\n",
      "Epoch [1018/5000], Step [27/47], Loss: 0.0054\n",
      "Epoch [1018/5000], Step [36/47], Loss: 0.0012\n",
      "Epoch [1018/5000], Step [45/47], Loss: 0.0025\n",
      "Epoch [1018/5000], Avg. Train Sample Loss: 0.0026, Avg. Validate Sample Loss: 0.0024,                             L2 Loss: 0.0064\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1019/5000] -------------------- \n",
      "Epoch [1019/5000], Step [1/47], Loss: 0.0022\n",
      "Epoch [1019/5000], Step [9/47], Loss: 0.0006\n",
      "Epoch [1019/5000], Step [18/47], Loss: 0.0053\n",
      "Epoch [1019/5000], Step [27/47], Loss: 0.0034\n",
      "Epoch [1019/5000], Step [36/47], Loss: 0.0007\n",
      "Epoch [1019/5000], Step [45/47], Loss: 0.0076\n",
      "Epoch [1019/5000], Avg. Train Sample Loss: 0.0018, Avg. Validate Sample Loss: 0.0041,                             L2 Loss: 0.0090\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1020/5000] -------------------- \n",
      "Epoch [1020/5000], Step [1/47], Loss: 0.0033\n",
      "Epoch [1020/5000], Step [9/47], Loss: 0.0022\n",
      "Epoch [1020/5000], Step [18/47], Loss: 0.0013\n",
      "Epoch [1020/5000], Step [27/47], Loss: 0.0017\n",
      "Epoch [1020/5000], Step [36/47], Loss: 0.0013\n",
      "Epoch [1020/5000], Step [45/47], Loss: 0.0005\n",
      "Epoch [1020/5000], Avg. Train Sample Loss: 0.0015, Avg. Validate Sample Loss: 0.0008,                             L2 Loss: 0.0018\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1021/5000] -------------------- \n",
      "Epoch [1021/5000], Step [1/47], Loss: 0.0008\n",
      "Epoch [1021/5000], Step [9/47], Loss: 0.0015\n",
      "Epoch [1021/5000], Step [18/47], Loss: 0.0035\n",
      "Epoch [1021/5000], Step [27/47], Loss: 0.0018\n",
      "Epoch [1021/5000], Step [36/47], Loss: 0.0196\n",
      "Epoch [1021/5000], Step [45/47], Loss: 0.0119\n",
      "Epoch [1021/5000], Avg. Train Sample Loss: 0.0045, Avg. Validate Sample Loss: 0.0022,                             L2 Loss: 0.0056\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1022/5000] -------------------- \n",
      "Epoch [1022/5000], Step [1/47], Loss: 0.0013\n",
      "Epoch [1022/5000], Step [9/47], Loss: 0.0084\n",
      "Epoch [1022/5000], Step [18/47], Loss: 0.0051\n",
      "Epoch [1022/5000], Step [27/47], Loss: 0.0024\n",
      "Epoch [1022/5000], Step [36/47], Loss: 0.0056\n",
      "Epoch [1022/5000], Step [45/47], Loss: 0.0089\n",
      "Epoch [1022/5000], Avg. Train Sample Loss: 0.0054, Avg. Validate Sample Loss: 0.0015,                             L2 Loss: 0.0017\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1023/5000] -------------------- \n",
      "Epoch [1023/5000], Step [1/47], Loss: 0.0013\n",
      "Epoch [1023/5000], Step [9/47], Loss: 0.0026\n",
      "Epoch [1023/5000], Step [18/47], Loss: 0.0007\n",
      "Epoch [1023/5000], Step [27/47], Loss: 0.0070\n",
      "Epoch [1023/5000], Step [36/47], Loss: 0.0146\n",
      "Epoch [1023/5000], Step [45/47], Loss: 0.0582\n",
      "Epoch [1023/5000], Avg. Train Sample Loss: 0.0072, Avg. Validate Sample Loss: 0.0207,                             L2 Loss: 0.0214\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1024/5000] -------------------- \n",
      "Epoch [1024/5000], Step [1/47], Loss: 0.0183\n",
      "Epoch [1024/5000], Step [9/47], Loss: 0.0055\n",
      "Epoch [1024/5000], Step [18/47], Loss: 0.0014\n",
      "Epoch [1024/5000], Step [27/47], Loss: 0.0006\n",
      "Epoch [1024/5000], Step [36/47], Loss: 0.0029\n",
      "Epoch [1024/5000], Step [45/47], Loss: 0.0033\n",
      "Epoch [1024/5000], Avg. Train Sample Loss: 0.0056, Avg. Validate Sample Loss: 0.0016,                             L2 Loss: 0.0050\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1025/5000] -------------------- \n",
      "Epoch [1025/5000], Step [1/47], Loss: 0.0014\n",
      "Epoch [1025/5000], Step [9/47], Loss: 0.0044\n",
      "Epoch [1025/5000], Step [18/47], Loss: 0.0013\n",
      "Epoch [1025/5000], Step [27/47], Loss: 0.0044\n",
      "Epoch [1025/5000], Step [36/47], Loss: 0.0042\n",
      "Epoch [1025/5000], Step [45/47], Loss: 0.0062\n",
      "Epoch [1025/5000], Avg. Train Sample Loss: 0.0038, Avg. Validate Sample Loss: 0.0024,                             L2 Loss: 0.0059\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1026/5000] -------------------- \n",
      "Epoch [1026/5000], Step [1/47], Loss: 0.0038\n",
      "Epoch [1026/5000], Step [9/47], Loss: 0.0226\n",
      "Epoch [1026/5000], Step [18/47], Loss: 0.0015\n",
      "Epoch [1026/5000], Step [27/47], Loss: 0.0056\n",
      "Epoch [1026/5000], Step [36/47], Loss: 0.0007\n",
      "Epoch [1026/5000], Step [45/47], Loss: 0.0082\n",
      "Epoch [1026/5000], Avg. Train Sample Loss: 0.0057, Avg. Validate Sample Loss: 0.0075,                             L2 Loss: 0.0125\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1027/5000] -------------------- \n",
      "Epoch [1027/5000], Step [1/47], Loss: 0.0072\n",
      "Epoch [1027/5000], Step [9/47], Loss: 0.0172\n",
      "Epoch [1027/5000], Step [18/47], Loss: 0.0010\n",
      "Epoch [1027/5000], Step [27/47], Loss: 0.0088\n",
      "Epoch [1027/5000], Step [36/47], Loss: 0.0109\n",
      "Epoch [1027/5000], Step [45/47], Loss: 0.0014\n",
      "Epoch [1027/5000], Avg. Train Sample Loss: 0.0126, Avg. Validate Sample Loss: 0.0010,                             L2 Loss: 0.0019\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1028/5000] -------------------- \n",
      "Epoch [1028/5000], Step [1/47], Loss: 0.0010\n",
      "Epoch [1028/5000], Step [9/47], Loss: 0.0019\n",
      "Epoch [1028/5000], Step [18/47], Loss: 0.0013\n",
      "Epoch [1028/5000], Step [27/47], Loss: 0.0014\n",
      "Epoch [1028/5000], Step [36/47], Loss: 0.0008\n",
      "Epoch [1028/5000], Step [45/47], Loss: 0.0006\n",
      "Epoch [1028/5000], Avg. Train Sample Loss: 0.0013, Avg. Validate Sample Loss: 0.0011,                             L2 Loss: 0.0024\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1029/5000] -------------------- \n",
      "Epoch [1029/5000], Step [1/47], Loss: 0.0014\n",
      "Epoch [1029/5000], Step [9/47], Loss: 0.0017\n",
      "Epoch [1029/5000], Step [18/47], Loss: 0.0020\n",
      "Epoch [1029/5000], Step [27/47], Loss: 0.0030\n",
      "Epoch [1029/5000], Step [36/47], Loss: 0.0013\n",
      "Epoch [1029/5000], Step [45/47], Loss: 0.0009\n",
      "Epoch [1029/5000], Avg. Train Sample Loss: 0.0021, Avg. Validate Sample Loss: 0.0010,                             L2 Loss: 0.0020\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1030/5000] -------------------- \n",
      "Epoch [1030/5000], Step [1/47], Loss: 0.0007\n",
      "Epoch [1030/5000], Step [9/47], Loss: 0.0004\n",
      "Epoch [1030/5000], Step [18/47], Loss: 0.0010\n",
      "Epoch [1030/5000], Step [27/47], Loss: 0.0010\n",
      "Epoch [1030/5000], Step [36/47], Loss: 0.0007\n",
      "Epoch [1030/5000], Step [45/47], Loss: 0.0019\n",
      "Epoch [1030/5000], Avg. Train Sample Loss: 0.0022, Avg. Validate Sample Loss: 0.0010,                             L2 Loss: 0.0013\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1031/5000] -------------------- \n",
      "Epoch [1031/5000], Step [1/47], Loss: 0.0009\n",
      "Epoch [1031/5000], Step [9/47], Loss: 0.0008\n",
      "Epoch [1031/5000], Step [18/47], Loss: 0.0009\n",
      "Epoch [1031/5000], Step [27/47], Loss: 0.0006\n",
      "Epoch [1031/5000], Step [36/47], Loss: 0.0015\n",
      "Epoch [1031/5000], Step [45/47], Loss: 0.0017\n",
      "Epoch [1031/5000], Avg. Train Sample Loss: 0.0018, Avg. Validate Sample Loss: 0.0012,                             L2 Loss: 0.0031\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1032/5000] -------------------- \n",
      "Epoch [1032/5000], Step [1/47], Loss: 0.0008\n",
      "Epoch [1032/5000], Step [9/47], Loss: 0.0189\n",
      "Epoch [1032/5000], Step [18/47], Loss: 0.0337\n",
      "Epoch [1032/5000], Step [27/47], Loss: 0.0384\n",
      "Epoch [1032/5000], Step [36/47], Loss: 0.0102\n",
      "Epoch [1032/5000], Step [45/47], Loss: 0.0077\n",
      "Epoch [1032/5000], Avg. Train Sample Loss: 0.0145, Avg. Validate Sample Loss: 0.0044,                             L2 Loss: 0.0090\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1033/5000] -------------------- \n",
      "Epoch [1033/5000], Step [1/47], Loss: 0.0044\n",
      "Epoch [1033/5000], Step [9/47], Loss: 0.0067\n",
      "Epoch [1033/5000], Step [18/47], Loss: 0.0096\n",
      "Epoch [1033/5000], Step [27/47], Loss: 0.0081\n",
      "Epoch [1033/5000], Step [36/47], Loss: 0.0031\n",
      "Epoch [1033/5000], Step [45/47], Loss: 0.0032\n",
      "Epoch [1033/5000], Avg. Train Sample Loss: 0.0056, Avg. Validate Sample Loss: 0.0017,                             L2 Loss: 0.0045\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1034/5000] -------------------- \n",
      "Epoch [1034/5000], Step [1/47], Loss: 0.0015\n",
      "Epoch [1034/5000], Step [9/47], Loss: 0.0028\n",
      "Epoch [1034/5000], Step [18/47], Loss: 0.0006\n",
      "Epoch [1034/5000], Step [27/47], Loss: 0.0121\n",
      "Epoch [1034/5000], Step [36/47], Loss: 0.0101\n",
      "Epoch [1034/5000], Step [45/47], Loss: 0.0012\n",
      "Epoch [1034/5000], Avg. Train Sample Loss: 0.0049, Avg. Validate Sample Loss: 0.0013,                             L2 Loss: 0.0038\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1035/5000] -------------------- \n",
      "Epoch [1035/5000], Step [1/47], Loss: 0.0011\n",
      "Epoch [1035/5000], Step [9/47], Loss: 0.0089\n",
      "Epoch [1035/5000], Step [18/47], Loss: 0.0033\n",
      "Epoch [1035/5000], Step [27/47], Loss: 0.0015\n",
      "Epoch [1035/5000], Step [36/47], Loss: 0.0023\n",
      "Epoch [1035/5000], Step [45/47], Loss: 0.0035\n",
      "Epoch [1035/5000], Avg. Train Sample Loss: 0.0043, Avg. Validate Sample Loss: 0.0010,                             L2 Loss: 0.0029\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1036/5000] -------------------- \n",
      "Epoch [1036/5000], Step [1/47], Loss: 0.0011\n",
      "Epoch [1036/5000], Step [9/47], Loss: 0.0027\n",
      "Epoch [1036/5000], Step [18/47], Loss: 0.0007\n",
      "Epoch [1036/5000], Step [27/47], Loss: 0.0039\n",
      "Epoch [1036/5000], Step [36/47], Loss: 0.0013\n",
      "Epoch [1036/5000], Step [45/47], Loss: 0.0009\n",
      "Epoch [1036/5000], Avg. Train Sample Loss: 0.0027, Avg. Validate Sample Loss: 0.0009,                             L2 Loss: 0.0017\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1037/5000] -------------------- \n",
      "Epoch [1037/5000], Step [1/47], Loss: 0.0008\n",
      "Epoch [1037/5000], Step [9/47], Loss: 0.0005\n",
      "Epoch [1037/5000], Step [18/47], Loss: 0.0022\n",
      "Epoch [1037/5000], Step [27/47], Loss: 0.0009\n",
      "Epoch [1037/5000], Step [36/47], Loss: 0.0027\n",
      "Epoch [1037/5000], Step [45/47], Loss: 0.0022\n",
      "Epoch [1037/5000], Avg. Train Sample Loss: 0.0017, Avg. Validate Sample Loss: 0.0029,                             L2 Loss: 0.0077\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1038/5000] -------------------- \n",
      "Epoch [1038/5000], Step [1/47], Loss: 0.0027\n",
      "Epoch [1038/5000], Step [9/47], Loss: 0.0005\n",
      "Epoch [1038/5000], Step [18/47], Loss: 0.0015\n",
      "Epoch [1038/5000], Step [27/47], Loss: 0.0008\n",
      "Epoch [1038/5000], Step [36/47], Loss: 0.0011\n",
      "Epoch [1038/5000], Step [45/47], Loss: 0.0008\n",
      "Epoch [1038/5000], Avg. Train Sample Loss: 0.0016, Avg. Validate Sample Loss: 0.0013,                             L2 Loss: 0.0036\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1039/5000] -------------------- \n",
      "Epoch [1039/5000], Step [1/47], Loss: 0.0015\n",
      "Epoch [1039/5000], Step [9/47], Loss: 0.0006\n",
      "Epoch [1039/5000], Step [18/47], Loss: 0.0019\n",
      "Epoch [1039/5000], Step [27/47], Loss: 0.0022\n",
      "Epoch [1039/5000], Step [36/47], Loss: 0.0030\n",
      "Epoch [1039/5000], Step [45/47], Loss: 0.0008\n",
      "Epoch [1039/5000], Avg. Train Sample Loss: 0.0015, Avg. Validate Sample Loss: 0.0040,                             L2 Loss: 0.0085\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1040/5000] -------------------- \n",
      "Epoch [1040/5000], Step [1/47], Loss: 0.0061\n",
      "Epoch [1040/5000], Step [9/47], Loss: 0.0072\n",
      "Epoch [1040/5000], Step [18/47], Loss: 0.0054\n",
      "Epoch [1040/5000], Step [27/47], Loss: 0.0037\n",
      "Epoch [1040/5000], Step [36/47], Loss: 0.0108\n",
      "Epoch [1040/5000], Step [45/47], Loss: 0.0025\n",
      "Epoch [1040/5000], Avg. Train Sample Loss: 0.0075, Avg. Validate Sample Loss: 0.0014,                             L2 Loss: 0.0031\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1041/5000] -------------------- \n",
      "Epoch [1041/5000], Step [1/47], Loss: 0.0008\n",
      "Epoch [1041/5000], Step [9/47], Loss: 0.0009\n",
      "Epoch [1041/5000], Step [18/47], Loss: 0.0018\n",
      "Epoch [1041/5000], Step [27/47], Loss: 0.0010\n",
      "Epoch [1041/5000], Step [36/47], Loss: 0.0025\n",
      "Epoch [1041/5000], Step [45/47], Loss: 0.0020\n",
      "Epoch [1041/5000], Avg. Train Sample Loss: 0.0011, Avg. Validate Sample Loss: 0.0015,                             L2 Loss: 0.0043\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1042/5000] -------------------- \n",
      "Epoch [1042/5000], Step [1/47], Loss: 0.0015\n",
      "Epoch [1042/5000], Step [9/47], Loss: 0.0012\n",
      "Epoch [1042/5000], Step [18/47], Loss: 0.0024\n",
      "Epoch [1042/5000], Step [27/47], Loss: 0.0119\n",
      "Epoch [1042/5000], Step [36/47], Loss: 0.0011\n",
      "Epoch [1042/5000], Step [45/47], Loss: 0.0052\n",
      "Epoch [1042/5000], Avg. Train Sample Loss: 0.0039, Avg. Validate Sample Loss: 0.0025,                             L2 Loss: 0.0062\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1043/5000] -------------------- \n",
      "Epoch [1043/5000], Step [1/47], Loss: 0.0021\n",
      "Epoch [1043/5000], Step [9/47], Loss: 0.0013\n",
      "Epoch [1043/5000], Step [18/47], Loss: 0.0022\n",
      "Epoch [1043/5000], Step [27/47], Loss: 0.0006\n",
      "Epoch [1043/5000], Step [36/47], Loss: 0.0011\n",
      "Epoch [1043/5000], Step [45/47], Loss: 0.0036\n",
      "Epoch [1043/5000], Avg. Train Sample Loss: 0.0015, Avg. Validate Sample Loss: 0.0016,                             L2 Loss: 0.0044\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1044/5000] -------------------- \n",
      "Epoch [1044/5000], Step [1/47], Loss: 0.0011\n",
      "Epoch [1044/5000], Step [9/47], Loss: 0.0007\n",
      "Epoch [1044/5000], Step [18/47], Loss: 0.0019\n",
      "Epoch [1044/5000], Step [27/47], Loss: 0.0013\n",
      "Epoch [1044/5000], Step [36/47], Loss: 0.0104\n",
      "Epoch [1044/5000], Step [45/47], Loss: 0.0066\n",
      "Epoch [1044/5000], Avg. Train Sample Loss: 0.0055, Avg. Validate Sample Loss: 0.0337,                             L2 Loss: 0.0282\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1045/5000] -------------------- \n",
      "Epoch [1045/5000], Step [1/47], Loss: 0.0446\n",
      "Epoch [1045/5000], Step [9/47], Loss: 0.0826\n",
      "Epoch [1045/5000], Step [18/47], Loss: 0.0621\n",
      "Epoch [1045/5000], Step [27/47], Loss: 0.0331\n",
      "Epoch [1045/5000], Step [36/47], Loss: 0.0049\n",
      "Epoch [1045/5000], Step [45/47], Loss: 0.0008\n",
      "Epoch [1045/5000], Avg. Train Sample Loss: 0.0221, Avg. Validate Sample Loss: 0.0007,                             L2 Loss: 0.0013\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1046/5000] -------------------- \n",
      "Epoch [1046/5000], Step [1/47], Loss: 0.0008\n",
      "Epoch [1046/5000], Step [9/47], Loss: 0.0008\n",
      "Epoch [1046/5000], Step [18/47], Loss: 0.0007\n",
      "Epoch [1046/5000], Step [27/47], Loss: 0.0020\n",
      "Epoch [1046/5000], Step [36/47], Loss: 0.0009\n",
      "Epoch [1046/5000], Step [45/47], Loss: 0.0005\n",
      "Epoch [1046/5000], Avg. Train Sample Loss: 0.0019, Avg. Validate Sample Loss: 0.0011,                             L2 Loss: 0.0027\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1047/5000] -------------------- \n",
      "Epoch [1047/5000], Step [1/47], Loss: 0.0010\n",
      "Epoch [1047/5000], Step [9/47], Loss: 0.0048\n",
      "Epoch [1047/5000], Step [18/47], Loss: 0.0008\n",
      "Epoch [1047/5000], Step [27/47], Loss: 0.0017\n",
      "Epoch [1047/5000], Step [36/47], Loss: 0.0011\n",
      "Epoch [1047/5000], Step [45/47], Loss: 0.0008\n",
      "Epoch [1047/5000], Avg. Train Sample Loss: 0.0014, Avg. Validate Sample Loss: 0.0014,                             L2 Loss: 0.0041\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1048/5000] -------------------- \n",
      "Epoch [1048/5000], Step [1/47], Loss: 0.0015\n",
      "Epoch [1048/5000], Step [9/47], Loss: 0.0050\n",
      "Epoch [1048/5000], Step [18/47], Loss: 0.0010\n",
      "Epoch [1048/5000], Step [27/47], Loss: 0.0105\n",
      "Epoch [1048/5000], Step [36/47], Loss: 0.0052\n",
      "Epoch [1048/5000], Step [45/47], Loss: 0.0011\n",
      "Epoch [1048/5000], Avg. Train Sample Loss: 0.0054, Avg. Validate Sample Loss: 0.0020,                             L2 Loss: 0.0060\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1049/5000] -------------------- \n",
      "Epoch [1049/5000], Step [1/47], Loss: 0.0020\n",
      "Epoch [1049/5000], Step [9/47], Loss: 0.0008\n",
      "Epoch [1049/5000], Step [18/47], Loss: 0.0012\n",
      "Epoch [1049/5000], Step [27/47], Loss: 0.0016\n",
      "Epoch [1049/5000], Step [36/47], Loss: 0.0006\n",
      "Epoch [1049/5000], Step [45/47], Loss: 0.0007\n",
      "Epoch [1049/5000], Avg. Train Sample Loss: 0.0017, Avg. Validate Sample Loss: 0.0008,                             L2 Loss: 0.0016\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1050/5000] -------------------- \n",
      "Epoch [1050/5000], Step [1/47], Loss: 0.0011\n",
      "Epoch [1050/5000], Step [9/47], Loss: 0.0019\n",
      "Epoch [1050/5000], Step [18/47], Loss: 0.0028\n",
      "Epoch [1050/5000], Step [27/47], Loss: 0.0010\n",
      "Epoch [1050/5000], Step [36/47], Loss: 0.0010\n",
      "Epoch [1050/5000], Step [45/47], Loss: 0.0011\n",
      "Epoch [1050/5000], Avg. Train Sample Loss: 0.0018, Avg. Validate Sample Loss: 0.0010,                             L2 Loss: 0.0031\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1051/5000] -------------------- \n",
      "Epoch [1051/5000], Step [1/47], Loss: 0.0017\n",
      "Epoch [1051/5000], Step [9/47], Loss: 0.0011\n",
      "Epoch [1051/5000], Step [18/47], Loss: 0.0011\n",
      "Epoch [1051/5000], Step [27/47], Loss: 0.0006\n",
      "Epoch [1051/5000], Step [36/47], Loss: 0.0073\n",
      "Epoch [1051/5000], Step [45/47], Loss: 0.0079\n",
      "Epoch [1051/5000], Avg. Train Sample Loss: 0.0045, Avg. Validate Sample Loss: 0.0012,                             L2 Loss: 0.0040\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1052/5000] -------------------- \n",
      "Epoch [1052/5000], Step [1/47], Loss: 0.0007\n",
      "Epoch [1052/5000], Step [9/47], Loss: 0.0115\n",
      "Epoch [1052/5000], Step [18/47], Loss: 0.0007\n",
      "Epoch [1052/5000], Step [27/47], Loss: 0.0017\n",
      "Epoch [1052/5000], Step [36/47], Loss: 0.0081\n",
      "Epoch [1052/5000], Step [45/47], Loss: 0.0031\n",
      "Epoch [1052/5000], Avg. Train Sample Loss: 0.0042, Avg. Validate Sample Loss: 0.0022,                             L2 Loss: 0.0061\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1053/5000] -------------------- \n",
      "Epoch [1053/5000], Step [1/47], Loss: 0.0024\n",
      "Epoch [1053/5000], Step [9/47], Loss: 0.0017\n",
      "Epoch [1053/5000], Step [18/47], Loss: 0.0043\n",
      "Epoch [1053/5000], Step [27/47], Loss: 0.0053\n",
      "Epoch [1053/5000], Step [36/47], Loss: 0.0017\n",
      "Epoch [1053/5000], Step [45/47], Loss: 0.0079\n",
      "Epoch [1053/5000], Avg. Train Sample Loss: 0.0038, Avg. Validate Sample Loss: 0.0037,                             L2 Loss: 0.0084\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1054/5000] -------------------- \n",
      "Epoch [1054/5000], Step [1/47], Loss: 0.0040\n",
      "Epoch [1054/5000], Step [9/47], Loss: 0.0015\n",
      "Epoch [1054/5000], Step [18/47], Loss: 0.0046\n",
      "Epoch [1054/5000], Step [27/47], Loss: 0.0007\n",
      "Epoch [1054/5000], Step [36/47], Loss: 0.0009\n",
      "Epoch [1054/5000], Step [45/47], Loss: 0.0008\n",
      "Epoch [1054/5000], Avg. Train Sample Loss: 0.0017, Avg. Validate Sample Loss: 0.0009,                             L2 Loss: 0.0016\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1055/5000] -------------------- \n",
      "Epoch [1055/5000], Step [1/47], Loss: 0.0024\n",
      "Epoch [1055/5000], Step [9/47], Loss: 0.0008\n",
      "Epoch [1055/5000], Step [18/47], Loss: 0.0091\n",
      "Epoch [1055/5000], Step [27/47], Loss: 0.0035\n",
      "Epoch [1055/5000], Step [36/47], Loss: 0.0074\n",
      "Epoch [1055/5000], Step [45/47], Loss: 0.0007\n",
      "Epoch [1055/5000], Avg. Train Sample Loss: 0.0048, Avg. Validate Sample Loss: 0.0011,                             L2 Loss: 0.0029\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1056/5000] -------------------- \n",
      "Epoch [1056/5000], Step [1/47], Loss: 0.0008\n",
      "Epoch [1056/5000], Step [9/47], Loss: 0.0035\n",
      "Epoch [1056/5000], Step [18/47], Loss: 0.0119\n",
      "Epoch [1056/5000], Step [27/47], Loss: 0.0008\n",
      "Epoch [1056/5000], Step [36/47], Loss: 0.0012\n",
      "Epoch [1056/5000], Step [45/47], Loss: 0.0025\n",
      "Epoch [1056/5000], Avg. Train Sample Loss: 0.0025, Avg. Validate Sample Loss: 0.0017,                             L2 Loss: 0.0048\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1057/5000] -------------------- \n",
      "Epoch [1057/5000], Step [1/47], Loss: 0.0014\n",
      "Epoch [1057/5000], Step [9/47], Loss: 0.0015\n",
      "Epoch [1057/5000], Step [18/47], Loss: 0.0076\n",
      "Epoch [1057/5000], Step [27/47], Loss: 0.0014\n",
      "Epoch [1057/5000], Step [36/47], Loss: 0.0006\n",
      "Epoch [1057/5000], Step [45/47], Loss: 0.0030\n",
      "Epoch [1057/5000], Avg. Train Sample Loss: 0.0022, Avg. Validate Sample Loss: 0.0038,                             L2 Loss: 0.0086\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1058/5000] -------------------- \n",
      "Epoch [1058/5000], Step [1/47], Loss: 0.0035\n",
      "Epoch [1058/5000], Step [9/47], Loss: 0.0011\n",
      "Epoch [1058/5000], Step [18/47], Loss: 0.0027\n",
      "Epoch [1058/5000], Step [27/47], Loss: 0.0038\n",
      "Epoch [1058/5000], Step [36/47], Loss: 0.0019\n",
      "Epoch [1058/5000], Step [45/47], Loss: 0.0018\n",
      "Epoch [1058/5000], Avg. Train Sample Loss: 0.0031, Avg. Validate Sample Loss: 0.0034,                             L2 Loss: 0.0076\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1059/5000] -------------------- \n",
      "Epoch [1059/5000], Step [1/47], Loss: 0.0028\n",
      "Epoch [1059/5000], Step [9/47], Loss: 0.0013\n",
      "Epoch [1059/5000], Step [18/47], Loss: 0.0017\n",
      "Epoch [1059/5000], Step [27/47], Loss: 0.0015\n",
      "Epoch [1059/5000], Step [36/47], Loss: 0.0015\n",
      "Epoch [1059/5000], Step [45/47], Loss: 0.0019\n",
      "Epoch [1059/5000], Avg. Train Sample Loss: 0.0020, Avg. Validate Sample Loss: 0.0009,                             L2 Loss: 0.0025\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1060/5000] -------------------- \n",
      "Epoch [1060/5000], Step [1/47], Loss: 0.0006\n",
      "Epoch [1060/5000], Step [9/47], Loss: 0.0120\n",
      "Epoch [1060/5000], Step [18/47], Loss: 0.0072\n",
      "Epoch [1060/5000], Step [27/47], Loss: 0.0029\n",
      "Epoch [1060/5000], Step [36/47], Loss: 0.0012\n",
      "Epoch [1060/5000], Step [45/47], Loss: 0.0015\n",
      "Epoch [1060/5000], Avg. Train Sample Loss: 0.0028, Avg. Validate Sample Loss: 0.0006,                             L2 Loss: 0.0012\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1061/5000] -------------------- \n",
      "Epoch [1061/5000], Step [1/47], Loss: 0.0005\n",
      "Epoch [1061/5000], Step [9/47], Loss: 0.0008\n",
      "Epoch [1061/5000], Step [18/47], Loss: 0.0026\n",
      "Epoch [1061/5000], Step [27/47], Loss: 0.0051\n",
      "Epoch [1061/5000], Step [36/47], Loss: 0.0110\n",
      "Epoch [1061/5000], Step [45/47], Loss: 0.0112\n",
      "Epoch [1061/5000], Avg. Train Sample Loss: 0.0055, Avg. Validate Sample Loss: 0.0016,                             L2 Loss: 0.0041\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1062/5000] -------------------- \n",
      "Epoch [1062/5000], Step [1/47], Loss: 0.0010\n",
      "Epoch [1062/5000], Step [9/47], Loss: 0.0043\n",
      "Epoch [1062/5000], Step [18/47], Loss: 0.0004\n",
      "Epoch [1062/5000], Step [27/47], Loss: 0.0036\n",
      "Epoch [1062/5000], Step [36/47], Loss: 0.0158\n",
      "Epoch [1062/5000], Step [45/47], Loss: 0.0036\n",
      "Epoch [1062/5000], Avg. Train Sample Loss: 0.0079, Avg. Validate Sample Loss: 0.0015,                             L2 Loss: 0.0040\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1063/5000] -------------------- \n",
      "Epoch [1063/5000], Step [1/47], Loss: 0.0012\n",
      "Epoch [1063/5000], Step [9/47], Loss: 0.0378\n",
      "Epoch [1063/5000], Step [18/47], Loss: 0.0499\n",
      "Epoch [1063/5000], Step [27/47], Loss: 0.1001\n",
      "Epoch [1063/5000], Step [36/47], Loss: 0.0503\n",
      "Epoch [1063/5000], Step [45/47], Loss: 0.0014\n",
      "Epoch [1063/5000], Avg. Train Sample Loss: 0.0569, Avg. Validate Sample Loss: 0.0180,                             L2 Loss: 0.0205\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1064/5000] -------------------- \n",
      "Epoch [1064/5000], Step [1/47], Loss: 0.0172\n",
      "Epoch [1064/5000], Step [9/47], Loss: 0.0097\n",
      "Epoch [1064/5000], Step [18/47], Loss: 0.0058\n",
      "Epoch [1064/5000], Step [27/47], Loss: 0.0019\n",
      "Epoch [1064/5000], Step [36/47], Loss: 0.0011\n",
      "Epoch [1064/5000], Step [45/47], Loss: 0.0012\n",
      "Epoch [1064/5000], Avg. Train Sample Loss: 0.0050, Avg. Validate Sample Loss: 0.0025,                             L2 Loss: 0.0064\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1065/5000] -------------------- \n",
      "Epoch [1065/5000], Step [1/47], Loss: 0.0020\n",
      "Epoch [1065/5000], Step [9/47], Loss: 0.0018\n",
      "Epoch [1065/5000], Step [18/47], Loss: 0.0008\n",
      "Epoch [1065/5000], Step [27/47], Loss: 0.0010\n",
      "Epoch [1065/5000], Step [36/47], Loss: 0.0005\n",
      "Epoch [1065/5000], Step [45/47], Loss: 0.0004\n",
      "Epoch [1065/5000], Avg. Train Sample Loss: 0.0010, Avg. Validate Sample Loss: 0.0008,                             L2 Loss: 0.0015\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1066/5000] -------------------- \n",
      "Epoch [1066/5000], Step [1/47], Loss: 0.0005\n",
      "Epoch [1066/5000], Step [9/47], Loss: 0.0010\n",
      "Epoch [1066/5000], Step [18/47], Loss: 0.0035\n",
      "Epoch [1066/5000], Step [27/47], Loss: 0.0005\n",
      "Epoch [1066/5000], Step [36/47], Loss: 0.0007\n",
      "Epoch [1066/5000], Step [45/47], Loss: 0.0011\n",
      "Epoch [1066/5000], Avg. Train Sample Loss: 0.0011, Avg. Validate Sample Loss: 0.0010,                             L2 Loss: 0.0035\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1067/5000] -------------------- \n",
      "Epoch [1067/5000], Step [1/47], Loss: 0.0008\n",
      "Epoch [1067/5000], Step [9/47], Loss: 0.0019\n",
      "Epoch [1067/5000], Step [18/47], Loss: 0.0006\n",
      "Epoch [1067/5000], Step [27/47], Loss: 0.0012\n",
      "Epoch [1067/5000], Step [36/47], Loss: 0.0005\n",
      "Epoch [1067/5000], Step [45/47], Loss: 0.0039\n",
      "Epoch [1067/5000], Avg. Train Sample Loss: 0.0012, Avg. Validate Sample Loss: 0.0040,                             L2 Loss: 0.0090\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1068/5000] -------------------- \n",
      "Epoch [1068/5000], Step [1/47], Loss: 0.0048\n",
      "Epoch [1068/5000], Step [9/47], Loss: 0.0032\n",
      "Epoch [1068/5000], Step [18/47], Loss: 0.0009\n",
      "Epoch [1068/5000], Step [27/47], Loss: 0.0031\n",
      "Epoch [1068/5000], Step [36/47], Loss: 0.0005\n",
      "Epoch [1068/5000], Step [45/47], Loss: 0.0007\n",
      "Epoch [1068/5000], Avg. Train Sample Loss: 0.0021, Avg. Validate Sample Loss: 0.0025,                             L2 Loss: 0.0063\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1069/5000] -------------------- \n",
      "Epoch [1069/5000], Step [1/47], Loss: 0.0022\n",
      "Epoch [1069/5000], Step [9/47], Loss: 0.0009\n",
      "Epoch [1069/5000], Step [18/47], Loss: 0.0020\n",
      "Epoch [1069/5000], Step [27/47], Loss: 0.0023\n",
      "Epoch [1069/5000], Step [36/47], Loss: 0.0032\n",
      "Epoch [1069/5000], Step [45/47], Loss: 0.0018\n",
      "Epoch [1069/5000], Avg. Train Sample Loss: 0.0017, Avg. Validate Sample Loss: 0.0012,                             L2 Loss: 0.0033\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1070/5000] -------------------- \n",
      "Epoch [1070/5000], Step [1/47], Loss: 0.0011\n",
      "Epoch [1070/5000], Step [9/47], Loss: 0.0010\n",
      "Epoch [1070/5000], Step [18/47], Loss: 0.0004\n",
      "Epoch [1070/5000], Step [27/47], Loss: 0.0006\n",
      "Epoch [1070/5000], Step [36/47], Loss: 0.0012\n",
      "Epoch [1070/5000], Step [45/47], Loss: 0.0017\n",
      "Epoch [1070/5000], Avg. Train Sample Loss: 0.0013, Avg. Validate Sample Loss: 0.0010,                             L2 Loss: 0.0028\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1071/5000] -------------------- \n",
      "Epoch [1071/5000], Step [1/47], Loss: 0.0009\n",
      "Epoch [1071/5000], Step [9/47], Loss: 0.0016\n",
      "Epoch [1071/5000], Step [18/47], Loss: 0.0009\n",
      "Epoch [1071/5000], Step [27/47], Loss: 0.0012\n",
      "Epoch [1071/5000], Step [36/47], Loss: 0.0008\n",
      "Epoch [1071/5000], Step [45/47], Loss: 0.0015\n",
      "Epoch [1071/5000], Avg. Train Sample Loss: 0.0013, Avg. Validate Sample Loss: 0.0019,                             L2 Loss: 0.0054\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1072/5000] -------------------- \n",
      "Epoch [1072/5000], Step [1/47], Loss: 0.0023\n",
      "Epoch [1072/5000], Step [9/47], Loss: 0.0006\n",
      "Epoch [1072/5000], Step [18/47], Loss: 0.0005\n",
      "Epoch [1072/5000], Step [27/47], Loss: 0.0056\n",
      "Epoch [1072/5000], Step [36/47], Loss: 0.0042\n",
      "Epoch [1072/5000], Step [45/47], Loss: 0.0014\n",
      "Epoch [1072/5000], Avg. Train Sample Loss: 0.0018, Avg. Validate Sample Loss: 0.0017,                             L2 Loss: 0.0049\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1073/5000] -------------------- \n",
      "Epoch [1073/5000], Step [1/47], Loss: 0.0027\n",
      "Epoch [1073/5000], Step [9/47], Loss: 0.0014\n",
      "Epoch [1073/5000], Step [18/47], Loss: 0.0006\n",
      "Epoch [1073/5000], Step [27/47], Loss: 0.0007\n",
      "Epoch [1073/5000], Step [36/47], Loss: 0.0005\n",
      "Epoch [1073/5000], Step [45/47], Loss: 0.0006\n",
      "Epoch [1073/5000], Avg. Train Sample Loss: 0.0011, Avg. Validate Sample Loss: 0.0010,                             L2 Loss: 0.0025\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1074/5000] -------------------- \n",
      "Epoch [1074/5000], Step [1/47], Loss: 0.0010\n",
      "Epoch [1074/5000], Step [9/47], Loss: 0.0007\n",
      "Epoch [1074/5000], Step [18/47], Loss: 0.0036\n",
      "Epoch [1074/5000], Step [27/47], Loss: 0.0020\n",
      "Epoch [1074/5000], Step [36/47], Loss: 0.0006\n",
      "Epoch [1074/5000], Step [45/47], Loss: 0.0035\n",
      "Epoch [1074/5000], Avg. Train Sample Loss: 0.0014, Avg. Validate Sample Loss: 0.0024,                             L2 Loss: 0.0062\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1075/5000] -------------------- \n",
      "Epoch [1075/5000], Step [1/47], Loss: 0.0016\n",
      "Epoch [1075/5000], Step [9/47], Loss: 0.0035\n",
      "Epoch [1075/5000], Step [18/47], Loss: 0.0039\n",
      "Epoch [1075/5000], Step [27/47], Loss: 0.0048\n",
      "Epoch [1075/5000], Step [36/47], Loss: 0.0049\n",
      "Epoch [1075/5000], Step [45/47], Loss: 0.0097\n",
      "Epoch [1075/5000], Avg. Train Sample Loss: 0.0040, Avg. Validate Sample Loss: 0.0013,                             L2 Loss: 0.0037\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1076/5000] -------------------- \n",
      "Epoch [1076/5000], Step [1/47], Loss: 0.0011\n",
      "Epoch [1076/5000], Step [9/47], Loss: 0.0060\n",
      "Epoch [1076/5000], Step [18/47], Loss: 0.0010\n",
      "Epoch [1076/5000], Step [27/47], Loss: 0.0026\n",
      "Epoch [1076/5000], Step [36/47], Loss: 0.0063\n",
      "Epoch [1076/5000], Step [45/47], Loss: 0.0075\n",
      "Epoch [1076/5000], Avg. Train Sample Loss: 0.0039, Avg. Validate Sample Loss: 0.0023,                             L2 Loss: 0.0064\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1077/5000] -------------------- \n",
      "Epoch [1077/5000], Step [1/47], Loss: 0.0025\n",
      "Epoch [1077/5000], Step [9/47], Loss: 0.0066\n",
      "Epoch [1077/5000], Step [18/47], Loss: 0.0011\n",
      "Epoch [1077/5000], Step [27/47], Loss: 0.0043\n",
      "Epoch [1077/5000], Step [36/47], Loss: 0.0010\n",
      "Epoch [1077/5000], Step [45/47], Loss: 0.0023\n",
      "Epoch [1077/5000], Avg. Train Sample Loss: 0.0020, Avg. Validate Sample Loss: 0.0009,                             L2 Loss: 0.0024\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1078/5000] -------------------- \n",
      "Epoch [1078/5000], Step [1/47], Loss: 0.0011\n",
      "Epoch [1078/5000], Step [9/47], Loss: 0.0013\n",
      "Epoch [1078/5000], Step [18/47], Loss: 0.0029\n",
      "Epoch [1078/5000], Step [27/47], Loss: 0.0051\n",
      "Epoch [1078/5000], Step [36/47], Loss: 0.0013\n",
      "Epoch [1078/5000], Step [45/47], Loss: 0.0037\n",
      "Epoch [1078/5000], Avg. Train Sample Loss: 0.0029, Avg. Validate Sample Loss: 0.0012,                             L2 Loss: 0.0034\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1079/5000] -------------------- \n",
      "Epoch [1079/5000], Step [1/47], Loss: 0.0012\n",
      "Epoch [1079/5000], Step [9/47], Loss: 0.0117\n",
      "Epoch [1079/5000], Step [18/47], Loss: 0.0005\n",
      "Epoch [1079/5000], Step [27/47], Loss: 0.0014\n",
      "Epoch [1079/5000], Step [36/47], Loss: 0.0020\n",
      "Epoch [1079/5000], Step [45/47], Loss: 0.0060\n",
      "Epoch [1079/5000], Avg. Train Sample Loss: 0.0037, Avg. Validate Sample Loss: 0.0013,                             L2 Loss: 0.0036\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1080/5000] -------------------- \n",
      "Epoch [1080/5000], Step [1/47], Loss: 0.0007\n",
      "Epoch [1080/5000], Step [9/47], Loss: 0.0064\n",
      "Epoch [1080/5000], Step [18/47], Loss: 0.0033\n",
      "Epoch [1080/5000], Step [27/47], Loss: 0.0027\n",
      "Epoch [1080/5000], Step [36/47], Loss: 0.0013\n",
      "Epoch [1080/5000], Step [45/47], Loss: 0.0004\n",
      "Epoch [1080/5000], Avg. Train Sample Loss: 0.0039, Avg. Validate Sample Loss: 0.0010,                             L2 Loss: 0.0026\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1081/5000] -------------------- \n",
      "Epoch [1081/5000], Step [1/47], Loss: 0.0010\n",
      "Epoch [1081/5000], Step [9/47], Loss: 0.0221\n",
      "Epoch [1081/5000], Step [18/47], Loss: 0.0017\n",
      "Epoch [1081/5000], Step [27/47], Loss: 0.0042\n",
      "Epoch [1081/5000], Step [36/47], Loss: 0.0058\n",
      "Epoch [1081/5000], Step [45/47], Loss: 0.0007\n",
      "Epoch [1081/5000], Avg. Train Sample Loss: 0.0089, Avg. Validate Sample Loss: 0.0021,                             L2 Loss: 0.0064\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1082/5000] -------------------- \n",
      "Epoch [1082/5000], Step [1/47], Loss: 0.0022\n",
      "Epoch [1082/5000], Step [9/47], Loss: 0.0048\n",
      "Epoch [1082/5000], Step [18/47], Loss: 0.0169\n",
      "Epoch [1082/5000], Step [27/47], Loss: 0.0045\n",
      "Epoch [1082/5000], Step [36/47], Loss: 0.0088\n",
      "Epoch [1082/5000], Step [45/47], Loss: 0.0031\n",
      "Epoch [1082/5000], Avg. Train Sample Loss: 0.0081, Avg. Validate Sample Loss: 0.0011,                             L2 Loss: 0.0028\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1083/5000] -------------------- \n",
      "Epoch [1083/5000], Step [1/47], Loss: 0.0004\n",
      "Epoch [1083/5000], Step [9/47], Loss: 0.0009\n",
      "Epoch [1083/5000], Step [18/47], Loss: 0.0027\n",
      "Epoch [1083/5000], Step [27/47], Loss: 0.0065\n",
      "Epoch [1083/5000], Step [36/47], Loss: 0.0005\n",
      "Epoch [1083/5000], Step [45/47], Loss: 0.0019\n",
      "Epoch [1083/5000], Avg. Train Sample Loss: 0.0023, Avg. Validate Sample Loss: 0.0021,                             L2 Loss: 0.0058\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1084/5000] -------------------- \n",
      "Epoch [1084/5000], Step [1/47], Loss: 0.0019\n",
      "Epoch [1084/5000], Step [9/47], Loss: 0.0007\n",
      "Epoch [1084/5000], Step [18/47], Loss: 0.0038\n",
      "Epoch [1084/5000], Step [27/47], Loss: 0.0111\n",
      "Epoch [1084/5000], Step [36/47], Loss: 0.0007\n",
      "Epoch [1084/5000], Step [45/47], Loss: 0.0017\n",
      "Epoch [1084/5000], Avg. Train Sample Loss: 0.0031, Avg. Validate Sample Loss: 0.0007,                             L2 Loss: 0.0029\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1085/5000] -------------------- \n",
      "Epoch [1085/5000], Step [1/47], Loss: 0.0010\n",
      "Epoch [1085/5000], Step [9/47], Loss: 0.0007\n",
      "Epoch [1085/5000], Step [18/47], Loss: 0.0011\n",
      "Epoch [1085/5000], Step [27/47], Loss: 0.0046\n",
      "Epoch [1085/5000], Step [36/47], Loss: 0.0008\n",
      "Epoch [1085/5000], Step [45/47], Loss: 0.0005\n",
      "Epoch [1085/5000], Avg. Train Sample Loss: 0.0018, Avg. Validate Sample Loss: 0.0009,                             L2 Loss: 0.0024\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1086/5000] -------------------- \n",
      "Epoch [1086/5000], Step [1/47], Loss: 0.0008\n",
      "Epoch [1086/5000], Step [9/47], Loss: 0.0025\n",
      "Epoch [1086/5000], Step [18/47], Loss: 0.0019\n",
      "Epoch [1086/5000], Step [27/47], Loss: 0.0058\n",
      "Epoch [1086/5000], Step [36/47], Loss: 0.0121\n",
      "Epoch [1086/5000], Step [45/47], Loss: 0.0028\n",
      "Epoch [1086/5000], Avg. Train Sample Loss: 0.0051, Avg. Validate Sample Loss: 0.0038,                             L2 Loss: 0.0082\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1087/5000] -------------------- \n",
      "Epoch [1087/5000], Step [1/47], Loss: 0.0028\n",
      "Epoch [1087/5000], Step [9/47], Loss: 0.0008\n",
      "Epoch [1087/5000], Step [18/47], Loss: 0.0012\n",
      "Epoch [1087/5000], Step [27/47], Loss: 0.0025\n",
      "Epoch [1087/5000], Step [36/47], Loss: 0.0064\n",
      "Epoch [1087/5000], Step [45/47], Loss: 0.0082\n",
      "Epoch [1087/5000], Avg. Train Sample Loss: 0.0034, Avg. Validate Sample Loss: 0.0078,                             L2 Loss: 0.0134\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1088/5000] -------------------- \n",
      "Epoch [1088/5000], Step [1/47], Loss: 0.0086\n",
      "Epoch [1088/5000], Step [9/47], Loss: 0.0034\n",
      "Epoch [1088/5000], Step [18/47], Loss: 0.0098\n",
      "Epoch [1088/5000], Step [27/47], Loss: 0.0011\n",
      "Epoch [1088/5000], Step [36/47], Loss: 0.0008\n",
      "Epoch [1088/5000], Step [45/47], Loss: 0.0006\n",
      "Epoch [1088/5000], Avg. Train Sample Loss: 0.0036, Avg. Validate Sample Loss: 0.0028,                             L2 Loss: 0.0072\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1089/5000] -------------------- \n",
      "Epoch [1089/5000], Step [1/47], Loss: 0.0028\n",
      "Epoch [1089/5000], Step [9/47], Loss: 0.0010\n",
      "Epoch [1089/5000], Step [18/47], Loss: 0.0289\n",
      "Epoch [1089/5000], Step [27/47], Loss: 0.0071\n",
      "Epoch [1089/5000], Step [36/47], Loss: 0.0028\n",
      "Epoch [1089/5000], Step [45/47], Loss: 0.0009\n",
      "Epoch [1089/5000], Avg. Train Sample Loss: 0.0067, Avg. Validate Sample Loss: 0.0026,                             L2 Loss: 0.0070\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1090/5000] -------------------- \n",
      "Epoch [1090/5000], Step [1/47], Loss: 0.0021\n",
      "Epoch [1090/5000], Step [9/47], Loss: 0.0012\n",
      "Epoch [1090/5000], Step [18/47], Loss: 0.0010\n",
      "Epoch [1090/5000], Step [27/47], Loss: 0.0021\n",
      "Epoch [1090/5000], Step [36/47], Loss: 0.0014\n",
      "Epoch [1090/5000], Step [45/47], Loss: 0.0138\n",
      "Epoch [1090/5000], Avg. Train Sample Loss: 0.0026, Avg. Validate Sample Loss: 0.0095,                             L2 Loss: 0.0143\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1091/5000] -------------------- \n",
      "Epoch [1091/5000], Step [1/47], Loss: 0.0089\n",
      "Epoch [1091/5000], Step [9/47], Loss: 0.0017\n",
      "Epoch [1091/5000], Step [18/47], Loss: 0.0021\n",
      "Epoch [1091/5000], Step [27/47], Loss: 0.0019\n",
      "Epoch [1091/5000], Step [36/47], Loss: 0.0118\n",
      "Epoch [1091/5000], Step [45/47], Loss: 0.0084\n",
      "Epoch [1091/5000], Avg. Train Sample Loss: 0.0044, Avg. Validate Sample Loss: 0.0053,                             L2 Loss: 0.0099\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1092/5000] -------------------- \n",
      "Epoch [1092/5000], Step [1/47], Loss: 0.0051\n",
      "Epoch [1092/5000], Step [9/47], Loss: 0.0057\n",
      "Epoch [1092/5000], Step [18/47], Loss: 0.0006\n",
      "Epoch [1092/5000], Step [27/47], Loss: 0.0004\n",
      "Epoch [1092/5000], Step [36/47], Loss: 0.0028\n",
      "Epoch [1092/5000], Step [45/47], Loss: 0.0011\n",
      "Epoch [1092/5000], Avg. Train Sample Loss: 0.0026, Avg. Validate Sample Loss: 0.0015,                             L2 Loss: 0.0045\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1093/5000] -------------------- \n",
      "Epoch [1093/5000], Step [1/47], Loss: 0.0012\n",
      "Epoch [1093/5000], Step [9/47], Loss: 0.0006\n",
      "Epoch [1093/5000], Step [18/47], Loss: 0.0038\n",
      "Epoch [1093/5000], Step [27/47], Loss: 0.0015\n",
      "Epoch [1093/5000], Step [36/47], Loss: 0.0035\n",
      "Epoch [1093/5000], Step [45/47], Loss: 0.0122\n",
      "Epoch [1093/5000], Avg. Train Sample Loss: 0.0039, Avg. Validate Sample Loss: 0.0098,                             L2 Loss: 0.0147\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1094/5000] -------------------- \n",
      "Epoch [1094/5000], Step [1/47], Loss: 0.0097\n",
      "Epoch [1094/5000], Step [9/47], Loss: 0.0064\n",
      "Epoch [1094/5000], Step [18/47], Loss: 0.0033\n",
      "Epoch [1094/5000], Step [27/47], Loss: 0.0041\n",
      "Epoch [1094/5000], Step [36/47], Loss: 0.0062\n",
      "Epoch [1094/5000], Step [45/47], Loss: 0.0007\n",
      "Epoch [1094/5000], Avg. Train Sample Loss: 0.0041, Avg. Validate Sample Loss: 0.0021,                             L2 Loss: 0.0056\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1095/5000] -------------------- \n",
      "Epoch [1095/5000], Step [1/47], Loss: 0.0023\n",
      "Epoch [1095/5000], Step [9/47], Loss: 0.0007\n",
      "Epoch [1095/5000], Step [18/47], Loss: 0.0073\n",
      "Epoch [1095/5000], Step [27/47], Loss: 0.0134\n",
      "Epoch [1095/5000], Step [36/47], Loss: 0.0367\n",
      "Epoch [1095/5000], Step [45/47], Loss: 0.0021\n",
      "Epoch [1095/5000], Avg. Train Sample Loss: 0.0081, Avg. Validate Sample Loss: 0.0017,                             L2 Loss: 0.0037\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1096/5000] -------------------- \n",
      "Epoch [1096/5000], Step [1/47], Loss: 0.0037\n",
      "Epoch [1096/5000], Step [9/47], Loss: 0.0032\n",
      "Epoch [1096/5000], Step [18/47], Loss: 0.0178\n",
      "Epoch [1096/5000], Step [27/47], Loss: 0.0049\n",
      "Epoch [1096/5000], Step [36/47], Loss: 0.0035\n",
      "Epoch [1096/5000], Step [45/47], Loss: 0.0034\n",
      "Epoch [1096/5000], Avg. Train Sample Loss: 0.0077, Avg. Validate Sample Loss: 0.0038,                             L2 Loss: 0.0092\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1097/5000] -------------------- \n",
      "Epoch [1097/5000], Step [1/47], Loss: 0.0044\n",
      "Epoch [1097/5000], Step [9/47], Loss: 0.0065\n",
      "Epoch [1097/5000], Step [18/47], Loss: 0.0049\n",
      "Epoch [1097/5000], Step [27/47], Loss: 0.0021\n",
      "Epoch [1097/5000], Step [36/47], Loss: 0.0012\n",
      "Epoch [1097/5000], Step [45/47], Loss: 0.0053\n",
      "Epoch [1097/5000], Avg. Train Sample Loss: 0.0057, Avg. Validate Sample Loss: 0.0023,                             L2 Loss: 0.0064\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1098/5000] -------------------- \n",
      "Epoch [1098/5000], Step [1/47], Loss: 0.0027\n",
      "Epoch [1098/5000], Step [9/47], Loss: 0.0018\n",
      "Epoch [1098/5000], Step [18/47], Loss: 0.0012\n",
      "Epoch [1098/5000], Step [27/47], Loss: 0.0005\n",
      "Epoch [1098/5000], Step [36/47], Loss: 0.0028\n",
      "Epoch [1098/5000], Step [45/47], Loss: 0.0005\n",
      "Epoch [1098/5000], Avg. Train Sample Loss: 0.0022, Avg. Validate Sample Loss: 0.0063,                             L2 Loss: 0.0114\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1099/5000] -------------------- \n",
      "Epoch [1099/5000], Step [1/47], Loss: 0.0069\n",
      "Epoch [1099/5000], Step [9/47], Loss: 0.0023\n",
      "Epoch [1099/5000], Step [18/47], Loss: 0.0021\n",
      "Epoch [1099/5000], Step [27/47], Loss: 0.0152\n",
      "Epoch [1099/5000], Step [36/47], Loss: 0.0034\n",
      "Epoch [1099/5000], Step [45/47], Loss: 0.0018\n",
      "Epoch [1099/5000], Avg. Train Sample Loss: 0.0051, Avg. Validate Sample Loss: 0.0010,                             L2 Loss: 0.0025\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1100/5000] -------------------- \n",
      "Epoch [1100/5000], Step [1/47], Loss: 0.0007\n",
      "Epoch [1100/5000], Step [9/47], Loss: 0.0041\n",
      "Epoch [1100/5000], Step [18/47], Loss: 0.0036\n",
      "Epoch [1100/5000], Step [27/47], Loss: 0.0010\n",
      "Epoch [1100/5000], Step [36/47], Loss: 0.0051\n",
      "Epoch [1100/5000], Step [45/47], Loss: 0.0052\n",
      "Epoch [1100/5000], Avg. Train Sample Loss: 0.0043, Avg. Validate Sample Loss: 0.0022,                             L2 Loss: 0.0055\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1101/5000] -------------------- \n",
      "Epoch [1101/5000], Step [1/47], Loss: 0.0017\n",
      "Epoch [1101/5000], Step [9/47], Loss: 0.0015\n",
      "Epoch [1101/5000], Step [18/47], Loss: 0.0011\n",
      "Epoch [1101/5000], Step [27/47], Loss: 0.0006\n",
      "Epoch [1101/5000], Step [36/47], Loss: 0.0019\n",
      "Epoch [1101/5000], Step [45/47], Loss: 0.0008\n",
      "Epoch [1101/5000], Avg. Train Sample Loss: 0.0017, Avg. Validate Sample Loss: 0.0066,                             L2 Loss: 0.0115\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1102/5000] -------------------- \n",
      "Epoch [1102/5000], Step [1/47], Loss: 0.0054\n",
      "Epoch [1102/5000], Step [9/47], Loss: 0.0029\n",
      "Epoch [1102/5000], Step [18/47], Loss: 0.0033\n",
      "Epoch [1102/5000], Step [27/47], Loss: 0.0008\n",
      "Epoch [1102/5000], Step [36/47], Loss: 0.0122\n",
      "Epoch [1102/5000], Step [45/47], Loss: 0.0036\n",
      "Epoch [1102/5000], Avg. Train Sample Loss: 0.0044, Avg. Validate Sample Loss: 0.0009,                             L2 Loss: 0.0014\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1103/5000] -------------------- \n",
      "Epoch [1103/5000], Step [1/47], Loss: 0.0007\n",
      "Epoch [1103/5000], Step [9/47], Loss: 0.0058\n",
      "Epoch [1103/5000], Step [18/47], Loss: 0.0202\n",
      "Epoch [1103/5000], Step [27/47], Loss: 0.0090\n",
      "Epoch [1103/5000], Step [36/47], Loss: 0.0114\n",
      "Epoch [1103/5000], Step [45/47], Loss: 0.0059\n",
      "Epoch [1103/5000], Avg. Train Sample Loss: 0.0076, Avg. Validate Sample Loss: 0.0028,                             L2 Loss: 0.0061\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1104/5000] -------------------- \n",
      "Epoch [1104/5000], Step [1/47], Loss: 0.0038\n",
      "Epoch [1104/5000], Step [9/47], Loss: 0.0010\n",
      "Epoch [1104/5000], Step [18/47], Loss: 0.0034\n",
      "Epoch [1104/5000], Step [27/47], Loss: 0.0007\n",
      "Epoch [1104/5000], Step [36/47], Loss: 0.0007\n",
      "Epoch [1104/5000], Step [45/47], Loss: 0.0006\n",
      "Epoch [1104/5000], Avg. Train Sample Loss: 0.0020, Avg. Validate Sample Loss: 0.0012,                             L2 Loss: 0.0033\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1105/5000] -------------------- \n",
      "Epoch [1105/5000], Step [1/47], Loss: 0.0005\n",
      "Epoch [1105/5000], Step [9/47], Loss: 0.0065\n",
      "Epoch [1105/5000], Step [18/47], Loss: 0.0019\n",
      "Epoch [1105/5000], Step [27/47], Loss: 0.0012\n",
      "Epoch [1105/5000], Step [36/47], Loss: 0.0021\n",
      "Epoch [1105/5000], Step [45/47], Loss: 0.0009\n",
      "Epoch [1105/5000], Avg. Train Sample Loss: 0.0020, Avg. Validate Sample Loss: 0.0035,                             L2 Loss: 0.0080\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1106/5000] -------------------- \n",
      "Epoch [1106/5000], Step [1/47], Loss: 0.0029\n",
      "Epoch [1106/5000], Step [9/47], Loss: 0.0005\n",
      "Epoch [1106/5000], Step [18/47], Loss: 0.0007\n",
      "Epoch [1106/5000], Step [27/47], Loss: 0.0011\n",
      "Epoch [1106/5000], Step [36/47], Loss: 0.0028\n",
      "Epoch [1106/5000], Step [45/47], Loss: 0.0098\n",
      "Epoch [1106/5000], Avg. Train Sample Loss: 0.0037, Avg. Validate Sample Loss: 0.0048,                             L2 Loss: 0.0101\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1107/5000] -------------------- \n",
      "Epoch [1107/5000], Step [1/47], Loss: 0.0040\n",
      "Epoch [1107/5000], Step [9/47], Loss: 0.0006\n",
      "Epoch [1107/5000], Step [18/47], Loss: 0.0071\n",
      "Epoch [1107/5000], Step [27/47], Loss: 0.0081\n",
      "Epoch [1107/5000], Step [36/47], Loss: 0.0009\n",
      "Epoch [1107/5000], Step [45/47], Loss: 0.0016\n",
      "Epoch [1107/5000], Avg. Train Sample Loss: 0.0046, Avg. Validate Sample Loss: 0.0009,                             L2 Loss: 0.0029\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1108/5000] -------------------- \n",
      "Epoch [1108/5000], Step [1/47], Loss: 0.0007\n",
      "Epoch [1108/5000], Step [9/47], Loss: 0.0006\n",
      "Epoch [1108/5000], Step [18/47], Loss: 0.0006\n",
      "Epoch [1108/5000], Step [27/47], Loss: 0.0025\n",
      "Epoch [1108/5000], Step [36/47], Loss: 0.0006\n",
      "Epoch [1108/5000], Step [45/47], Loss: 0.0032\n",
      "Epoch [1108/5000], Avg. Train Sample Loss: 0.0014, Avg. Validate Sample Loss: 0.0008,                             L2 Loss: 0.0014\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1109/5000] -------------------- \n",
      "Epoch [1109/5000], Step [1/47], Loss: 0.0006\n",
      "Epoch [1109/5000], Step [9/47], Loss: 0.0010\n",
      "Epoch [1109/5000], Step [18/47], Loss: 0.0009\n",
      "Epoch [1109/5000], Step [27/47], Loss: 0.0037\n",
      "Epoch [1109/5000], Step [36/47], Loss: 0.0052\n",
      "Epoch [1109/5000], Step [45/47], Loss: 0.0011\n",
      "Epoch [1109/5000], Avg. Train Sample Loss: 0.0026, Avg. Validate Sample Loss: 0.0015,                             L2 Loss: 0.0041\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1110/5000] -------------------- \n",
      "Epoch [1110/5000], Step [1/47], Loss: 0.0032\n",
      "Epoch [1110/5000], Step [9/47], Loss: 0.0006\n",
      "Epoch [1110/5000], Step [18/47], Loss: 0.0029\n",
      "Epoch [1110/5000], Step [27/47], Loss: 0.0014\n",
      "Epoch [1110/5000], Step [36/47], Loss: 0.0016\n",
      "Epoch [1110/5000], Step [45/47], Loss: 0.0108\n",
      "Epoch [1110/5000], Avg. Train Sample Loss: 0.0047, Avg. Validate Sample Loss: 0.0151,                             L2 Loss: 0.0184\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1111/5000] -------------------- \n",
      "Epoch [1111/5000], Step [1/47], Loss: 0.0146\n",
      "Epoch [1111/5000], Step [9/47], Loss: 0.0048\n",
      "Epoch [1111/5000], Step [18/47], Loss: 0.0016\n",
      "Epoch [1111/5000], Step [27/47], Loss: 0.0120\n",
      "Epoch [1111/5000], Step [36/47], Loss: 0.0381\n",
      "Epoch [1111/5000], Step [45/47], Loss: 0.0052\n",
      "Epoch [1111/5000], Avg. Train Sample Loss: 0.0135, Avg. Validate Sample Loss: 0.0015,                             L2 Loss: 0.0036\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1112/5000] -------------------- \n",
      "Epoch [1112/5000], Step [1/47], Loss: 0.0017\n",
      "Epoch [1112/5000], Step [9/47], Loss: 0.0052\n",
      "Epoch [1112/5000], Step [18/47], Loss: 0.0033\n",
      "Epoch [1112/5000], Step [27/47], Loss: 0.0026\n",
      "Epoch [1112/5000], Step [36/47], Loss: 0.0018\n",
      "Epoch [1112/5000], Step [45/47], Loss: 0.0014\n",
      "Epoch [1112/5000], Avg. Train Sample Loss: 0.0036, Avg. Validate Sample Loss: 0.0013,                             L2 Loss: 0.0034\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1113/5000] -------------------- \n",
      "Epoch [1113/5000], Step [1/47], Loss: 0.0009\n",
      "Epoch [1113/5000], Step [9/47], Loss: 0.0009\n",
      "Epoch [1113/5000], Step [18/47], Loss: 0.0009\n",
      "Epoch [1113/5000], Step [27/47], Loss: 0.0009\n",
      "Epoch [1113/5000], Step [36/47], Loss: 0.0014\n",
      "Epoch [1113/5000], Step [45/47], Loss: 0.0005\n",
      "Epoch [1113/5000], Avg. Train Sample Loss: 0.0011, Avg. Validate Sample Loss: 0.0008,                             L2 Loss: 0.0020\n",
      "------------------------------------------------------- \n",
      "-------------------- Epoch [1114/5000] -------------------- \n",
      "Epoch [1114/5000], Step [1/47], Loss: 0.0007\n",
      "Epoch [1114/5000], Step [9/47], Loss: 0.0010\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/vd/jk9yq2n522z116gdzng439fm0000gn/T/ipykernel_47185/96697132.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msolver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mODE_PINN_HARDBC\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mub\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_hidden\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m solver.Train(train_num=3000, train_batch_size=64, learning_rate=0.01, \n\u001b[0m\u001b[1;32m      6\u001b[0m              \u001b[0mlr_step_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_lr\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m1e-10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_gamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m              abs_tolerance=1e-4, max_epoch=5000, compute_L2_loss=True, true_sol=true_sol)\n",
      "\u001b[0;32m~/Downloads/MMSC-Computing-Case-Study-PINN-main/src/ode_PINN_hardBC.py\u001b[0m in \u001b[0;36mTrain\u001b[0;34m(self, train_num, train_batch_size, learning_rate, lr_step_size, min_lr, lr_gamma, abs_tolerance, max_epoch, compute_L2_loss, true_sol, display)\u001b[0m\n\u001b[1;32m    185\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResidualLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_hat\u001b[0m\u001b[0;34m)\u001b[0m         \u001b[0;31m# evaluate loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m                      \u001b[0;31m# clear gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m                            \u001b[0;31m# back propgation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m                           \u001b[0;31m# update parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m                 \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m                  \u001b[0;31m# compute the total loss for all batches in train set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "f = lambda x , y , D1y:  - (torch.pi)**2*y\n",
    "true_sol = lambda x :  torch.sin(torch.pi*x)\n",
    "\n",
    "solver = ODE_PINN_HARDBC( f=f, lb=-1, ub=1, BC=(2,0,-np.pi), n_hidden=16, n_layers=3)\n",
    "solver.Train(train_num=3000, train_batch_size=64, learning_rate=0.01, \n",
    "             lr_step_size=100, min_lr =1e-10, lr_gamma=0.5,\n",
    "             abs_tolerance=1e-4, max_epoch=5000, compute_L2_loss=True, true_sol=true_sol)\n",
    "SaveModel(solver,'./data/ode_sin_slover_n1_3layer_16unit.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x , y , D1y:  - (5*torch.pi)**2*y\n",
    "true_sol = lambda x :  torch.sin(5*torch.pi*x)\n",
    "\n",
    "solver = ODE_PINN_HARDBC( f=f, lb=-1, ub=1, BC=(2,0,-5*np.pi), n_hidden=16, n_layers=3)   ###\n",
    "solver.Train(train_num=3000, train_batch_size=64, learning_rate=0.001, \n",
    "             lr_step_size=100, min_lr =1e-10, lr_gamma=0.5,\n",
    "             abs_tolerance=1e-4, max_epoch=5000, compute_L2_loss=True, true_sol=true_sol)\n",
    "SaveModel(solver,'./data/ode_sin_slover_n5_3layer_16unit.pkl')\n",
    "\n",
    "\n",
    "solver = ODE_PINN_HARDBC( f=f, lb=-1, ub=1, BC=(2,0,-5*np.pi), n_hidden=16, n_layers=5)    ###\n",
    "solver.Train(train_num=3000, train_batch_size=64, learning_rate=0.001, \n",
    "             lr_step_size=100, min_lr =1e-10, lr_gamma=0.5,\n",
    "             abs_tolerance=1e-4, max_epoch=5000, compute_L2_loss=True, true_sol=true_sol)\n",
    "SaveModel(solver,'./data/ode_sin_slover_n5_5layer_16unit.pkl')\n",
    "\n",
    "\n",
    "solver = ODE_PINN_HARDBC( f=f, lb=-1, ub=1, BC=(2,0,-5*np.pi), n_hidden=32, n_layers=5)    ###\n",
    "solver.Train(train_num=3000, train_batch_size=64, learning_rate=0.001, \n",
    "             lr_step_size=100, min_lr =1e-10, lr_gamma=0.5,\n",
    "             abs_tolerance=1e-4, max_epoch=5000, compute_L2_loss=True, true_sol=true_sol)\n",
    "SaveModel(solver,'./data/ode_sin_slover_n5_5layer_32unit.pkl')\n",
    "\n",
    "\n",
    "solver = ODE_PINN_HARDBC( f=f, lb=-1, ub=1, BC=(2,0,-5*np.pi), n_hidden=64, n_layers=5)    \n",
    "solver.Train(train_num=3000, train_batch_size=64, learning_rate=0.001, \n",
    "             lr_step_size=100, min_lr =1e-10, lr_gamma=0.5,\n",
    "             abs_tolerance=1e-4, max_epoch=5000, compute_L2_loss=True, true_sol=true_sol)\n",
    "SaveModel(solver,'./data/ode_sin_slover_n5_5layer_64unit.pkl')\n",
    "\n",
    " \n",
    "solver = ODE_PINN_HARDBC( f=f, lb=-1, ub=1, BC=(2,0,-5*np.pi),                             ###\n",
    "                         n_hidden=16, n_layers=3, set_rff=True, rff_num=8, u=0, std=1)\n",
    "solver.Train(train_num=3000, train_batch_size=64, learning_rate=0.001, \n",
    "             lr_step_size=100, min_lr =1e-10, lr_gamma=0.5,\n",
    "             abs_tolerance=1e-4, max_epoch=5000, compute_L2_loss=True, true_sol=true_sol)\n",
    "SaveModel(solver,'./data/rff_ode_sin_slover_n5_3layer_16unit.pkl')\n",
    "\n",
    "\n",
    "solver = ODE_PINN_AdaptiveCollectionPoint( f=f, lb=-1, ub=1, BC=(2,0,-5*np.pi), n_hidden=64, n_layers=5) ##\n",
    "solver.Train(train_num=3000, train_batch_size=64, learning_rate=0.001, \n",
    "             lr_step_size=100, min_lr =1e-10, lr_gamma=0.5,\n",
    "             abs_tolerance=1e-4, max_epoch=5000, compute_L2_loss=True, true_sol=true_sol)\n",
    "SaveModel(solver,'./data/adp_ode_sin_slover_n5_5layer_64unit.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = lambda x , y , D1y:  - (5*torch.pi)**2*y\n",
    "# true_sol = lambda x :  torch.sin(5*torch.pi*x)\n",
    "\n",
    "# solver = ODE_PINN_HARDBC( f=f, lb=-1, ub=1, BC=(2,0,-5*np.pi),                             #\n",
    "#                          n_hidden=16, n_layers=3, set_rff=True, rff_num=8, u=0, std=1)\n",
    "# solver.Train(train_num=3000, train_batch_size=64, learning_rate=0.001, \n",
    "#              lr_step_size=100, min_lr =1e-10, lr_gamma=0.5,\n",
    "#              abs_tolerance=1e-4, max_epoch=5000, compute_L2_loss=True, true_sol=true_sol)\n",
    "# #SaveModel(solver,'./data/rff_ode_sin_slover_n5_3layer_16unit.pkl')\n",
    "\n",
    "\n",
    "# fig  = plt.figure(figsize = (14,5))\n",
    "# true_sol = lambda x:  torch.sin(5*torch.pi*x)\n",
    "# x = torch.linspace(-1,1,200).view(-1,1)\n",
    "# x_grid = x.squeeze().detach().numpy()\n",
    "\n",
    "# plt.subplot(1,2,1)\n",
    "# y_gt = true_sol(x).squeeze().detach().numpy()\n",
    "# y_hat = solver(x).squeeze().detach().numpy()\n",
    "# plt.plot(x_grid, y_gt, linewidth=2)\n",
    "# plt.plot(x_grid, y_hat, linewidth=2)\n",
    "\n",
    "# plt.subplot(1,2,2)\n",
    "# loss = np.log10(solver.L2_loss)\n",
    "# plt.plot(loss, linewidth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [ './data/ode_sin_slover_n1_3layer_16unit.pkl',\n",
    "          './data/ode_sin_slover_n5_3layer_16unit.pkl',\n",
    "          './data/ode_sin_slover_n5_5layer_16unit.pkl',\n",
    "          './data/ode_sin_slover_n5_5layer_32unit.pkl',\n",
    "          './data/rff_ode_sin_slover_n5_3layer_16unit.pkl' ]\n",
    "\n",
    "solvers = [LoadModel(path) for path in paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 792x360 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "title_fontsize , tick_size , leg_size = 19, 17, 16\n",
    "fig  = plt.figure(figsize = (11,5))\n",
    "# color   = ['#2e2a79', '#473477', '#915270', '#C25371', '#E94220', '#6d6875']\n",
    "# color   = ['#c1121f', '#802000','#003049','#386641','#6a4c93', '#6d6875']\n",
    "color   = ['#000000', '#521400', '#802000', '#F53D00', '#FF7547', '#6d6875']\n",
    "# color   = [\"#0d0c1d\",\"#161b33\", \"#474973\", '#a69cac', '#f1dac4', '#6d6875']\n",
    "\n",
    "legend  = ['REGPINN  n = 1\\n3 layers  16 units',\n",
    "           'REGPINN  n = 5\\n3 layers  16 units',\n",
    "           'REGPINN  n = 5\\n5 layers  16 units',\n",
    "           'REGPINN  n = 5\\n5 layers  32 units',\n",
    "           'RFFPINN   n = 5\\n3 layers  16 units',]\n",
    "\n",
    "\n",
    "ax_fit  = [plt.subplot(521), plt.subplot(523), plt.subplot(525), plt.subplot(527), plt.subplot(529)]\n",
    "ax_loss = plt.subplot(122)\n",
    "\n",
    "true_sol_1 = lambda x:  torch.sin(torch.pi*x)\n",
    "true_sol_2 = lambda x:  torch.sin(5*torch.pi*x)\n",
    "x = torch.linspace(-1,1,200).view(-1,1)\n",
    "x_grid = x.squeeze().detach().numpy()\n",
    "\n",
    "for i in range(5):\n",
    "    if i == 0:\n",
    "        y_gt = true_sol_1(x).squeeze().detach().numpy()\n",
    "    else:\n",
    "        y_gt = true_sol_2(x).squeeze().detach().numpy()\n",
    "    y_hat = solvers[i](x).squeeze().detach().numpy()\n",
    "\n",
    "    ax_fit[i].plot(x_grid, y_gt, color=color[-1], linewidth=2, linestyle='--')\n",
    "    ax_fit[i].plot(x_grid, y_hat, color=color[i], linewidth=2, label=legend[i])\n",
    "    ax_fit[i].grid()\n",
    "    \n",
    "    half_window_size = 10\n",
    "    loss = np.log10(solvers[i].L2_loss)\n",
    "    loss = moving_average(loss, half_window_size)\n",
    "    ax_loss.plot(loss, color=color[i], linewidth=2, label=legend[i])\n",
    "    \n",
    "    ax_fit[i].tick_params(axis='x', labelsize=tick_size)\n",
    "    ax_fit[i].tick_params(axis='y', labelsize=tick_size)\n",
    "    \n",
    "    if i == 4 :\n",
    "        ax_fit[i].set_xlabel('x', fontsize=title_fontsize+5, fontweight='bold')\n",
    "    ax_fit[i].set_ylabel('y', fontsize=title_fontsize, fontweight='bold', rotation=0)\n",
    "    \n",
    "ax_loss.grid()\n",
    "ax_loss.set_ylim(-3, 1.5)\n",
    "ax_loss.tick_params(axis='x', labelsize=tick_size)\n",
    "ax_loss.tick_params(axis='y', labelsize=tick_size)\n",
    "\n",
    "leg = ax_loss.legend(loc='center right', bbox_to_anchor=(1.63, 0.5), \n",
    "                     fontsize=leg_size, handlelength=1.0, labelspacing=1.4)\n",
    "\n",
    "ax_loss.set_ylabel('Log10   L2   Loss', fontsize=title_fontsize, fontweight='bold')\n",
    "ax_loss.set_xlabel('Epoch', fontsize=title_fontsize, fontweight='bold')\n",
    "\n",
    "plt.subplots_adjust(wspace=0.25, hspace=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other ODE Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training model 1\n",
      "start training model 2\n",
      "start training model 3\n",
      "start training model 4\n"
     ]
    }
   ],
   "source": [
    "# BC 1 Examples Training\n",
    "f1 = lambda x , y , D1y : -y\n",
    "f2 = lambda x , y , D1y : y ** 2 - 2 - (1+x*(1-x))**2\n",
    "f3 = lambda x , y , D1y : y\n",
    "f4 = lambda x , y , D1y:  torch.exp(x) * ( ( 1 - 16*torch.pi**2 ) * torch.sin(4*torch.pi*x) + 8*torch.pi*torch.cos(4*torch.pi*x) )\n",
    "fs = [f1, f2, f3, f4]\n",
    "\n",
    "y_true1 = lambda x : torch.sin(x)\n",
    "y_true2 = lambda x : 1 + x * ( 1 - x )\n",
    "y_true3 = lambda x : torch.exp(x)\n",
    "y_true4 = lambda x : torch.exp(x) * torch.sin(4*torch.pi*x) \n",
    "y_trues = [y_true1, y_true2, y_true3, y_true4]\n",
    "\n",
    "BCs = [(1,0,np.sin(1)) , (1,1,1) , (1,1,np.exp(1)) , (1,0,0)]\n",
    "\n",
    "# Initialize models\n",
    "models = [ ODE_PINN_HARDBC( f=fs[i], lb=0, ub=1, BC=BCs[i], n_hidden=50, n_layers=3) for i in range(4) ] \n",
    "\n",
    "## Training\n",
    "for i in range(4):\n",
    "    print('start training model %d'%(i+1))\n",
    "    torch.manual_seed(100)  \n",
    "    models[i].Train(train_num=1000, train_batch_size=32, learning_rate=0.01, \n",
    "             lr_step_size=100, min_lr =1e-10, lr_gamma=0.5,\n",
    "             abs_tolerance=0, max_epoch=200, compute_L2_loss=True, true_sol=y_trues[i], display=False)\n",
    "#     SaveModel(models[i], './data/ode_slover_BC1exp%d.pkl'%(i+1))\n",
    "# Savepickle(y_trues, './data/ode_slover_BC1exp_gt.pkl')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training model 1\n",
      "start training model 2\n",
      "start training model 3\n",
      "start training model 4\n"
     ]
    }
   ],
   "source": [
    "# BC 2 Examples Training\n",
    "f1 = lambda x , y , D1y : - y * ( y**2 + D1y**2 )\n",
    "f2 = lambda x , y , D1y : - 2 * x * D1y - 2 * y\n",
    "f3 = lambda x , y , D1y :  1 - D1y**2\n",
    "f4 = lambda x , y , D1y :  -y\n",
    "fs = [f1, f2, f3, f4]\n",
    "\n",
    "y_true1 = lambda x : torch.cos(x)\n",
    "y_true2 = lambda x : 2 * torch.exp(-x**2)\n",
    "y_true3 = lambda x : torch.log( (torch.exp(x)+torch.exp(-x))/2 ) \n",
    "y_true4 = lambda x : torch.sin(x)\n",
    "y_trues = [y_true1, y_true2, y_true3, y_true4]\n",
    "\n",
    "BCs = [(2, 1, -np.sin(1)), (2, 2, -4*np.exp(-1)), \n",
    "       (2, 0, (np.exp(1) - np.exp(-1))/(np.exp(1) + np.exp(-1))), \n",
    "       (2, 0, np.cos(1))]\n",
    "\n",
    "# Initialize models\n",
    "models = [ ODE_PINN_HARDBC( f=fs[i], lb=0, ub=1, BC=BCs[i], n_hidden=50, n_layers=3) for i in range(4) ] \n",
    "\n",
    "## Training\n",
    "for i in range(4):\n",
    "    print('start training model %d'%(i+1))\n",
    "    torch.manual_seed(100)  \n",
    "    models[i].Train(train_num=1000, train_batch_size=32, learning_rate=0.001, \n",
    "             lr_step_size=100, min_lr =1e-10, lr_gamma=0.5,\n",
    "             abs_tolerance=0, max_epoch=100, compute_L2_loss=True, true_sol=y_trues[i], display=False)\n",
    "#     SaveModel(models[i], './data/ode_slover_BC2exp%d.pkl'%(i+1))\n",
    "# Savepickle(y_trues, './data/ode_slover_BC2exp_gt.pkl')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training model 1\n",
      "start training model 2\n",
      "start training model 3\n",
      "start training model 4\n"
     ]
    }
   ],
   "source": [
    "# BC 3 Examples Training\n",
    "f1 = lambda x , y , D1y : - y * ( y**2 + D1y**2 )\n",
    "f2 = lambda x , y , D1y : - 2 * x * D1y - 2 * y\n",
    "f3 = lambda x , y , D1y :  1 - D1y**2\n",
    "f4 = lambda x , y , D1y :  -y\n",
    "fs = [f1, f2, f3, f4]\n",
    "\n",
    "y_true1 = lambda x : torch.cos(x)\n",
    "y_true2 = lambda x : 2 * torch.exp(-x**2)\n",
    "y_true3 = lambda x : torch.log( (torch.exp(x)+torch.exp(-x))/2 ) \n",
    "y_true4 = lambda x : torch.sin(x)\n",
    "y_trues = [y_true1, y_true2, y_true3, y_true4]\n",
    "\n",
    "BCs = [(3,1,-np.sin(1)), (3,2,-4*np.exp(-1)), \n",
    "       (3,0, (np.exp(1) - np.exp(-1))/(np.exp(1) + np.exp(-1)) ), \n",
    "       (3, 1, np.cos(1))]\n",
    "\n",
    "# Initialize models\n",
    "models = [ ODE_PINN_HARDBC( f=fs[i], lb=0, ub=1, BC=BCs[i], n_hidden=50, n_layers=3) for i in range(4) ] \n",
    "\n",
    "## Training\n",
    "for i in range(4):\n",
    "    print('start training model %d'%(i+1))\n",
    "    torch.manual_seed(100)  \n",
    "    models[i].Train(train_num=1000, train_batch_size=32, learning_rate=0.001, \n",
    "             lr_step_size=100, min_lr =1e-10, lr_gamma=0.5,\n",
    "             abs_tolerance=0, max_epoch=100, compute_L2_loss=True, true_sol=y_trues[i], display=False)\n",
    "#     SaveModel(models[i], './data/ode_slover_BC3exp%d.pkl'%(i+1))\n",
    "# Savepickle(y_trues, './data/ode_slover_BC3exp_gt.pkl')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BC 1 ODESYS Examples Training\n",
    "f = lambda x , y , D1y : torch.cat( ( x * y[:,0].unsqueeze(1) - y[:,1].unsqueeze(1) - 2 - x - (torch.pi/2)**2 * torch.sin(torch.pi*x/2), \n",
    "                                      y[:,1].unsqueeze(1) - 6 * x + 2 - x**2 * (1-x) ), dim=1)\n",
    "y_true = lambda x : torch.cat( (x*(1-x)+torch.sin(torch.pi*x/2), x**2*(1-x) ), dim=1)\n",
    "\n",
    "model = ORDER2_ODESYS_PINN_HARDBC( f=f, lb=0, ub=1, BC=(1,[0,0],[1,0]), n_hidden=50, n_layers=3)\n",
    "model.Train(train_num=2000, train_batch_size=32, learning_rate=0.001, lr_step_size=1000, min_lr =1e-10, lr_gamma=0.5,\n",
    "             abs_tolerance=0, max_epoch=100, compute_L2_loss=True, true_sol=y_true, display=True)\n",
    "# SaveModel(model, './data/odesys_slover_exp.pkl')\n",
    "# Savepickle(y_true, './data/odesys_slover_exp_gt.pkl') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "for k in range(4):\n",
    "\n",
    "    if k==3 :\n",
    "        model  = LoadModel('./data/odesys_slover_exp.pkl')\n",
    "        y_true = Readpickle('./data/odesys_slover_exp_gt.pkl')\n",
    "        data.append((model, y_true))\n",
    "    else:\n",
    "        paths   = [ './data/ode_slover_BC%dexp%d.pkl'%(k+1,i+1) for i in range(4) ]\n",
    "        models  = [LoadModel(path) for path in paths]\n",
    "        y_trues = Readpickle('./data/ode_slover_BC%dexp_gt.pkl'%(k+1))\n",
    "        data.append((models, y_trues))\n",
    "    \n",
    "\n",
    "true_names_lst  = [['$y_1$', '$y_2$', '$y_3$', '$y_4$'],\n",
    "                   ['$y_5$', '$y_6$', '$y_7$', '$y_8$'],\n",
    "                   ['$y_5$', '$y_6$', '$y_7$', '$y_8$'],\n",
    "                   ['$u$', '$v$']]\n",
    "\n",
    "model_names_lst = [['$y_1$ PINN', '$y_2$ PINN', '$y_3$ PINN', '$y_4$ PINN'],\n",
    "                   ['$y_5$ PINN', '$y_6$ PINN', '$y_7$ PINN', '$y_8$ PINN'],\n",
    "                   ['$y_5$ PINN', '$y_6$ PINN', '$y_7$ PINN', '$y_8$ PINN'],\n",
    "                   ['$u$ PINN', '$v$ PINN']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1008x468 with 8 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Plot loss changes\n",
    "fig, axes = plt.subplots( 2 , 4 , figsize=( 14, 6.5 ) )\n",
    "title_fontsize = 17\n",
    "tick_size = 14\n",
    "leg_size = 12\n",
    "half_window_size = 1\n",
    "\n",
    "PINNColor  = ['#000000','#802000','#F53D00','#FF7547']\n",
    "PINNColor2  = ['#000000','#F53D00']\n",
    "\n",
    "# PINNColor   = [\"#161b33\", \"#474973\", '#a69cac', '#f1dac4']\n",
    "# PINNColor2  = [\"#161b33\", \"#474973\"]\n",
    "\n",
    "titles = ['Type 1 BC Example', 'Type 2 BC Example', 'Type 3 BC Example', 'ODE System Example']\n",
    "\n",
    "for k in range(4):\n",
    "    \n",
    "    true_names = true_names_lst[k]\n",
    "    model_names = model_names_lst[k]\n",
    "    models , y_trues = data[k]\n",
    "    \n",
    "    x = torch.linspace(0,1,200).view(-1,1)\n",
    "    if k < 3:\n",
    "        y_trues_ = [ y_trues[i](x).squeeze().detach().numpy() for i in range(4) ]\n",
    "        y_hats_  = [ models[i](x).squeeze().detach().numpy()  for i in range(4) ]\n",
    "    else:\n",
    "        y_trues_ = y_trues(x).detach().numpy()\n",
    "        y_hats_  = models(x).detach().numpy()\n",
    "    x = x.squeeze().detach().numpy()\n",
    "    \n",
    "    num_lines = []\n",
    "    if k < 3:\n",
    "        for i in range(4):\n",
    "            # Fitting\n",
    "            axes[0,k].scatter(x[::20], y_trues_[i][::20] , linewidth=2, label=true_names[i], color=PINNColor[i])\n",
    "            num_line, = axes[0,k].plot(x, y_hats_[i], linewidth=2, label=model_names[i], color=PINNColor[i])\n",
    "            num_lines.append(num_line) \n",
    "            \n",
    "            # Loss\n",
    "            loss = np.log10(models[i].L2_loss)[:]\n",
    "            loss = moving_average(loss, half_window_size)\n",
    "            axes[1,k].plot(loss, linewidth=2, label=model_names[i], color=PINNColor[i])\n",
    "    else:\n",
    "        for i in range(2):\n",
    "            # Fitting\n",
    "            axes[0,k].scatter(x[::20], y_trues_[:,i][::20] , linewidth=2, label=true_names[i], color=PINNColor2[i])\n",
    "            num_line, = axes[0,k].plot(x, y_hats_[:,i], linewidth=2, label=model_names[i], color=PINNColor2[i])\n",
    "            num_lines.append(num_line) \n",
    "            \n",
    "            # Loss\n",
    "            loss = np.log10([loss_[i] for loss_ in models.L2_loss])[:]\n",
    "            loss = moving_average(loss, half_window_size)\n",
    "            axes[1,k].plot(loss, linewidth=2, label=model_names[i], color=PINNColor2[i])\n",
    "            \n",
    "    # Legend         \n",
    "    axes[0,k].legend(handles=num_lines, handlelength=1, loc='upper center', labelspacing=0.5, columnspacing=0.5,\n",
    "                        fontsize=leg_size, ncol=2, fancybox=True)\n",
    "    axes[1,k].legend(loc='upper center', handlelength=1, labelspacing=0.5, columnspacing=0.5,\n",
    "                        fontsize=leg_size, ncol=2, fancybox=True)\n",
    "\n",
    "    # Grid & y_lim\n",
    "    axes[0,k].grid()\n",
    "    axes[1,k].grid()    \n",
    "    \n",
    "    if k == 0:\n",
    "        axes[0,k].set_ylim(-2.8, 5.5)\n",
    "        axes[1,k].set_ylim(-5.5, 2)\n",
    "    elif k == 1 or k == 2:\n",
    "        axes[0,k].set_ylim(-0.3, 3)\n",
    "        axes[1,k].set_ylim(-4.5, 0)\n",
    "    else:\n",
    "        axes[0,k].set_ylim(-0.2, 1.6)\n",
    "        \n",
    "    # xy axes and title\n",
    "    axes[0,k].tick_params(axis='x', labelsize=tick_size)\n",
    "    axes[0,k].tick_params(axis='y', labelsize=tick_size)\n",
    "    axes[1,k].tick_params(axis='x', labelsize=tick_size)\n",
    "    axes[1,k].tick_params(axis='y', labelsize=tick_size)\n",
    "    \n",
    "    axes[0,k].set_title(titles[k], fontsize=title_fontsize, fontweight='bold', pad=15) \n",
    "    axes[1,k].set_xlabel('Epoch', fontsize=title_fontsize, fontweight='bold') \n",
    "\n",
    "axes[0,0].set_ylabel('Target Function Value', fontsize=title_fontsize, fontweight='bold') \n",
    "axes[1,0].set_ylabel('Log10  L2  Loss', fontsize=title_fontsize, fontweight='bold') \n",
    "\n",
    "plt.subplots_adjust(wspace=0.19, hspace=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
